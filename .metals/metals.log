[0m2021.02.25 09:56:43 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.53.2.[0m
[0m2021.02.25 09:56:44 INFO  time: initialize in 0.47s[0m
[0m2021.02.25 09:56:44 WARN  Build server is not auto-connectable.[0m
[0m2021.02.25 09:56:50 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals2835613066379044972/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 09:56:50 WARN  no build target for: /home/delaneylekien/project3/scalas3read/project/metals.sbt[0m
[0m2021.02.25 09:56:50 WARN  no build target for: /home/delaneylekien/project3/scalas3read/project/project/metals.sbt[0m
[0m2021.02.25 09:56:50 INFO  skipping build import with status 'Started'[0m
[0m2021.02.25 09:56:53 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
[0m2021.02.25 09:56:53 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
// DO NOT EDIT! This file is auto-generated.
// This file enables sbt-bloop to create bloop config files.

addSbtPlugin("ch.epfl.scala" % "sbt-bloop" % "1.4.8")

// DO NOT EDIT! This file is auto-generated.
// This file enables sbt-bloop to create bloop config files.

addSbtPlugin("ch.epfl.scala" % "sbt-bloop" % "1.4.8")

[0m2021.02.25 09:56:57 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.02.25 09:56:57 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.02.25 09:57:03 INFO  [info] loading settings for project scalas3read-build from metals.sbt ...[0m
[0m2021.02.25 09:57:03 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.02.25 09:57:05 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.02.25 09:57:05 INFO  [info] compiling 1 Scala source to /home/delaneylekien/project3/scalas3read/project/target/scala-2.12/sbt-1.0/classes ...[0m
[0m2021.02.25 09:57:09 INFO  [info] done compiling[0m
[0m2021.02.25 09:57:09 INFO  [success] Total time: 7 s, completed Feb 25, 2021 9:57:09 AM[0m
[0m2021.02.25 09:57:12 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 09:57:12 INFO  [info] set current project to scalas3read (in build file:/home/delaneylekien/project3/scalas3read/)[0m
[0m2021.02.25 09:57:12 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 09:57:12 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 09:57:12 INFO  [success] Total time: 0 s, completed Feb 25, 2021 9:57:13 AM[0m
[0m2021.02.25 09:57:13 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 09:57:13 INFO  time: ran 'sbt bloopInstall' in 23s[0m
[0m2021.02.25 09:57:13 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4938452289360382906/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.13.4.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/net/java/dev/jna/jna/5.3.1/jna-5.3.1.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/jline/jline/3.16.0/jline-3.16.0.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.13.4/scala-compiler-2.13.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.13.4/scala-library-2.13.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.13.4/scala-reflect-2.13.4.jar
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4938452289360382906/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4938452289360382906/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 09:57:14 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 09:57:16 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher390014680332409582/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher390014680332409582/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher390014680332409582/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 09:57:16 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 09:57:16 INFO  time: Connected to build server in 2.63s[0m
[0m2021.02.25 09:57:16 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 09:57:17 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 09:57:20 INFO  time: indexed workspace in 4.07s[0m
[0m2021.02.25 09:58:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 09:58:50 INFO  time: compiled root in 3.97s[0m
[0m2021.02.25 09:59:19 INFO  compiling scalas3read-build (1 scala source)[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.13.4"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.example"
ThisBuild / organizationName := "example"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.02.25 09:59:23 INFO  time: compiled scalas3read-build in 4.99s[0m
[0m2021.02.25 09:59:24 INFO  time: code lens generation in 3.32s[0m
[0m2021.02.25 09:59:24 INFO  time: code lens generation in 1.37s[0m
[0m2021.02.25 09:59:24 INFO  time: code lens generation in 5.44s[0m
[0m2021.02.25 09:59:24 INFO  time: code lens generation in 1.06s[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.13.4"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7"
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.02.25 10:00:22 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals1293261345922807990/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 10:00:23 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
[0m2021.02.25 10:00:24 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.02.25 10:00:24 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.02.25 10:00:26 INFO  [info] loading settings for project scalas3read-build from metals.sbt ...[0m
[0m2021.02.25 10:00:26 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.02.25 10:00:29 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.02.25 10:00:29 INFO  [success] Total time: 3 s, completed Feb 25, 2021 10:00:30 AM[0m
[0m2021.02.25 10:00:33 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 10:00:33 INFO  [info] set current project to scalas3read (in build file:/home/delaneylekien/project3/scalas3read/)[0m
[0m2021.02.25 10:00:36 INFO  [warn] [0m
[0m2021.02.25 10:00:36 INFO  [warn] 	Note: Unresolved dependencies path:[0m
[0m2021.02.25 10:00:36 INFO  [error] sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-sql_2.13:2.4.7[0m
[0m2021.02.25 10:00:36 INFO  [error]   Not found[0m
[0m2021.02.25 10:00:36 INFO  [error]   Not found[0m
[0m2021.02.25 10:00:36 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.apache.spark/spark-sql_2.13/2.4.7/ivys/ivy.xml[0m
[0m2021.02.25 10:00:36 INFO  [error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-sql_2.13/2.4.7/spark-sql_2.13-2.4.7.pom[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at lmcoursier.CoursierDependencyResolution.unresolvedWarningOrThrow(CoursierDependencyResolution.scala:258)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at lmcoursier.CoursierDependencyResolution.$anonfun$update$38(CoursierDependencyResolution.scala:227)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at scala.util.Either$LeftProjection.map(Either.scala:573)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at lmcoursier.CoursierDependencyResolution.update(CoursierDependencyResolution.scala:227)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.librarymanagement.DependencyResolution.update(DependencyResolution.scala:60)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.internal.LibraryManagement$.resolve$1(LibraryManagement.scala:53)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$12(LibraryManagement.scala:103)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.util.Tracked$.$anonfun$lastOutput$1(Tracked.scala:73)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$20(LibraryManagement.scala:116)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at scala.util.control.Exception$Catch.apply(Exception.scala:228)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11(LibraryManagement.scala:116)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11$adapted(LibraryManagement.scala:97)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.util.Tracked$.$anonfun$inputChangedW$1(Tracked.scala:219)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.internal.LibraryManagement$.cachedUpdate(LibraryManagement.scala:130)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.Classpaths$.$anonfun$updateTask0$5(Defaults.scala:3486)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:68)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:282)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:23)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.Execute.work(Execute.scala:291)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:282)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:64)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m2021.02.25 10:00:36 INFO  [error] 	at java.lang.Thread.run(Thread.java:748)[0m
[0m2021.02.25 10:00:36 INFO  [error] (update) sbt.librarymanagement.ResolveException: Error downloading org.apache.spark:spark-sql_2.13:2.4.7[0m
[0m2021.02.25 10:00:36 INFO  [error]   Not found[0m
[0m2021.02.25 10:00:36 INFO  [error]   Not found[0m
[0m2021.02.25 10:00:36 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.apache.spark/spark-sql_2.13/2.4.7/ivys/ivy.xml[0m
[0m2021.02.25 10:00:36 INFO  [error]   not found: https://repo1.maven.org/maven2/org/apache/spark/spark-sql_2.13/2.4.7/spark-sql_2.13-2.4.7.pom[0m
[0m2021.02.25 10:00:36 INFO  [error] Total time: 2 s, completed Feb 25, 2021 10:00:36 AM[0m
[0m2021.02.25 10:00:36 INFO  sbt bloopInstall exit: 1[0m
[0m2021.02.25 10:00:36 INFO  time: ran 'sbt bloopInstall' in 14s[0m
[0m2021.02.25 10:00:36 ERROR sbt command failed: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals1293261345922807990/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall[0m
[0m2021.02.25 10:00:36 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 10:00:36 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:00:36 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.02.25 10:00:36 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1519142608410377669/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1519142608410377669/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1519142608410377669/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 10:00:36 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:00:36 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4273260395827831530/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4273260395827831530/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4273260395827831530/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 10:00:36 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:00:36 INFO  time: Connected to build server in 0.18s[0m
[0m2021.02.25 10:00:36 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 10:00:36 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 10:00:36 INFO  time: indexed workspace in 0.49s[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.13.4"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7"
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.12.13"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7"
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.02.25 10:00:56 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals7221735879379284149/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 10:00:58 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
[0m2021.02.25 10:00:59 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.02.25 10:00:59 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.02.25 10:01:01 INFO  [info] loading settings for project scalas3read-build from metals.sbt ...[0m
[0m2021.02.25 10:01:01 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.02.25 10:01:04 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.02.25 10:01:04 INFO  [success] Total time: 3 s, completed Feb 25, 2021 10:01:04 AM[0m
[0m2021.02.25 10:01:06 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 10:01:06 INFO  [info] set current project to scalas3read (in build file:/home/delaneylekien/project3/scalas3read/)[0m
[0m2021.02.25 10:01:16 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 10:01:20 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 10:01:20 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 10:01:20 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 10:01:20 INFO  [success] Total time: 14 s, completed Feb 25, 2021 10:01:20 AM[0m
[0m2021.02.25 10:01:20 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 10:01:21 INFO  time: ran 'sbt bloopInstall' in 24s[0m
[0m2021.02.25 10:01:21 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 10:01:21 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:01:21 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.02.25 10:01:21 INFO  Attempting to connect to the build server...[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3229399939174104909/bsp.socket'...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.12.13.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.6/jline-2.14.6.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.12/jansi-1.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.12/1.0.6/scala-xml_2.12-1.0.6.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.12.13/scala-compiler-2.12.13.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.12.13/scala-library-2.12.13.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.12.13/scala-reflect-2.12.13.jar
[0m[32m[D][0m Resolving org.scalameta:semanticdb-scalac_2.12.13:4.4.10
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3229399939174104909/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3229399939174104909/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 10:01:22 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:01:22 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher948413817045220330/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher948413817045220330/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher948413817045220330/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 10:01:22 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:01:22 INFO  time: Connected to build server in 1.44s[0m
[0m2021.02.25 10:01:22 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 10:01:24 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
Feb 25, 2021 10:01:25 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 144
[0m2021.02.25 10:01:25 INFO  time: indexed workspace in 3.18s[0m
[0m2021.02.25 10:01:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:01:43 INFO  time: compiled root in 17s[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.12.13"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7"
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

Feb 25, 2021 10:02:55 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 161
Feb 25, 2021 10:02:56 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 167
[0m2021.02.25 10:03:14 WARN  no build target for: /home/delaneylekien/project3/scalas3read/project/plugins.sbt[0m
[0m2021.02.25 10:03:14 WARN  no build target for: /home/delaneylekien/project3/scalas3read/project/plugins.sbt[0m
[0m2021.02.25 10:03:14 INFO  skipping build import with status 'Installed'[0m
[0m2021.02.25 10:03:14 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m





[0m2021.02.25 10:03:16 WARN  no build target for: /home/delaneylekien/project3/scalas3read/project/plugins.sbt[0m
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.15.0")
[0m2021.02.25 10:03:18 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals1419569005514601549/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.15.0")
[0m2021.02.25 10:03:19 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
[0m2021.02.25 10:03:22 INFO  [info] loading settings for project scalas3read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 10:03:23 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project/project[0m
[0m2021.02.25 10:03:26 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.02.25 10:03:26 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.02.25 10:03:28 INFO  [success] Generated .bloop/scalas3read-build-build.json[0m
[0m2021.02.25 10:03:28 INFO  [success] Total time: 2 s, completed Feb 25, 2021 10:03:28 AM[0m
[0m2021.02.25 10:03:29 INFO  [info] loading settings for project scalas3read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 10:03:29 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.02.25 10:03:31 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.02.25 10:03:31 INFO  [success] Total time: 2 s, completed Feb 25, 2021 10:03:32 AM[0m
[0m2021.02.25 10:03:34 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 10:03:34 INFO  [info] set current project to scalas3read (in build file:/home/delaneylekien/project3/scalas3read/)[0m
[0m2021.02.25 10:03:35 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 10:03:35 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 10:03:35 INFO  [success] Total time: 1 s, completed Feb 25, 2021 10:03:35 AM[0m
[0m2021.02.25 10:03:35 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 10:03:36 INFO  time: ran 'sbt bloopInstall' in 18s[0m
[0m2021.02.25 10:03:36 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 10:03:36 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...[0m
2021.02.25 10:03:36 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
[0m2021.02.25 10:03:36 INFO  Attempting to connect to the build server...[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6188625698140699101/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6188625698140699101/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6188625698140699101/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 10:03:36 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:03:36 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.25 10:03:36 INFO  Attempting to connect to the build server...[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5081985281808447682/bsp.socket'...
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2155966218335962523/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5081985281808447682/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5081985281808447682/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 10:03:36 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2155966218335962523/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2155966218335962523/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 10:03:36 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:03:36 INFO  time: Connected to build server in 0.17s[0m
[0m2021.02.25 10:03:36 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 10:03:38 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 10:03:38 INFO  time: indexed workspace in 1.77s[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.15.0")
Feb 25, 2021 10:03:41 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 185
[0m2021.02.25 10:03:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:03:41 INFO  compiling scalas3read-build (1 scala source)[0m
[0m2021.02.25 10:03:41 INFO  time: compiled root in 0.69s[0m
[0m2021.02.25 10:03:43 INFO  time: compiled scalas3read-build in 2.07s[0m
[0m2021.02.25 10:04:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:04:41 INFO  time: compiled root in 1.27s[0m
Feb 25, 2021 10:05:45 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 338
Feb 25, 2021 10:05:45 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 340
[0m2021.02.25 10:05:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:05:47 INFO  time: compiled root in 1.05s[0m
[0m2021.02.25 10:06:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:06:07 INFO  time: compiled root in 1.57s[0m
[0m2021.02.25 10:10:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:10:38 INFO  time: compiled root in 1.26s[0m
[0m2021.02.25 10:10:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:10:56 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:24:51: stale bloop error: unclosed string literal
    val rddFromFile = spark.sparkContext.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz
                                                  ^[0m
[0m2021.02.25 10:10:56 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:24:51: stale bloop error: unclosed string literal
    val rddFromFile = spark.sparkContext.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz
                                                  ^[0m
[0m2021.02.25 10:10:56 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:25:1: stale bloop error: unclosed string literal
")
^[0m
[0m2021.02.25 10:10:56 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:30:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 10:10:56 INFO  time: compiled root in 0.21s[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._, _root_.sbtassembly.AssemblyPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin, _root_.sbtassembly.AssemblyPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.12.13"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7"
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._, _root_.sbtassembly.AssemblyPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin, _root_.sbtassembly.AssemblyPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.12.13"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7"
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.02.25 10:11:44 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals6372905161232671333/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 10:11:46 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
[0m2021.02.25 10:11:47 INFO  [info] loading settings for project scalas3read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 10:11:48 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project/project[0m
[0m2021.02.25 10:11:49 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.02.25 10:11:49 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.02.25 10:11:52 INFO  [success] Generated .bloop/scalas3read-build-build.json[0m
[0m2021.02.25 10:11:52 INFO  [success] Total time: 3 s, completed Feb 25, 2021 10:11:52 AM[0m
[0m2021.02.25 10:11:52 INFO  [info] loading settings for project scalas3read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 10:11:52 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.02.25 10:11:54 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.02.25 10:11:54 INFO  [success] Total time: 1 s, completed Feb 25, 2021 10:11:54 AM[0m
[0m2021.02.25 10:11:59 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 10:11:59 INFO  [info] set current project to scalas3read (in build file:/home/delaneylekien/project3/scalas3read/)[0m
[0m2021.02.25 10:12:03 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 10:12:03 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 10:12:04 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 10:12:04 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 10:12:04 INFO  [success] Total time: 5 s, completed Feb 25, 2021 10:12:04 AM[0m
[0m2021.02.25 10:12:04 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 10:12:05 INFO  time: ran 'sbt bloopInstall' in 20s[0m
[0m2021.02.25 10:12:05 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 10:12:05 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:12:05 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:12:05 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.02.25 10:12:05 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:12:05 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:12:05 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:12:05 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:12:05 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:12:05 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:12:05 INFO  time: Connected to build server in 0.38s[0m
[0m2021.02.25 10:12:05 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 10:12:07 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 10:12:07 INFO  time: indexed workspace in 1.24s[0m
[0m2021.02.25 10:12:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:12:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:24:51: stale bloop error: unclosed string literal
    val rddFromFile = spark.sparkContext.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz
                                                  ^[0m
[0m2021.02.25 10:12:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:24:51: stale bloop error: unclosed string literal
    val rddFromFile = spark.sparkContext.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz
                                                  ^[0m
[0m2021.02.25 10:12:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:25:1: stale bloop error: unclosed string literal
")
^[0m
[0m2021.02.25 10:12:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:30:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 10:12:09 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 10:12:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:24:51: stale bloop error: unclosed string literal
    val rddFromFile = spark.sparkContext.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz
                                                  ^[0m
[0m2021.02.25 10:12:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:25:1: stale bloop error: unclosed string literal
")
^[0m
[0m2021.02.25 10:12:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:30:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 10:12:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:24:51: stale bloop error: unclosed string literal
    val rddFromFile = spark.sparkContext.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz
                                                  ^[0m
[0m2021.02.25 10:12:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:25:1: stale bloop error: unclosed string literal
")
^[0m
[0m2021.02.25 10:12:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:30:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 10:12:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:12:29 INFO  time: compiled root in 1.16s[0m
[0m2021.02.25 10:13:36 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals8621900406446952243/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 10:13:38 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
[0m2021.02.25 10:13:39 INFO  [info] loading settings for project scalas3read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 10:13:39 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project/project[0m
[0m2021.02.25 10:13:41 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.02.25 10:13:41 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.02.25 10:13:44 INFO  [success] Generated .bloop/scalas3read-build-build.json[0m
[0m2021.02.25 10:13:44 INFO  [success] Total time: 3 s, completed Feb 25, 2021 10:13:44 AM[0m
[0m2021.02.25 10:13:44 INFO  [info] loading settings for project scalas3read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 10:13:44 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.02.25 10:13:45 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.02.25 10:13:45 INFO  [success] Total time: 1 s, completed Feb 25, 2021 10:13:46 AM[0m
[0m2021.02.25 10:13:48 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 10:13:48 INFO  [info] set current project to scalas3read (in build file:/home/delaneylekien/project3/scalas3read/)[0m
[0m2021.02.25 10:13:50 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 10:13:50 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 10:13:50 INFO  [success] Total time: 1 s, completed Feb 25, 2021 10:13:50 AM[0m
[0m2021.02.25 10:13:50 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 10:13:50 INFO  time: ran 'sbt bloopInstall' in 13s[0m
[0m2021.02.25 10:13:50 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 10:13:50 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:13:50 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:13:50 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:13:50 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:13:50 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:13:50 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:13:50 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:13:50 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:13:50 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:13:50 INFO  time: Connected to build server in 0.28s[0m
[0m2021.02.25 10:13:50 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 10:13:52 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 10:13:52 INFO  time: indexed workspace in 1.16s[0m
[0m2021.02.25 10:17:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:17:40 INFO  time: compiled root in 1.29s[0m
[0m2021.02.25 10:19:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:19:35 INFO  time: compiled root in 0.86s[0m
[0m2021.02.25 10:23:06 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals2021300844874581885/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 10:23:07 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_275)[0m
[0m2021.02.25 10:23:09 INFO  [info] loading settings for project scalas3read-build-build-build from metals.sbt ...[0m
[0m2021.02.25 10:23:09 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project/project[0m
[0m2021.02.25 10:23:11 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.02.25 10:23:11 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.02.25 10:23:14 INFO  [success] Generated .bloop/scalas3read-build-build.json[0m
[0m2021.02.25 10:23:14 INFO  [success] Total time: 3 s, completed Feb 25, 2021 10:23:14 AM[0m
[0m2021.02.25 10:23:14 INFO  [info] loading settings for project scalas3read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 10:23:14 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.02.25 10:23:15 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.02.25 10:23:15 INFO  [success] Total time: 2 s, completed Feb 25, 2021 10:23:16 AM[0m
[0m2021.02.25 10:23:18 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 10:23:18 INFO  [info] set current project to scalas3read (in build file:/home/delaneylekien/project3/scalas3read/)[0m
[0m2021.02.25 10:23:22 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 10:23:22 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 10:23:23 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 10:23:23 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 10:23:23 INFO  [success] Total time: 5 s, completed Feb 25, 2021 10:23:23 AM[0m
[0m2021.02.25 10:23:23 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 10:23:24 INFO  time: ran 'sbt bloopInstall' in 17s[0m
[0m2021.02.25 10:23:24 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 10:23:24 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:23:24 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:23:24 INFO  Shut down connection with build server.[0m
[0m2021.02.25 10:23:24 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:23:24 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:23:24 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:23:24 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 10:23:24 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:23:24 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 10:23:24 INFO  time: Connected to build server in 0.3s[0m
[0m2021.02.25 10:23:24 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 10:23:26 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 10:23:27 INFO  time: indexed workspace in 3.16s[0m
[0m2021.02.25 10:23:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:23:35 INFO  time: compiled root in 7.88s[0m
[0m2021.02.25 10:25:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:25:48 INFO  time: compiled root in 1.67s[0m
[0m2021.02.25 10:26:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:26:53 INFO  time: compiled root in 1.29s[0m
[0m2021.02.25 10:29:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:29:46 INFO  time: compiled root in 1.27s[0m
[0m2021.02.25 10:32:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:32:41 INFO  time: compiled root in 1.2s[0m
[0m2021.02.25 10:37:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:37:09 INFO  time: compiled root in 1.24s[0m
[0m2021.02.25 10:38:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 10:38:42 INFO  time: compiled root in 1.05s[0m
[0m2021.02.25 11:08:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:08:06 INFO  time: compiled root in 1.05s[0m
[0m2021.02.25 11:08:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:08:25 INFO  time: compiled root in 1.05s[0m
[0m2021.02.25 11:11:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:11:27 INFO  time: compiled root in 1.04s[0m
[0m2021.02.25 11:17:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:17:13 INFO  time: compiled root in 1.12s[0m
[0m2021.02.25 11:18:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:18:07 INFO  time: compiled root in 1.1s[0m
[0m2021.02.25 11:19:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:19:35 INFO  time: compiled root in 0.29s[0m
[0m2021.02.25 11:19:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:19:40 INFO  time: compiled root in 1.16s[0m
[0m2021.02.25 11:20:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:20:51 INFO  time: compiled root in 1.06s[0m
[0m2021.02.25 11:23:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:23:20 INFO  time: compiled root in 0.98s[0m
[0m2021.02.25 11:26:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:26:48 INFO  time: compiled root in 1.04s[0m
[0m2021.02.25 11:33:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:33:11 INFO  time: compiled root in 0.91s[0m
[0m2021.02.25 11:33:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:33:31 INFO  time: compiled root in 1.31s[0m
[0m2021.02.25 11:39:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:39:43 INFO  time: compiled root in 1.17s[0m
[0m2021.02.25 11:40:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:40:28 INFO  time: compiled root in 0.85s[0m
[0m2021.02.25 11:41:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:41:08 INFO  time: compiled root in 0.82s[0m
[0m2021.02.25 11:41:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:41:53 INFO  time: compiled root in 0.89s[0m
[0m2021.02.25 11:42:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:42:30 INFO  time: compiled root in 0.89s[0m
[0m2021.02.25 11:42:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:42:40 INFO  time: compiled root in 0.82s[0m
[0m2021.02.25 11:43:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:43:39 INFO  time: compiled root in 0.88s[0m
[0m2021.02.25 11:44:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:44:44 INFO  time: compiled root in 0.95s[0m
[0m2021.02.25 14:05:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:05:54 INFO  time: compiled root in 0.82s[0m
[0m2021.02.25 14:07:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:07:55 INFO  time: compiled root in 1.02s[0m
[0m2021.02.25 14:08:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:08:07 INFO  time: compiled root in 0.94s[0m
[0m2021.02.25 14:08:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:08:39 INFO  time: compiled root in 0.94s[0m
[0m2021.02.25 14:09:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:09:48 INFO  time: compiled root in 1.04s[0m
[0m2021.02.25 14:09:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:09:54 INFO  time: compiled root in 1.01s[0m
[0m2021.02.25 14:11:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:11:01 INFO  time: compiled root in 1.11s[0m
[0m2021.02.25 14:11:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:11:56 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 14:14:37 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: value show is not a member of Array[org.apache.spark.sql.Row]
    x.show()
    ^^^^^^[0m
[0m2021.02.25 14:14:37 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: value show is not a member of Array[org.apache.spark.sql.Row]
    x.show()
    ^^^^^^[0m
[0m2021.02.25 14:14:37 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: value show is not a member of Array[org.apache.spark.sql.Row]
    x.show()
    ^^^^^^[0m
[0m2021.02.25 14:14:37 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: value show is not a member of Array[org.apache.spark.sql.Row]
    x.show()
    ^^^^^^[0m
[0m2021.02.25 14:14:37 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: value show is not a member of Array[org.apache.spark.sql.Row]
    x.show()
    ^^^^^^[0m
[0m2021.02.25 14:14:37 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: value show is not a member of Array[org.apache.spark.sql.Row]
    x.show()
    ^^^^^^[0m
[0m2021.02.25 14:14:37 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: value show is not a member of Array[org.apache.spark.sql.Row]
    x.show()
    ^^^^^^[0m
[0m2021.02.25 14:14:37 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: value show is not a member of Array[org.apache.spark.sql.Row]
    x.show()
    ^^^^^^[0m
[0m2021.02.25 14:14:38 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: value show is not a member of Array[org.apache.spark.sql.Row]
    x.show()
    ^^^^^^[0m
[0m2021.02.25 14:14:38 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: value show is not a member of Array[org.apache.spark.sql.Row]
    x.show()
    ^^^^^^[0m
[0m2021.02.25 14:14:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:14:50 INFO  time: compiled root in 0.95s[0m
[0m2021.02.25 14:14:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:14:53 INFO  time: compiled root in 0.97s[0m
[0m2021.02.25 14:16:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:16:38 INFO  time: compiled root in 0.98s[0m
[0m2021.02.25 14:22:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:22:41 INFO  time: compiled root in 0.98s[0m
[0m2021.02.25 14:23:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:23:21 INFO  time: compiled root in 0.87s[0m
[0m2021.02.25 14:26:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:26:21 INFO  time: compiled root in 0.89s[0m
[0m2021.02.25 14:31:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:31:59 INFO  time: compiled root in 1.06s[0m
[0m2021.02.25 14:34:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:34:02 INFO  time: compiled root in 0.94s[0m
[0m2021.02.25 14:36:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:36:52 INFO  time: compiled root in 0.95s[0m
[0m2021.02.25 14:41:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:41:20 INFO  time: compiled root in 1.96s[0m
[0m2021.02.25 14:42:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:42:12 INFO  time: compiled root in 0.88s[0m
[0m2021.02.25 14:45:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:45:15 INFO  time: compiled root in 1.01s[0m
[0m2021.02.25 14:45:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:45:59 INFO  time: compiled root in 0.95s[0m
[0m2021.02.25 14:46:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:46:10 INFO  time: compiled root in 1.53s[0m
[0m2021.02.25 14:46:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:46:55 INFO  time: compiled root in 0.97s[0m
[0m2021.02.25 14:55:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:55:33 INFO  time: compiled root in 1.14s[0m
[0m2021.02.25 14:57:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:57:39 INFO  time: compiled root in 1.06s[0m
[0m2021.02.25 14:58:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:58:21 INFO  time: compiled root in 1.51s[0m
[0m2021.02.25 14:59:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:59:24 INFO  time: compiled root in 1.13s[0m
[0m2021.02.25 15:07:49 INFO  shutting down Metals[0m
[0m2021.02.25 15:07:49 INFO  Shut down connection with build server.[0m
[0m2021.02.25 15:07:49 INFO  Shut down connection with build server.[0m
[0m2021.02.25 15:07:49 INFO  Shut down connection with build server.[0m
[0m2021.02.25 15:08:16 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.53.2.[0m
[0m2021.02.25 15:08:17 INFO  time: initialize in 0.49s[0m
[0m2021.02.25 15:08:17 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2394198510101502197/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.25 15:08:17 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
[0m2021.02.25 15:08:17 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.02.25 15:08:20 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val schema = new StructType() 
        .add("Container", StringType, true)
        .add("Filename", StringType, true)
        .add("Envelope", StringType, true)
        .add("_corrupt_record", StringType, true)

    val rddFromFile = spark.read.schema(schema).json(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz"
        )
    
    // s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz
    // s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz

    rddFromFile.printSchema()
    rddFromFile.createOrReplaceTempView("data")

    val df = spark.sql("SELECT * FROM data").take(1000)

  }
}
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/delaneylekien/project3/scalas3read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2394198510101502197/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2394198510101502197/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 15:08:23 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val schema = new StructType() 
        .add("Container", StringType, true)
        .add("Filename", StringType, true)
        .add("Envelope", StringType, true)
        .add("_corrupt_record", StringType, true)

    val rddFromFile = spark.read.schema(schema).json(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz"
        )
    
    // s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz
    // s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz

    rddFromFile.printSchema()
    rddFromFile.createOrReplaceTempView("data")

    val df = spark.sql("SELECT * FROM data").take(1000)

  }
}
[0m2021.02.25 15:08:24 INFO  time: code lens generation in 6s[0m
[0m2021.02.25 15:08:23 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher855058264810556186/bsp.socket'...
[0m2021.02.25 15:08:23 INFO  Attempting to connect to the build server...Waiting for the bsp connection to come up...[0m

Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6680814045398584224/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher855058264810556186/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher855058264810556186/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6680814045398584224/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6680814045398584224/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 15:08:24 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 15:08:24 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 15:08:24 INFO  time: Connected to build server in 6.51s[0m
[0m2021.02.25 15:08:24 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 15:08:24 INFO  time: Imported build in 0.29s[0m
[0m2021.02.25 15:08:27 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 15:08:28 INFO  time: indexed workspace in 4.04s[0m
[0m2021.02.25 15:08:58 INFO  compiling root (1 scala source)[0m
Feb 25, 2021 3:08:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.InterruptedException
java.util.concurrent.CompletionException: java.lang.InterruptedException
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:673)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:42)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.pc.VirtualFileParams.checkCanceled(VirtualFileParams.java:25)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:76)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:75)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.toStream(Iterator.scala:1320)
	at scala.collection.AbstractIterator.toStream(Iterator.scala:1334)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)
	at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:134)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:40)
	at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:69)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.write(ReflectiveTypeAdapterFactory.java:125)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.write(ReflectiveTypeAdapterFactory.java:243)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:423)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:55)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at com.google.gson.Gson.toJson(Gson.java:648)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:145)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:140)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:59)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.lambda$handleRequest$1(RemoteEndpoint.java:281)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
	... 9 more

[0m2021.02.25 15:09:02 INFO  time: compiled root in 3.13s[0m
Feb 25, 2021 3:09:20 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 41
Feb 25, 2021 3:09:20 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 44
[0m2021.02.25 15:09:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:09:22 INFO  time: compiled root in 0.51s[0m
[0m2021.02.25 15:09:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:09:30 INFO  time: compiled root in 2.65s[0m
[0m2021.02.25 15:12:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:12:10 INFO  time: compiled root in 1.37s[0m
[0m2021.02.25 15:12:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:12:52 INFO  time: compiled root in 2.29s[0m
[0m2021.02.25 15:13:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:13:11 INFO  time: compiled root in 1.29s[0m
[0m2021.02.25 15:13:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:13:23 INFO  time: compiled root in 1.39s[0m
[0m2021.02.25 15:14:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:14:41 INFO  time: compiled root in 1.02s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.CharArrayWriter

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.commons.lang3.StringUtils

import org.apache.spark.TaskContext
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.api.java.function._
import org.apache.spark.api.python.{PythonRDD, SerDeUtil}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection
import org.apache.spark.sql.catalyst.json.{JacksonGenerator, JSONOptions}
import org.apache.spark.sql.catalyst.optimizer.CombineUnions
import org.apache.spark.sql.catalyst.parser.{ParseException, ParserUtils}
import org.apache.spark.sql.catalyst.plans._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, PartitioningCollection}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.arrow.{ArrowBatchStreamWriter, ArrowConverters}
import org.apache.spark.sql.execution.command._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.execution.python.EvaluatePython
import org.apache.spark.sql.execution.stat.StatFunctions
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.types._
import org.apache.spark.sql.util.SchemaUtils
import org.apache.spark.storage.StorageLevel
import org.apache.spark.unsafe.array.ByteArrayMethods
import org.apache.spark.unsafe.types.CalendarInterval
import org.apache.spark.util.Utils

private[sql] object Dataset {
  def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = {
    val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]])
    // Eagerly bind the encoder so we verify that the encoder matches the underlying
    // schema. The user will get an error if this is not the case.
    // optimization: it is guaranteed that [[InternalRow]] can be converted to [[Row]] so
    // do not do this check in that case. this check can be expensive since it requires running
    // the whole [[Analyzer]] to resolve the deserializer
    if (dataset.exprEnc.clsTag.runtimeClass != classOf[Row]) {
      dataset.deserializer
    }
    dataset
  }

  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {
    val qe = sparkSession.sessionState.executePlan(logicalPlan)
    qe.assertAnalyzed()
    new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))
  }
}

/**
 * A Dataset is a strongly typed collection of domain-specific objects that can be transformed
 * in parallel using functional or relational operations. Each Dataset also has an untyped view
 * called a `DataFrame`, which is a Dataset of [[Row]].
 *
 * Operations available on Datasets are divided into transformations and actions. Transformations
 * are the ones that produce new Datasets, and actions are the ones that trigger computation and
 * return results. Example transformations include map, filter, select, and aggregate (`groupBy`).
 * Example actions count, show, or writing data out to file systems.
 *
 * Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally,
 * a Dataset represents a logical plan that describes the computation required to produce the data.
 * When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
 * physical plan for efficient execution in a parallel and distributed manner. To explore the
 * logical plan as well as optimized physical plan, use the `explain` function.
 *
 * To efficiently support domain-specific objects, an [[Encoder]] is required. The encoder maps
 * the domain specific type `T` to Spark's internal type system. For example, given a class `Person`
 * with two fields, `name` (string) and `age` (int), an encoder is used to tell Spark to generate
 * code at runtime to serialize the `Person` object into a binary structure. This binary structure
 * often has much lower memory footprint as well as are optimized for efficiency in data processing
 * (e.g. in a columnar format). To understand the internal binary representation for data, use the
 * `schema` function.
 *
 * There are typically two ways to create a Dataset. The most common way is by pointing Spark
 * to some files on storage systems, using the `read` function available on a `SparkSession`.
 * {{{
 *   val people = spark.read.parquet("...").as[Person]  // Scala
 *   Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java
 * }}}
 *
 * Datasets can also be created through transformations available on existing Datasets. For example,
 * the following creates a new Dataset by applying a filter on the existing one:
 * {{{
 *   val names = people.map(_.name)  // in Scala; names is a Dataset[String]
 *   Dataset<String> names = people.map((Person p) -> p.name, Encoders.STRING));
 * }}}
 *
 * Dataset operations can also be untyped, through various domain-specific-language (DSL)
 * functions defined in: Dataset (this class), [[Column]], and [[functions]]. These operations
 * are very similar to the operations available in the data frame abstraction in R or Python.
 *
 * To select a column from the Dataset, use `apply` method in Scala and `col` in Java.
 * {{{
 *   val ageCol = people("age")  // in Scala
 *   Column ageCol = people.col("age"); // in Java
 * }}}
 *
 * Note that the [[Column]] type can also be manipulated through its various functions.
 * {{{
 *   // The following creates a new column that increases everybody's age by 10.
 *   people("age") + 10  // in Scala
 *   people.col("age").plus(10);  // in Java
 * }}}
 *
 * A more concrete example in Scala:
 * {{{
 *   // To create Dataset[Row] using SparkSession
 *   val people = spark.read.parquet("...")
 *   val department = spark.read.parquet("...")
 *
 *   people.filter("age > 30")
 *     .join(department, people("deptId") === department("id"))
 *     .groupBy(department("name"), people("gender"))
 *     .agg(avg(people("salary")), max(people("age")))
 * }}}
 *
 * and in Java:
 * {{{
 *   // To create Dataset<Row> using SparkSession
 *   Dataset<Row> people = spark.read().parquet("...");
 *   Dataset<Row> department = spark.read().parquet("...");
 *
 *   people.filter(people.col("age").gt(30))
 *     .join(department, people.col("deptId").equalTo(department.col("id")))
 *     .groupBy(department.col("name"), people.col("gender"))
 *     .agg(avg(people.col("salary")), max(people.col("age")));
 * }}}
 *
 * @groupname basic Basic Dataset functions
 * @groupname action Actions
 * @groupname untypedrel Untyped transformations
 * @groupname typedrel Typed transformations
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class Dataset[T] private[sql](
    @transient val sparkSession: SparkSession,
    @DeveloperApi @InterfaceStability.Unstable @transient val queryExecution: QueryExecution,
    encoder: Encoder[T])
  extends Serializable {

  queryExecution.assertAnalyzed()

  // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure
  // you wrap it with `withNewExecutionId` if this actions doesn't call other action.

  def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sparkSession, sparkSession.sessionState.executePlan(logicalPlan), encoder)
  }

  def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sqlContext.sparkSession, logicalPlan, encoder)
  }

  @transient private[sql] val logicalPlan: LogicalPlan = {
    // For various commands (like DDL) and queries with side effects, we force query execution
    // to happen right away to let these side effects take place eagerly.
    queryExecution.analyzed match {
      case c: Command =>
        LocalRelation(c.output, withAction("command", queryExecution)(_.executeCollect()))
      case u @ Union(children) if children.forall(_.isInstanceOf[Command]) =>
        LocalRelation(u.output, withAction("command", queryExecution)(_.executeCollect()))
      case _ =>
        queryExecution.analyzed
    }
  }

  /**
   * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the
   * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use
   * it when constructing new Dataset objects that have the same object type (that will be
   * possibly resolved to a different schema).
   */
  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)

  // The deserializer expression which can be used to build a projection and turn rows to objects
  // of type T, after collecting rows to the driver side.
  private lazy val deserializer =
    exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer).deserializer

  private implicit def classTag = exprEnc.clsTag

  // sqlContext must be val because a stable identifier is expected when you import implicits
  @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext

  private[sql] def resolve(colName: String): NamedExpression = {
    queryExecution.analyzed.resolveQuoted(colName, sparkSession.sessionState.analyzer.resolver)
      .getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
  }

  private[sql] def numericColumns: Seq[Expression] = {
    schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n =>
      queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get
    }
  }

  /**
   * Get rows represented in Sequence by specific truncate and vertical requirement.
   *
   * @param numRows Number of rows to return
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   */
  private[sql] def getRows(
      numRows: Int,
      truncate: Int): Seq[Seq[String]] = {
    val newDf = toDF()
    val castCols = newDf.logicalPlan.output.map { col =>
      // Since binary types in top-level schema fields have a specific format to print,
      // so we do not cast them to strings here.
      if (col.dataType == BinaryType) {
        Column(col)
      } else {
        Column(col).cast(StringType)
      }
    }
    val data = newDf.select(castCols: _*).take(numRows + 1)

    // For array values, replace Seq and Array with square brackets
    // For cells that are beyond `truncate` characters, replace it with the
    // first `truncate-3` and "..."
    schema.fieldNames.toSeq +: data.map { row =>
      row.toSeq.map { cell =>
        val str = cell match {
          case null => "null"
          case binary: Array[Byte] => binary.map("%02X".format(_)).mkString("[", " ", "]")
          case _ => cell.toString
        }
        if (truncate > 0 && str.length > truncate) {
          // do not show ellipses for strings shorter than 4 characters.
          if (truncate < 4) str.substring(0, truncate)
          else str.substring(0, truncate - 3) + "..."
        } else {
          str
        }
      }: Seq[String]
    }
  }

  /**
   * Compose the string representing rows for output
   *
   * @param _numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   */
  private[sql] def showString(
      _numRows: Int,
      truncate: Int = 20,
      vertical: Boolean = false): String = {
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data.
    val tmpRows = getRows(numRows, truncate)

    val hasMoreData = tmpRows.length - 1 > numRows
    val rows = tmpRows.take(numRows + 1)

    val sb = new StringBuilder
    val numCols = schema.fieldNames.length
    // We set a minimum column width at '3'
    val minimumColWidth = 3

    if (!vertical) {
      // Initialise the width of each column to a minimum value
      val colWidths = Array.fill(numCols)(minimumColWidth)

      // Compute the width of each column
      for (row <- rows) {
        for ((cell, i) <- row.zipWithIndex) {
          colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell))
        }
      }

      val paddedRows = rows.map { row =>
        row.zipWithIndex.map { case (cell, i) =>
          if (truncate > 0) {
            StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          } else {
            StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          }
        }
      }

      // Create SeparateLine
      val sep: String = colWidths.map("-" * _).addString(sb, "+", "+", "+\n").toString()

      // column names
      paddedRows.head.addString(sb, "|", "|", "|\n")
      sb.append(sep)

      // data
      paddedRows.tail.foreach(_.addString(sb, "|", "|", "|\n"))
      sb.append(sep)
    } else {
      // Extended display mode enabled
      val fieldNames = rows.head
      val dataRows = rows.tail

      // Compute the width of field name and data columns
      val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) =>
        math.max(curMax, Utils.stringHalfWidth(fieldName))
      }
      val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) =>
        math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max)
      }

      dataRows.zipWithIndex.foreach { case (row, i) =>
        // "+ 5" in size means a character length except for padded names and data
        val rowHeader = StringUtils.rightPad(
          s"-RECORD $i", fieldNameColWidth + dataColWidth + 5, "-")
        sb.append(rowHeader).append("\n")
        row.zipWithIndex.map { case (cell, j) =>
          val fieldName = StringUtils.rightPad(fieldNames(j),
            fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length)
          val data = StringUtils.rightPad(cell,
            dataColWidth - Utils.stringHalfWidth(cell) + cell.length)
          s" $fieldName | $data "
        }.addString(sb, "", "\n", "\n")
      }
    }

    // Print a footer
    if (vertical && rows.tail.isEmpty) {
      // In a vertical mode, print an empty row set explicitly
      sb.append("(0 rows)\n")
    } else if (hasMoreData) {
      // For Data that has more than "numRows" records
      val rowsString = if (numRows == 1) "row" else "rows"
      sb.append(s"only showing top $numRows $rowsString\n")
    }

    sb.toString()
  }

  override def toString: String = {
    try {
      val builder = new StringBuilder
      val fields = schema.take(2).map {
        case f => s"${f.name}: ${f.dataType.simpleString(2)}"
      }
      builder.append("[")
      builder.append(fields.mkString(", "))
      if (schema.length > 2) {
        if (schema.length - fields.size == 1) {
          builder.append(" ... 1 more field")
        } else {
          builder.append(" ... " + (schema.length - 2) + " more fields")
        }
      }
      builder.append("]").toString()
    } catch {
      case NonFatal(e) =>
        s"Invalid tree; ${e.getMessage}:\n$queryExecution"
    }
  }

  /**
   * Converts this strongly typed collection of data to generic Dataframe. In contrast to the
   * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]
   * objects that allow fields to be accessed by ordinal or name.
   *
   * @group basic
   * @since 1.6.0
   */
  // This is declared with parentheses to prevent the Scala compiler from treating
  // `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))

  /**
   * :: Experimental ::
   * Returns a new Dataset where each record has been mapped on to the specified type. The
   * method used to map columns depend on the type of `U`:
   *  - When `U` is a class, fields for the class will be mapped to columns of the same name
   *    (case sensitivity is determined by `spark.sql.caseSensitive`).
   *  - When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will
   *    be assigned to `_1`).
   *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the
   *    `DataFrame` will be used.
   *
   * If the schema of the Dataset does not match the desired `U` type, you can use `select`
   * along with `alias` or `as` to rearrange or rename as required.
   *
   * Note that `as[]` only changes the view of the data that is passed into typed operations,
   * such as `map()`, and does not eagerly project away any columns that are not present in
   * the specified class.
   *
   * @group basic
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan)

  /**
   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.
   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with
   * meaningful names. For example:
   * {{{
   *   val rdd: RDD[(Int, String)] = ...
   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`
   *   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
   * }}}
   *
   * @group basic
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def toDF(colNames: String*): DataFrame = {
    require(schema.size == colNames.size,
      "The number of columns doesn't match.\n" +
        s"Old column names (${schema.size}): " + schema.fields.map(_.name).mkString(", ") + "\n" +
        s"New column names (${colNames.size}): " + colNames.mkString(", "))

    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>
      Column(oldAttribute).as(newName)
    }
    select(newCols : _*)
  }

  /**
   * Returns the schema of this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def schema: StructType = queryExecution.analyzed.schema

  /**
   * Prints the schema to the console in a nice tree format.
   *
   * @group basic
   * @since 1.6.0
   */
  // scalastyle:off println
  def printSchema(): Unit = println(schema.treeString)
  // scalastyle:on println

  /**
   * Prints the plans (logical and physical) to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(extended: Boolean): Unit = {
    val explain = ExplainCommand(queryExecution.logical, extended = extended)
    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {
      // scalastyle:off println
      r => println(r.getString(0))
      // scalastyle:on println
    }
  }

  /**
   * Prints the physical plan to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(): Unit = explain(extended = false)

  /**
   * Returns all column names and their data types as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def dtypes: Array[(String, String)] = schema.fields.map { field =>
    (field.name, field.dataType.toString)
  }

  /**
   * Returns all column names as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def columns: Array[String] = schema.fields.map(_.name)

  /**
   * Returns true if the `collect` and `take` methods can be run locally
   * (without any Spark executors).
   *
   * @group basic
   * @since 1.6.0
   */
  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]

  /**
   * Returns true if the `Dataset` is empty.
   *
   * @group basic
   * @since 2.4.0
   */
  def isEmpty: Boolean = withAction("isEmpty", limit(1).groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0) == 0
  }

  /**
   * Returns true if this Dataset contains one or more sources that continuously
   * return data as it arrives. A Dataset that reads data from a streaming source
   * must be executed as a `StreamingQuery` using the `start()` method in
   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or
   * `collect()`, will throw an [[AnalysisException]] when there is a streaming
   * source present.
   *
   * @group streaming
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def isStreaming: Boolean = logicalPlan.isStreaming

  /**
   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true)

  /**
   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
   * logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true)

  /**
   * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be
   * used to truncate the logical plan of this Dataset, which is especially useful in iterative
   * algorithms where the plan may grow exponentially. Local checkpoints are written to executor
   * storage and despite potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false)

  /**
   * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. Local checkpoints are written to executor storage and despite
   * potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint(
    eager = eager,
    reliableCheckpoint = false
  )

  /**
   * Returns a checkpointed version of this Dataset.
   *
   * @param eager Whether to checkpoint this dataframe immediately
   * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the
   *                           checkpoint directory. If false creates a local checkpoint using
   *                           the caching subsystem
   */
  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = {
    val internalRdd = queryExecution.toRdd.map(_.copy())
    if (reliableCheckpoint) {
      internalRdd.checkpoint()
    } else {
      internalRdd.localCheckpoint()
    }

    if (eager) {
      internalRdd.count()
    }

    val physicalPlan = queryExecution.executedPlan

    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the
    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner
    // joins.
    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {
      partitioning match {
        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)
        case p => p
      }
    }

    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)

    Dataset.ofRows(
      sparkSession,
      LogicalRDD(
        logicalPlan.output,
        internalRdd,
        outputPartitioning,
        physicalPlan.outputOrdering,
        isStreaming
      )(sparkSession)).as[T]
  }

  /**
   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time
   * before which we assume no more late data is going to arrive.
   *
   * Spark will use this watermark for several purposes:
   *  - To know when a given time window aggregation can be finalized and thus can be emitted when
   *    using output modes that do not allow updates.
   *  - To minimize the amount of state that we need to keep for on-going aggregations,
   *    `mapGroupsWithState` and `dropDuplicates` operators.
   *
   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across
   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost
   *  of coordinating this value across partitions, the actual watermark used is only guaranteed
   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still
   *  process records that arrive more than `delayThreshold` late.
   *
   * @param eventTime the name of the column that contains the event time of the row.
   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest
   *                       record that has been processed in the form of an interval
   *                       (e.g. "1 minute" or "5 hours"). NOTE: This should not be negative.
   *
   * @group streaming
   * @since 2.1.0
   */
  @InterfaceStability.Evolving
  // We only accept an existing column name, not a derived column here as a watermark that is
  // defined on a derived column cannot referenced elsewhere in the plan.
  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {
    val parsedDelay =
      try {
        CalendarInterval.fromCaseInsensitiveString(delayThreshold)
      } catch {
        case e: IllegalArgumentException =>
          throw new AnalysisException(
            s"Unable to parse time delay '$delayThreshold'",
            cause = Some(e))
      }
    require(parsedDelay.milliseconds >= 0 && parsedDelay.months >= 0,
      s"delay threshold ($delayThreshold) should not be negative.")
    EliminateEventTimeWatermark(
      EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan))
  }

  /**
   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
   * and all cells will be aligned right. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   *
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int): Unit = show(numRows, truncate = true)

  /**
   * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters
   * will be truncated, and all cells will be aligned right.
   *
   * @group action
   * @since 1.6.0
   */
  def show(): Unit = show(20)

  /**
   * Displays the top 20 rows of Dataset in a tabular form.
   *
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *                 be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  def show(truncate: Boolean): Unit = show(20, truncate)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   * @param numRows Number of rows to show
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *              be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {
    println(showString(numRows, truncate = 20))
  } else {
    println(showString(numRows, truncate = 0))
  }

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * If `vertical` enabled, this command prints output rows vertically (one line per column value)?
   *
   * {{{
   * -RECORD 0-------------------
   *  year            | 1980
   *  month           | 12
   *  AVG('Adj Close) | 0.503218
   *  AVG('Adj Close) | 0.595103
   * -RECORD 1-------------------
   *  year            | 1981
   *  month           | 01
   *  AVG('Adj Close) | 0.523289
   *  AVG('Adj Close) | 0.570307
   * -RECORD 2-------------------
   *  year            | 1982
   *  month           | 02
   *  AVG('Adj Close) | 0.436504
   *  AVG('Adj Close) | 0.475256
   * -RECORD 3-------------------
   *  year            | 1983
   *  month           | 03
   *  AVG('Adj Close) | 0.410516
   *  AVG('Adj Close) | 0.442194
   * -RECORD 4-------------------
   *  year            | 1984
   *  month           | 04
   *  AVG('Adj Close) | 0.450090
   *  AVG('Adj Close) | 0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   * @group action
   * @since 2.3.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Int, vertical: Boolean): Unit =
    println(showString(numRows, truncate, vertical))
  // scalastyle:on println

  /**
   * Returns a [[DataFrameNaFunctions]] for working with missing data.
   * {{{
   *   // Dropping rows containing any null values.
   *   ds.na.drop()
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())

  /**
   * Returns a [[DataFrameStatFunctions]] for working statistic functions support.
   * {{{
   *   // Finding frequent items in column with name 'a'.
   *   ds.stat.freqItems(Seq("a"))
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())

  /**
   * Join with another `DataFrame`.
   *
   * Behaves as an INNER JOIN and requires a subsequent join predicate.
   *
   * @param right Right side of the join operation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)
  }

  /**
   * Inner equi-join with another `DataFrame` using the given column.
   *
   * Different from other join functions, the join column will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the column "user_id"
   *   df1.join(df2, "user_id")
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumn Name of the column to join on. This column must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumn: String): DataFrame = {
    join(right, Seq(usingColumn))
  }

  /**
   * Inner equi-join with another `DataFrame` using the given columns.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the columns "user_id" and "user_name"
   *   df1.join(df2, Seq("user_id", "user_name"))
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {
    join(right, usingColumns, "inner")
  }

  /**
   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate
   * is specified as an inner join. If you would explicitly like to perform a cross join use the
   * `crossJoin` method.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {
    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right
    // by creating a new instance for one of the branch.
    val joined = sparkSession.sessionState.executePlan(
      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))
      .analyzed.asInstanceOf[Join]

    withPlan {
      Join(
        joined.left,
        joined.right,
        UsingJoin(JoinType(joinType), usingColumns),
        None)
    }
  }

  /**
   * Inner join with another `DataFrame`, using the given join expression.
   *
   * {{{
   *   // The following two are equivalent:
   *   df1.join(df2, $"df1Key" === $"df2Key")
   *   df1.join(df2).where($"df1Key" === $"df2Key")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, "inner")

  /**
   * Join with another `DataFrame`, using the given join expression. The following performs
   * a full outer join between `df1` and `df2`.
   *
   * {{{
   *   // Scala:
   *   import org.apache.spark.sql.functions._
   *   df1.join(df2, $"df1Key" === $"df2Key", "outer")
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
   * }}}
   *
   * @param right Right side of the join.
   * @param joinExprs Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {
    // Note that in this function, we introduce a hack in the case of self-join to automatically
    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].
    // Consider this case: df.join(df, df("key") === df("key"))
    // Since df("key") === df("key") is a trivially true condition, this actually becomes a
    // cartesian join. However, most likely users expect to perform a self join using "key".
    // With that assumption, this hack turns the trivially true condition into equality on join
    // keys that are resolved to both sides.

    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.
    // After the cloning, left and right side will have distinct expression ids.
    val plan = withPlan(
      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))
      .queryExecution.analyzed.asInstanceOf[Join]

    // If auto self join alias is disabled, return the plan.
    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {
      return withPlan(plan)
    }

    // If left/right have no output set intersection, return the plan.
    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed
    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed
    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {
      return withPlan(plan)
    }

    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.
    // By the time we get here, since we have already run analysis, all attributes should've been
    // resolved and become AttributeReference.
    val cond = plan.condition.map { _.transform {
      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)
          if a.sameRef(b) =>
        catalyst.expressions.EqualTo(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
      case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference)
        if a.sameRef(b) =>
        catalyst.expressions.EqualNullSafe(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
    }}

    withPlan {
      plan.copy(condition = cond)
    }
  }

  /**
   * Explicit cartesian join with another `DataFrame`.
   *
   * @param right Right side of the join operation.
   *
   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.
   *
   * @group untypedrel
   * @since 2.1.0
   */
  def crossJoin(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)
  }

  /**
   * :: Experimental ::
   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to
   * true.
   *
   * This is similar to the relation `join` function with one important difference in the
   * result schema. Since `joinWith` preserves objects present on either side of the join, the
   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.
   *
   * This type of join can be useful both for preserving type-safety with the original object
   * types as well as working with relational data where either side of the join has column
   * names in common.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {
    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,
    // etc.
    val joined = sparkSession.sessionState.executePlan(
      Join(
        this.logicalPlan,
        other.logicalPlan,
        JoinType(joinType),
        Some(condition.expr))).analyzed.asInstanceOf[Join]

    if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) {
      throw new AnalysisException("Invalid join type in joinWith: " + joined.joinType.sql)
    }

    // For both join side, combine all outputs into a single column and alias it with "_1" or "_2",
    // to match the schema for the encoder of the join result.
    // Note that we do this before joining them, to enable the join operator to return null for one
    // side, in cases like outer-join.
    val left = {
      val combined = if (this.exprEnc.flat) {
        assert(joined.left.output.length == 1)
        Alias(joined.left.output.head, "_1")()
      } else {
        Alias(CreateStruct(joined.left.output), "_1")()
      }
      Project(combined :: Nil, joined.left)
    }

    val right = {
      val combined = if (other.exprEnc.flat) {
        assert(joined.right.output.length == 1)
        Alias(joined.right.output.head, "_2")()
      } else {
        Alias(CreateStruct(joined.right.output), "_2")()
      }
      Project(combined :: Nil, joined.right)
    }

    // Rewrites the join condition to make the attribute point to correct column/field, after we
    // combine the outputs of each join side.
    val conditionExpr = joined.condition.get transformUp {
      case a: Attribute if joined.left.outputSet.contains(a) =>
        if (this.exprEnc.flat) {
          left.output.head
        } else {
          val index = joined.left.output.indexWhere(_.exprId == a.exprId)
          GetStructField(left.output.head, index)
        }
      case a: Attribute if joined.right.outputSet.contains(a) =>
        if (other.exprEnc.flat) {
          right.output.head
        } else {
          val index = joined.right.output.indexWhere(_.exprId == a.exprId)
          GetStructField(right.output.head, index)
        }
    }

    implicit val tuple2Encoder: Encoder[(T, U)] =
      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)

    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))
  }

  /**
   * :: Experimental ::
   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair
   * where `condition` evaluates to true.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {
    joinWith(other, condition, "inner")
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {
    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = false, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the specified column, all in ascending order.
   * {{{
   *   // The following 3 are equivalent
   *   ds.sort("sortcol")
   *   ds.sort($"sortcol")
   *   ds.sort($"sortcol".asc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortCol: String, sortCols: String*): Dataset[T] = {
    sort((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset sorted by the given expressions. For example:
   * {{{
   *   ds.sort($"col1", $"col2".desc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = true, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def apply(colName: String): Column = col(colName)

  /**
   * Specifies some hint on the current Dataset. As an example, the following code specifies
   * that one of the plan can be broadcasted:
   *
   * {{{
   *   df1.join(df2.hint("broadcast"))
   * }}}
   *
   * @group basic
   * @since 2.2.0
   */
  @scala.annotation.varargs
  def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan {
    UnresolvedHint(name, parameters, logicalPlan)
  }

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def col(colName: String): Column = colName match {
    case "*" =>
      Column(ResolvedStar(queryExecution.analyzed.output))
    case _ =>
      if (sqlContext.conf.supportQuotedRegexColumnName) {
        colRegex(colName)
      } else {
        val expr = resolve(colName)
        Column(expr)
      }
  }

  /**
   * Selects column based on the column name specified as a regex and returns it as [[Column]].
   * @group untypedrel
   * @since 2.3.0
   */
  def colRegex(colName: String): Column = {
    val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis
    colName match {
      case ParserUtils.escapedIdentifier(columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, None, caseSensitive))
      case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive))
      case _ =>
        Column(resolve(colName))
    }
  }

  /**
   * Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def as(alias: String): Dataset[T] = withTypedPlan {
    SubqueryAlias(alias, logicalPlan)
  }

  /**
   * (Scala-specific) Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def as(alias: Symbol): Dataset[T] = as(alias.name)

  /**
   * Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: String): Dataset[T] = as(alias)

  /**
   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: Symbol): Dataset[T] = as(alias)

  /**
   * Selects a set of column based expressions.
   * {{{
   *   ds.select($"colA", $"colB" + 1)
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(cols: Column*): DataFrame = withPlan {
    Project(cols.map(_.named), logicalPlan)
  }

  /**
   * Selects a set of columns. This is a variant of `select` that can only select
   * existing columns using column names (i.e. cannot construct expressions).
   *
   * {{{
   *   // The following two are equivalent:
   *   ds.select("colA", "colB")
   *   ds.select($"colA", $"colB")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)

  /**
   * Selects a set of SQL expressions. This is a variant of `select` that accepts
   * SQL expressions.
   *
   * {{{
   *   // The following are equivalent:
   *   ds.selectExpr("colA", "colB as newName", "abs(colC)")
   *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def selectExpr(exprs: String*): DataFrame = {
    select(exprs.map { expr =>
      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))
    }: _*)
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expression for each element.
   *
   * {{{
   *   val ds = Seq(1, 2, 3).toDS()
   *   val newDS = ds.select(expr("value + 1").as[Int])
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {
    implicit val encoder = c1.encoder
    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan)

    if (encoder.flat) {
      new Dataset[U1](sparkSession, project, encoder)
    } else {
      // Flattens inner fields of U1
      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)
    }
  }

  /**
   * Internal helper function for building typed selects that return tuples. For simplicity and
   * code reuse, we do this without the help of the type system and then use helper functions
   * that cast appropriately for the user facing interface.
   */
  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {
    val encoders = columns.map(_.encoder)
    val namedColumns =
      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)
    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))
    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =
    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =
    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =
    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4, U5](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4],
      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =
    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]

  /**
   * Filters rows using the given condition.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(condition: Column): Dataset[T] = withTypedPlan {
    Filter(condition.expr, logicalPlan)
  }

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.filter("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Filters rows using the given condition. This is an alias for `filter`.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(condition: Column): Dataset[T] = filter(condition)

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.where("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Groups the Dataset using the specified columns, so we can run aggregation on them. See
   * [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy($"department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)
  }

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)
  }

  /**
   * Groups the Dataset using the specified columns, so that we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of groupBy that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy("department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: (T, T) => T): T = withNewRDDExecutionId {
    rdd.reduce(func)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {
    val withGroupingKey = AppendColumns(func, logicalPlan)
    val executed = sparkSession.sessionState.executePlan(withGroupingKey)

    new KeyValueGroupedDataset(
      encoderFor[K],
      encoderFor[T],
      executed,
      logicalPlan.output,
      withGroupingKey.newColumns)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =
    groupByKey(func.call(_))(encoder)

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of rollup that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup("department", "group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of cube that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube("department", "group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg("age" -> "max", "salary" -> "avg")
   *   ds.groupBy().agg("age" -> "max", "salary" -> "avg")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
    groupBy().agg(aggExpr, aggExprs : _*)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * (Java-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(max($"age"), avg($"salary"))
   *   ds.groupBy().agg(max($"age"), avg($"salary"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)

  /**
   * Returns a new Dataset by taking the first `n` rows. The difference between this function
   * and `head` is that `head` is an action and returns an array (by triggering query execution)
   * while `limit` returns a new Dataset.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def limit(n: Int): Dataset[T] = withTypedPlan {
    Limit(Literal(n), logicalPlan)
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @deprecated("use union()", "2.0.0")
  def unionAll(other: Dataset[T]): Dataset[T] = union(other)

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.union(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   4|   5|   6|
   *   // +----+----+----+
   * }}}
   *
   * Notice that the column positions in the schema aren't necessarily matched with the
   * fields in the strongly typed objects in a Dataset. This function resolves columns
   * by their positions in the schema, not the fields in the strongly typed objects. Use
   * [[unionByName]] to resolve columns by field name in the typed objects.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def union(other: Dataset[T]): Dataset[T] = withSetOperator {
    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, other.logicalPlan))
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set
   * union (that does deduplication of elements), use this function followed by a [[distinct]].
   *
   * The difference between this function and [[union]] is that this function
   * resolves columns by name (not by position):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.unionByName(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   6|   4|   5|
   *   // +----+----+----+
   * }}}
   *
   * @group typedrel
   * @since 2.3.0
   */
  def unionByName(other: Dataset[T]): Dataset[T] = withSetOperator {
    // Check column name duplication
    val resolver = sparkSession.sessionState.analyzer.resolver
    val leftOutputAttrs = logicalPlan.output
    val rightOutputAttrs = other.logicalPlan.output

    SchemaUtils.checkColumnNameDuplication(
      leftOutputAttrs.map(_.name),
      "in the left attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)
    SchemaUtils.checkColumnNameDuplication(
      rightOutputAttrs.map(_.name),
      "in the right attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    // Builds a project list for `other` based on `logicalPlan` output names
    val rightProjectList = leftOutputAttrs.map { lattr =>
      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "${lattr.name}" among """ +
            s"""(${rightOutputAttrs.map(_.name).mkString(", ")})""")
      }
    }

    // Delegates failure checks to `CheckAnalysis`
    val notFoundAttrs = rightOutputAttrs.diff(rightProjectList)
    val rightChild = Project(rightProjectList ++ notFoundAttrs, other.logicalPlan)

    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, rightChild))
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.
   * This is equivalent to `INTERSECT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset while
   * preserving the duplicates.
   * This is equivalent to `INTERSECT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard
   * in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = true)
  }


  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.
   * This is equivalent to `EXCEPT DISTINCT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def except(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset while
   * preserving the duplicates.
   * This is equivalent to `EXCEPT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in
   * SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = true)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a user-supplied seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double, seed: Long): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction, seed = seed)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a random seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {
    withTypedPlan {
      Sample(0.0, fraction, withReplacement, seed, logicalPlan)
    }
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the total count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {
    sample(withReplacement, fraction, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * For Java API, use [[randomSplitAsList]].
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its
    // constituent partitions each time a split is materialized which could result in
    // overlapping splits. To prevent this, we explicitly sort each input partition to make the
    // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out
    // from the sort order.
    val sortOrder = logicalPlan.output
      .filter(attr => RowOrdering.isOrderable(attr.dataType))
      .map(SortOrder(_, Ascending))
    val plan = if (sortOrder.nonEmpty) {
      Sort(sortOrder, global = false, logicalPlan)
    } else {
      // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism
      cache()
      logicalPlan
    }
    val sum = weights.sum
    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
    normalizedCumWeights.sliding(2).map { x =>
      new Dataset[T](
        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder)
    }.toArray
  }

  /**
   * Returns a Java list that contains randomly split Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {
    val values = randomSplit(weights, seed)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {
    randomSplit(weights, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   */
  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {
    randomSplit(weights.toArray, seed)
  }

  /**
   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of
   * the input row are implicitly joined with each row that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count
   * the number of books that contain a given word:
   *
   * {{{
   *   case class Book(title: String, words: String)
   *   val ds: Dataset[Book]
   *
   *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))
   *
   *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
   * }}}
   *
   * Using `flatMap()` this can similarly be exploded as:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {
    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]

    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)

    val rowFunction =
      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))
    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero
   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All
   * columns of the input row are implicitly joined with each value that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()`:
   *
   * {{{
   *   ds.select(explode(split('words, " ")).as("word"))
   * }}}
   *
   * or `flatMap()`:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])
    : DataFrame = {
    val dataType = ScalaReflection.schemaFor[B].dataType
    val attributes = AttributeReference(outputColumn, dataType)() :: Nil
    // TODO handle the metadata?
    val elementSchema = attributes.toStructType

    def rowFunction(row: Row): TraversableOnce[InternalRow] = {
      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)
      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))
    }
    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * Returns a new Dataset by adding a column or replacing the existing column that has
   * the same name.
   *
   * `column`'s expression must only refer to attributes supplied by this Dataset. It is an
   * error to add a column that refers to some other Dataset.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col))

  /**
   * Returns a new Dataset by adding columns or replacing the existing columns that has
   * the same names.
   */
  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
    require(colNames.size == cols.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of columns: ${cols.size}")
    SchemaUtils.checkColumnNameDuplication(
      colNames,
      "in given column names",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output

    val columnMap = colNames.zip(cols).toMap

    val replacedAndExistingColumns = output.map { field =>
      columnMap.find { case (colName, _) =>
        resolver(field.name, colName)
      } match {
        case Some((colName: String, col: Column)) => col.as(colName)
        case _ => Column(field)
      }
    }

    val newColumns = columnMap.filter { case (colName, col) =>
      !output.exists(f => resolver(f.name, colName))
    }.map { case (colName, col) => col.as(colName) }

    select(replacedAndExistingColumns ++ newColumns : _*)
  }

  /**
   * Returns a new Dataset by adding columns with metadata.
   */
  private[spark] def withColumns(
      colNames: Seq[String],
      cols: Seq[Column],
      metadata: Seq[Metadata]): DataFrame = {
    require(colNames.size == metadata.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of metadata elements: ${metadata.size}")
    val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) =>
      col.as(colName, metadata)
    }
    withColumns(colNames, newCols)
  }

  /**
   * Returns a new Dataset by adding a column with metadata.
   */
  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame =
    withColumns(Seq(colName), Seq(col), Seq(metadata))

  /**
   * Returns a new Dataset with a column renamed.
   * This is a no-op if schema doesn't contain existingName.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumnRenamed(existingName: String, newName: String): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output
    val shouldRename = output.exists(f => resolver(f.name, existingName))
    if (shouldRename) {
      val columns = output.map { col =>
        if (resolver(col.name, existingName)) {
          Column(col).as(newName)
        } else {
          Column(col)
        }
      }
      select(columns : _*)
    } else {
      toDF()
    }
  }

  /**
   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain
   * column name.
   *
   * This method can only be used to drop top level columns. the colName string is treated
   * literally without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(colName: String): DataFrame = {
    drop(Seq(colName) : _*)
  }

  /**
   * Returns a new Dataset with columns dropped.
   * This is a no-op if schema doesn't contain column name(s).
   *
   * This method can only be used to drop top level columns. the colName string is treated literally
   * without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def drop(colNames: String*): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val remainingCols = allColumns.filter { attribute =>
      colNames.forall(n => !resolver(attribute.name, n))
    }.map(attribute => Column(attribute))
    if (remainingCols.size == allColumns.size) {
      toDF()
    } else {
      this.select(remainingCols: _*)
    }
  }

  /**
   * Returns a new Dataset with a column dropped.
   * This version of drop accepts a [[Column]] rather than a name.
   * This is a no-op if the Dataset doesn't have a column
   * with an equivalent expression.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(col: Column): DataFrame = {
    val expression = col match {
      case Column(u: UnresolvedAttribute) =>
        queryExecution.analyzed.resolveQuoted(
          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)
      case Column(expr: Expression) => expr
    }
    val attrs = this.logicalPlan.output
    val colsAfterDrop = attrs.filter { attr =>
      attr != expression
    }.map(attr => Column(attr))
    select(colsAfterDrop : _*)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `distinct`.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)

  /**
   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>
      // It is possibly there are more than one columns with the same name,
      // so we call filter instead of find.
      val cols = allColumns.filter(col => resolver(col.name, colName))
      if (cols.isEmpty) {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
      cols
    }
    Deduplicate(groupCols, logicalPlan)
  }

  /**
   * Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)

  /**
   * Returns a new [[Dataset]] with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {
    val colNames: Seq[String] = col1 +: cols
    dropDuplicates(colNames)
  }

  /**
   * Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
   * and max. If no columns are given, this function computes statistics for all numerical or
   * string columns.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.describe("age", "height").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // max     92.0  192.0
   * }}}
   *
   * Use [[summary]] for expanded statistics and control over which statistics to compute.
   *
   * @param cols Columns to compute statistics on.
   *
   * @group action
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def describe(cols: String*): DataFrame = {
    val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*)
    selected.summary("count", "mean", "stddev", "min", "max")
  }

  /**
   * Computes specified statistics for numeric and string columns. Available statistics are:
   *
   * - count
   * - mean
   * - stddev
   * - min
   * - max
   * - arbitrary approximate percentiles specified as a percentage (eg, 75%)
   *
   * If no statistics are given, this function computes count, mean, stddev, min,
   * approximate quartiles (percentiles at 25%, 50%, and 75%), and max.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.summary().show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 50%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * {{{
   *   ds.summary("count", "min", "25%", "75%", "max").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * To do a summary for specific columns first select them:
   *
   * {{{
   *   ds.select("age", "height").summary().show()
   * }}}
   *
   * See also [[describe]] for basic statistics.
   *
   * @param statistics Statistics from above list to be computed.
   *
   * @group action
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq)

  /**
   * Returns the first `n` rows.
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @group action
   * @since 1.6.0
   */
  def head(n: Int): Array[T] = withAction("head", limit(n).queryExecution)(collectFromPlan)

  /**
   * Returns the first row.
   * @group action
   * @since 1.6.0
   */
  def head(): T = head(1).head

  /**
   * Returns the first row. Alias for head().
   * @group action
   * @since 1.6.0
   */
  def first(): T = head()

  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: Dataset[T]): Dataset[U] = ...
   *
   *   ds
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: T => Boolean): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: FilterFunction[T]): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {
    MapElements[T, U](func, logicalPlan)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    implicit val uEnc = encoder
    withTypedPlan(MapElements[T, U](func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {
    new Dataset[U](
      sparkSession,
      MapPartitions[T, U](func, logicalPlan),
      implicitly[Encoder[U]])
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `f` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala
    mapPartitions(func)(encoder)
  }

  /**
   * Returns a new `DataFrame` that contains the result of applying a serialized R function
   * `func` to each partition.
   */
  private[sql] def mapPartitionsInR(
      func: Array[Byte],
      packageNames: Array[Byte],
      broadcastVars: Array[Broadcast[Object]],
      schema: StructType): DataFrame = {
    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]
    Dataset.ofRows(
      sparkSession,
      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =
    mapPartitions(_.flatMap(func))

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (T) => Iterator[U] = x => f.call(x).asScala
    flatMap(func)(encoder)
  }

  /**
   * Applies a function `f` to all rows.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(f: T => Unit): Unit = withNewRDDExecutionId {
    rdd.foreach(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each element of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))

  /**
   * Applies a function `f` to each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId {
    rdd.foreachPartition(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = {
    foreachPartition((it: Iterator[T]) => func.call(it.asJava))
  }

  /**
   * Returns the first `n` rows in the Dataset.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def take(n: Int): Array[T] = head(n)

  /**
   * Returns the first `n` rows in the Dataset as a list.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)

  /**
   * Returns an array that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * For Java API, use [[collectAsList]].
   *
   * @group action
   * @since 1.6.0
   */
  def collect(): Array[T] = withAction("collect", queryExecution)(collectFromPlan)

  /**
   * Returns a Java list that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def collectAsList(): java.util.List[T] = withAction("collectAsList", queryExecution) { plan =>
    val values = collectFromPlan(plan)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Returns an iterator that contains all rows in this Dataset.
   *
   * The iterator will consume as much memory as the largest partition in this Dataset.
   *
   * @note this results in multiple Spark jobs, and if the input Dataset is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input Dataset should be cached first.
   *
   * @group action
   * @since 2.0.0
   */
  def toLocalIterator(): java.util.Iterator[T] = {
    withAction("toLocalIterator", queryExecution) { plan =>
      // This projection writes output to a `InternalRow`, which means applying this projection is
      // not thread-safe. Here we create the projection inside this method to make `Dataset`
      // thread-safe.
      val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
      plan.executeToIterator().map { row =>
        // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
        // parameter of its `get` method, so it's safe to use null here.
        objProj(row).get(0, null).asInstanceOf[T]
      }.asJava
    }
  }

  /**
   * Returns the number of rows in the Dataset.
   * @group action
   * @since 1.6.0
   */
  def count(): Long = withAction("count", groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = true, logicalPlan)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments.
    // However, we don't want to complicate the semantics of this API method.
    // Instead, let's give users a friendly error message, pointing them to the new method.
    val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder])
    if (sortOrders.nonEmpty) throw new IllegalArgumentException(
      s"""Invalid partitionExprs specified: $sortOrders
         |For range partitioning use repartitionByRange(...) instead.
       """.stripMargin)
    withTypedPlan {
      RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(partitionExprs: Column*): Dataset[T] = {
    repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    require(partitionExprs.nonEmpty, "At least one partition-by expression must be specified.")
    val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match {
      case expr: SortOrder => expr
      case expr: Expression => SortOrder(expr, Ascending)
    })
    withTypedPlan {
      RepartitionByExpression(sortOrder, logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(partitionExprs: Column*): Dataset[T] = {
    repartitionByRange(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions
   * are requested. If a larger number of partitions is requested, it will stay at the current
   * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in
   * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not
   * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can call repartition. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @group typedrel
   * @since 1.6.0
   */
  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = false, logicalPlan)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `dropDuplicates`.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def distinct(): Dataset[T] = dropDuplicates()

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this)
    this
  }

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def cache(): this.type = persist()

  /**
   * Persist this Dataset with the given storage level.
   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,
   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,
   *                 `MEMORY_AND_DISK_2`, etc.
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(newLevel: StorageLevel): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)
    this
  }

  /**
   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
   *
   * @group basic
   * @since 2.1.0
   */
  def storageLevel: StorageLevel = {
    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>
      cachedData.cachedRepresentation.cacheBuilder.storageLevel
    }.getOrElse(StorageLevel.NONE)
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @param blocking Whether to block until all blocks are deleted.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(blocking: Boolean): this.type = {
    sparkSession.sharedState.cacheManager.uncacheQuery(this, cascade = false, blocking)
    this
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(): this.type = unpersist(blocking = false)

  // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`.
  @transient private lazy val rddQueryExecution: QueryExecution = {
    val deserialized = CatalystSerde.deserialize[T](logicalPlan)
    sparkSession.sessionState.executePlan(deserialized)
  }

  /**
   * Represents the content of the Dataset as an `RDD` of `T`.
   *
   * @group basic
   * @since 1.6.0
   */
  lazy val rdd: RDD[T] = {
    val objectType = exprEnc.deserializer.dataType
    rddQueryExecution.toRdd.mapPartitions { rows =>
      rows.map(_.get(0, objectType).asInstanceOf[T])
    }
  }

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def javaRDD: JavaRDD[T] = toJavaRDD

  /**
   * Registers this Dataset as a temporary table using the given name. The lifetime of this
   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  @deprecated("Use createOrReplaceTempView(viewName) instead.", "2.0.0")
  def registerTempTable(tableName: String): Unit = {
    createOrReplaceTempView(tableName)
  }

  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
   * created it, i.e. it will be automatically dropped when the session terminates. It's not
   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.0.0
   */
  @throws[AnalysisException]
  def createTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = false)
  }



  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 2.0.0
   */
  def createOrReplaceTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = false)
  }

  /**
   * Creates a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.1.0
   */
  @throws[AnalysisException]
  def createGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = true)
  }

  /**
   * Creates or replaces a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @group basic
   * @since 2.2.0
   */
  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = true)
  }

  private def createTempViewCommand(
      viewName: String,
      replace: Boolean,
      global: Boolean): CreateViewCommand = {
    val viewType = if (global) GlobalTempView else LocalTempView

    val tableIdentifier = try {
      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)
    } catch {
      case _: ParseException => throw new AnalysisException(s"Invalid view name: $viewName")
    }
    CreateViewCommand(
      name = tableIdentifier,
      userSpecifiedColumns = Nil,
      comment = None,
      properties = Map.empty,
      originalText = None,
      child = logicalPlan,
      allowExisting = false,
      replace = replace,
      viewType = viewType)
  }

  /**
   * Interface for saving the content of the non-streaming Dataset out into external storage.
   *
   * @group basic
   * @since 1.6.0
   */
  def write: DataFrameWriter[T] = {
    if (isStreaming) {
      logicalPlan.failAnalysis(
        "'write' can not be called on streaming Dataset/DataFrame")
    }
    new DataFrameWriter[T](this)
  }

  /**
   * Interface for saving the content of the streaming Dataset out into external storage.
   *
   * @group basic
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def writeStream: DataStreamWriter[T] = {
    if (!isStreaming) {
      logicalPlan.failAnalysis(
        "'writeStream' can be called only on streaming Dataset/DataFrame")
    }
    new DataStreamWriter[T](this)
  }


  /**
   * Returns the content of the Dataset as a Dataset of JSON strings.
   * @since 2.0.0
   */
  def toJSON: Dataset[String] = {
    val rowSchema = this.schema
    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone
    mapPartitions { iter =>
      val writer = new CharArrayWriter()
      // create the Generator without separator inserted between 2 records
      val gen = new JacksonGenerator(rowSchema, writer,
        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))

      new Iterator[String] {
        override def hasNext: Boolean = iter.hasNext
        override def next(): String = {
          gen.write(exprEnc.toRow(iter.next()))
          gen.flush()

          val json = writer.toString
          if (hasNext) {
            writer.reset()
          } else {
            gen.close()
          }

          json
        }
      }
    } (Encoders.STRING)
  }

  /**
   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply
   * asks each constituent BaseRelation for its respective files and takes the union of all results.
   * Depending on the source relations, this may not find all input files. Duplicates are removed.
   *
   * @group basic
   * @since 2.0.0
   */
  def inputFiles: Array[String] = {
    val files: Seq[String] = queryExecution.optimizedPlan.collect {
      case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) =>
        fsBasedRelation.inputFiles
      case fr: FileRelation =>
        fr.inputFiles
      case r: HiveTableRelation =>
        r.tableMeta.storage.locationUri.map(_.toString).toArray
    }.flatten
    files.toSet.toArray
  }

  ////////////////////////////////////////////////////////////////////////////
  // For Python API
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Converts a JavaRDD to a PythonRDD.
   */
  private[sql] def javaToPython: JavaRDD[Array[Byte]] = {
    val structType = schema  // capture it for closure
    val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType))
    EvaluatePython.javaToPython(rdd)
  }

  private[sql] def collectToPython(): Array[Any] = {
    EvaluatePython.registerPicklers()
    withAction("collectToPython", queryExecution) { plan =>
      val toJava: (Any) => Any = EvaluatePython.toJava(_, schema)
      val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
        plan.executeCollect().iterator.map(toJava))
      PythonRDD.serveIterator(iter, "serve-DataFrame")
    }
  }

  private[sql] def getRowsToPython(
      _numRows: Int,
      truncate: Int): Array[Any] = {
    EvaluatePython.registerPicklers()
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    val rows = getRows(numRows, truncate).map(_.toArray).toArray
    val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType)))
    val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
      rows.iterator.map(toJava))
    PythonRDD.serveIterator(iter, "serve-GetRows")
  }

  /**
   * Collect a Dataset as Arrow batches and serve stream to PySpark.
   */
  private[sql] def collectAsArrowToPython(): Array[Any] = {
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone

    PythonRDD.serveToStreamWithSync("serve-Arrow") { out =>
      withAction("collectAsArrowToPython", queryExecution) { plan =>
        val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId)
        val arrowBatchRdd = toArrowBatchRdd(plan)
        val numPartitions = arrowBatchRdd.partitions.length

        // Store collection results for worst case of 1 to N-1 partitions
        val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1))
        var lastIndex = -1  // index of last partition written

        // Handler to eagerly write partitions to Python in order
        def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = {
          // If result is from next partition in order
          if (index - 1 == lastIndex) {
            batchWriter.writeBatches(arrowBatches.iterator)
            lastIndex += 1
            // Write stored partitions that come next in order
            while (lastIndex < results.length && results(lastIndex) != null) {
              batchWriter.writeBatches(results(lastIndex).iterator)
              results(lastIndex) = null
              lastIndex += 1
            }
            // After last batch, end the stream
            if (lastIndex == results.length) {
              batchWriter.end()
            }
          } else {
            // Store partitions received out of order
            results(index - 1) = arrowBatches
          }
        }

        sparkSession.sparkContext.runJob(
          arrowBatchRdd,
          (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray,
          0 until numPartitions,
          handlePartitionBatches)
      }
    }
  }

  private[sql] def toPythonIterator(): Array[Any] = {
    withNewExecutionId {
      PythonRDD.toLocalIteratorAndServe(javaToPython.rdd)
    }
  }

  ////////////////////////////////////////////////////////////////////////////
  // Private Helpers
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with
   * an execution.
   */
  private def withNewExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, queryExecution)(body)
  }

  /**
   * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect
   * them with an execution. Before performing the action, the metrics of the executed plan will be
   * reset.
   */
  private def withNewRDDExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, rddQueryExecution) {
      rddQueryExecution.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      body
    }
  }

  /**
   * Wrap a Dataset action to track the QueryExecution and time cost, then report to the
   * user-registered callback functions.
   */
  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {
    try {
      qe.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      val start = System.nanoTime()
      val result = SQLExecution.withNewExecutionId(sparkSession, qe) {
        action(qe.executedPlan)
      }
      val end = System.nanoTime()
      sparkSession.listenerManager.onSuccess(name, qe, end - start)
      result
    } catch {
      case e: Throwable =>
        sparkSession.listenerManager.onFailure(name, qe, e)
        throw e
    }
  }

  /**
   * Collect all elements from a spark plan.
   */
  private def collectFromPlan(plan: SparkPlan): Array[T] = {
    // This projection writes output to a `InternalRow`, which means applying this projection is not
    // thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe.
    val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
    plan.executeCollect().map { row =>
      // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
      // parameter of its `get` method, so it's safe to use null here.
      objProj(row).get(0, null).asInstanceOf[T]
    }
  }

  private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = {
    val sortOrder: Seq[SortOrder] = sortExprs.map { col =>
      col.expr match {
        case expr: SortOrder =>
          expr
        case expr: Expression =>
          SortOrder(expr, Ascending)
      }
    }
    withTypedPlan {
      Sort(sortOrder, global = global, logicalPlan)
    }
  }

  /** A convenient function to wrap a logical plan and produce a DataFrame. */
  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = {
    Dataset.ofRows(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a logical plan and produce a Dataset. */
  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    Dataset(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a set based logical plan and produce a Dataset. */
  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) {
      // Set operators widen types (change the schema), so we cannot reuse the row encoder.
      Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]]
    } else {
      Dataset(sparkSession, logicalPlan)
    }
  }

  /** Convert to an RDD of serialized ArrowRecordBatches. */
  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = {
    val schemaCaptured = this.schema
    val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone
    plan.execute().mapPartitionsInternal { iter =>
      val context = TaskContext.get()
      ArrowConverters.toBatchIterator(
        iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context)
    }
  }

  // This is only used in tests, for now.
  private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = {
    toArrowBatchRdd(queryExecution.executedPlan)
  }
}

[0m2021.02.25 15:16:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:16:28 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 15:16:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:16:33 INFO  time: compiled root in 0.4s[0m
[0m2021.02.25 15:16:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:16:37 INFO  time: compiled root in 1.03s[0m
[0m2021.02.25 15:16:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:16:44 INFO  time: compiled root in 1.05s[0m
[0m2021.02.25 15:18:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:18:53 INFO  time: compiled root in 0.94s[0m
[0m2021.02.25 15:24:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:24:44 INFO  time: compiled root in 0.98s[0m
[0m2021.02.25 15:45:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:45:41 INFO  time: compiled root in 0.9s[0m
[0m2021.02.25 15:45:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:45:57 INFO  time: compiled root in 1.34s[0m
[0m2021.02.25 15:46:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:46:11 INFO  time: compiled root in 0.86s[0m
[0m2021.02.25 15:46:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:46:15 INFO  time: compiled root in 0.9s[0m
[0m2021.02.25 16:02:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:02:20 INFO  time: compiled root in 0.72s[0m
[0m2021.02.25 16:23:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:23:24 INFO  time: compiled root in 0.37s[0m
[0m2021.02.25 16:23:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:23:30 INFO  time: compiled root in 0.74s[0m
[0m2021.02.25 16:24:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:24:20 INFO  time: compiled root in 0.82s[0m
[0m2021.02.25 16:25:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:25:57 INFO  time: compiled root in 0.8s[0m
[0m2021.02.25 16:27:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:27:08 INFO  time: compiled root in 0.89s[0m
[0m2021.02.25 16:28:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:28:50 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 16:29:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:29:07 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 16:29:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:29:50 INFO  time: compiled root in 0.76s[0m
[0m2021.02.25 16:34:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:34:19 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 16:34:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:34:25 INFO  time: compiled root in 0.83s[0m
[0m2021.02.25 16:37:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:37:21 INFO  time: compiled root in 0.74s[0m
[0m2021.02.25 16:38:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:38:31 INFO  time: compiled root in 0.86s[0m
[0m2021.02.25 16:39:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:39:46 INFO  time: compiled root in 0.88s[0m
[0m2021.02.25 16:40:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:40:41 INFO  time: compiled root in 1.07s[0m
[0m2021.02.25 16:41:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:41:53 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 16:42:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:42:58 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 16:45:11 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: value debug is not a member of org.apache.spark.sql.SparkSession
    spark.debug.maxToStringFields=100
    ^^^^^^^^^^^[0m
[0m2021.02.25 16:45:11 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: value debug is not a member of org.apache.spark.sql.SparkSession
    spark.debug.maxToStringFields=100
    ^^^^^^^^^^^[0m
[0m2021.02.25 16:45:11 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: value debug is not a member of org.apache.spark.sql.SparkSession
    spark.debug.maxToStringFields=100
    ^^^^^^^^^^^[0m
[0m2021.02.25 16:45:11 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: value debug is not a member of org.apache.spark.sql.SparkSession
    spark.debug.maxToStringFields=100
    ^^^^^^^^^^^[0m
[0m2021.02.25 16:45:11 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: value debug is not a member of org.apache.spark.sql.SparkSession
    spark.debug.maxToStringFields=100
    ^^^^^^^^^^^[0m
[0m2021.02.25 16:45:11 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: value debug is not a member of org.apache.spark.sql.SparkSession
    spark.debug.maxToStringFields=100
    ^^^^^^^^^^^[0m
[0m2021.02.25 16:45:17 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: value debug is not a member of org.apache.spark.sql.SparkSession
    spark.debug.maxToStringFields=100
    ^^^^^^^^^^^[0m
[0m2021.02.25 16:45:17 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: value debug is not a member of org.apache.spark.sql.SparkSession
    spark.debug.maxToStringFields=100
    ^^^^^^^^^^^[0m
[0m2021.02.25 16:45:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:45:27 INFO  time: compiled root in 0.74s[0m
[0m2021.02.25 16:45:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:45:31 INFO  time: compiled root in 0.8s[0m
[0m2021.02.25 16:46:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:46:30 INFO  time: compiled root in 0.75s[0m
[0m2021.02.25 18:40:01 INFO  shutting down Metals[0m
[0m2021.02.25 18:40:01 INFO  Shut down connection with build server.[0m
[0m2021.02.25 18:40:01 INFO  Shut down connection with build server.[0m
[0m2021.02.25 18:40:01 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.02.26 09:36:11 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.53.2.[0m
[0m2021.02.26 09:36:12 INFO  time: initialize in 0.46s[0m
[0m2021.02.26 09:36:12 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.26 09:36:12 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3842509255470967146/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.26 09:36:12 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.02.26 09:36:15 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // val schema = new StructType() 
    //     .add("Container", StringType, true)
    //     .add("Filename", StringType, true)
    //     .add("Envelope", StringType, true)
    //     .add("_corrupt_record", StringType, true)

    val rddFromFile = spark.read.parquet(
        "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet"
        )
    
    // s3a://commoncrawl/crawl-data/CC-MAIN-2021-10/segments/1614178351374.10/warc/CC-MAIN-20210225153633-20210225183633-00627.warc.gz
    // s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz
    // s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz
    // envelope.payload-metadata.actual-content-length

    rddFromFile.printSchema()
    rddFromFile.show(1)
    //rddFromFile
    //.filter(!functions.isnull($"envelope.format"))
    //.select(rddFromFile.col("envelope.format"), rddFromFile.col("envelope.payload-metadata.actual-content-length"))
    //.show()
    
  }
}
Waiting for the bsp connection to come up...
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // val schema = new StructType() 
    //     .add("Container", StringType, true)
    //     .add("Filename", StringType, true)
    //     .add("Envelope", StringType, true)
    //     .add("_corrupt_record", StringType, true)

    val rddFromFile = spark.read.parquet(
        "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet"
        )
    
    // s3a://commoncrawl/crawl-data/CC-MAIN-2021-10/segments/1614178351374.10/warc/CC-MAIN-20210225153633-20210225183633-00627.warc.gz
    // s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz
    // s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz
    // envelope.payload-metadata.actual-content-length

    rddFromFile.printSchema()
    rddFromFile.show(1)
    //rddFromFile
    //.filter(!functions.isnull($"envelope.format"))
    //.select(rddFromFile.col("envelope.format"), rddFromFile.col("envelope.payload-metadata.actual-content-length"))
    //.show()
    
  }
}
[0m2021.02.26 09:36:17 INFO  time: code lens generation in 4.3s[0m
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/delaneylekien/project3/scalas3read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3842509255470967146/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3842509255470967146/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.26 09:36:17 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.26 09:36:17 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2946409337897456581/bsp.socket'...[0m
2021.02.26 09:36:17 INFO  Attempting to connect to the build server...[0mWaiting for the bsp connection to come up...

Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher154544997489293366/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2946409337897456581/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2946409337897456581/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.26 09:36:18 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher154544997489293366/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher154544997489293366/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.26 09:36:18 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.02.26 09:36:18 INFO  time: Connected to build server in 5.51s[0m
[0m2021.02.26 09:36:18 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.26 09:36:18 INFO  time: Imported build in 0.35s[0m
[0m2021.02.26 09:36:21 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.26 09:36:22 INFO  time: indexed workspace in 3.75s[0m
[0m2021.02.26 09:36:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 09:36:35 INFO  time: compiled root in 2.37s[0m
[0m2021.02.26 09:37:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 09:37:36 INFO  time: compiled root in 2.38s[0m
[0m2021.02.26 09:37:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 09:37:55 INFO  time: compiled root in 1.43s[0m
Exception in thread "pool-4-thread-1" java.lang.Error: java.lang.InterruptedException
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$onCompilerJobQueue$1.apply$mcV$sp(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
[0m2021.02.26 09:38:14 INFO  compiling root (1 scala source)[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.CharArrayWriter

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.commons.lang3.StringUtils

import org.apache.spark.TaskContext
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.api.java.function._
import org.apache.spark.api.python.{PythonRDD, SerDeUtil}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection
import org.apache.spark.sql.catalyst.json.{JacksonGenerator, JSONOptions}
import org.apache.spark.sql.catalyst.optimizer.CombineUnions
import org.apache.spark.sql.catalyst.parser.{ParseException, ParserUtils}
import org.apache.spark.sql.catalyst.plans._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, PartitioningCollection}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.arrow.{ArrowBatchStreamWriter, ArrowConverters}
import org.apache.spark.sql.execution.command._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.execution.python.EvaluatePython
import org.apache.spark.sql.execution.stat.StatFunctions
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.types._
import org.apache.spark.sql.util.SchemaUtils
import org.apache.spark.storage.StorageLevel
import org.apache.spark.unsafe.array.ByteArrayMethods
import org.apache.spark.unsafe.types.CalendarInterval
import org.apache.spark.util.Utils

private[sql] object Dataset {
  def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = {
    val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]])
    // Eagerly bind the encoder so we verify that the encoder matches the underlying
    // schema. The user will get an error if this is not the case.
    // optimization: it is guaranteed that [[InternalRow]] can be converted to [[Row]] so
    // do not do this check in that case. this check can be expensive since it requires running
    // the whole [[Analyzer]] to resolve the deserializer
    if (dataset.exprEnc.clsTag.runtimeClass != classOf[Row]) {
      dataset.deserializer
    }
    dataset
  }

  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {
    val qe = sparkSession.sessionState.executePlan(logicalPlan)
    qe.assertAnalyzed()
    new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))
  }
}

/**
 * A Dataset is a strongly typed collection of domain-specific objects that can be transformed
 * in parallel using functional or relational operations. Each Dataset also has an untyped view
 * called a `DataFrame`, which is a Dataset of [[Row]].
 *
 * Operations available on Datasets are divided into transformations and actions. Transformations
 * are the ones that produce new Datasets, and actions are the ones that trigger computation and
 * return results. Example transformations include map, filter, select, and aggregate (`groupBy`).
 * Example actions count, show, or writing data out to file systems.
 *
 * Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally,
 * a Dataset represents a logical plan that describes the computation required to produce the data.
 * When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
 * physical plan for efficient execution in a parallel and distributed manner. To explore the
 * logical plan as well as optimized physical plan, use the `explain` function.
 *
 * To efficiently support domain-specific objects, an [[Encoder]] is required. The encoder maps
 * the domain specific type `T` to Spark's internal type system. For example, given a class `Person`
 * with two fields, `name` (string) and `age` (int), an encoder is used to tell Spark to generate
 * code at runtime to serialize the `Person` object into a binary structure. This binary structure
 * often has much lower memory footprint as well as are optimized for efficiency in data processing
 * (e.g. in a columnar format). To understand the internal binary representation for data, use the
 * `schema` function.
 *
 * There are typically two ways to create a Dataset. The most common way is by pointing Spark
 * to some files on storage systems, using the `read` function available on a `SparkSession`.
 * {{{
 *   val people = spark.read.parquet("...").as[Person]  // Scala
 *   Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java
 * }}}
 *
 * Datasets can also be created through transformations available on existing Datasets. For example,
 * the following creates a new Dataset by applying a filter on the existing one:
 * {{{
 *   val names = people.map(_.name)  // in Scala; names is a Dataset[String]
 *   Dataset<String> names = people.map((Person p) -> p.name, Encoders.STRING));
 * }}}
 *
 * Dataset operations can also be untyped, through various domain-specific-language (DSL)
 * functions defined in: Dataset (this class), [[Column]], and [[functions]]. These operations
 * are very similar to the operations available in the data frame abstraction in R or Python.
 *
 * To select a column from the Dataset, use `apply` method in Scala and `col` in Java.
 * {{{
 *   val ageCol = people("age")  // in Scala
 *   Column ageCol = people.col("age"); // in Java
 * }}}
 *
 * Note that the [[Column]] type can also be manipulated through its various functions.
 * {{{
 *   // The following creates a new column that increases everybody's age by 10.
 *   people("age") + 10  // in Scala
 *   people.col("age").plus(10);  // in Java
 * }}}
 *
 * A more concrete example in Scala:
 * {{{
 *   // To create Dataset[Row] using SparkSession
 *   val people = spark.read.parquet("...")
 *   val department = spark.read.parquet("...")
 *
 *   people.filter("age > 30")
 *     .join(department, people("deptId") === department("id"))
 *     .groupBy(department("name"), people("gender"))
 *     .agg(avg(people("salary")), max(people("age")))
 * }}}
 *
 * and in Java:
 * {{{
 *   // To create Dataset<Row> using SparkSession
 *   Dataset<Row> people = spark.read().parquet("...");
 *   Dataset<Row> department = spark.read().parquet("...");
 *
 *   people.filter(people.col("age").gt(30))
 *     .join(department, people.col("deptId").equalTo(department.col("id")))
 *     .groupBy(department.col("name"), people.col("gender"))
 *     .agg(avg(people.col("salary")), max(people.col("age")));
 * }}}
 *
 * @groupname basic Basic Dataset functions
 * @groupname action Actions
 * @groupname untypedrel Untyped transformations
 * @groupname typedrel Typed transformations
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class Dataset[T] private[sql](
    @transient val sparkSession: SparkSession,
    @DeveloperApi @InterfaceStability.Unstable @transient val queryExecution: QueryExecution,
    encoder: Encoder[T])
  extends Serializable {

  queryExecution.assertAnalyzed()

  // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure
  // you wrap it with `withNewExecutionId` if this actions doesn't call other action.

  def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sparkSession, sparkSession.sessionState.executePlan(logicalPlan), encoder)
  }

  def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sqlContext.sparkSession, logicalPlan, encoder)
  }

  @transient private[sql] val logicalPlan: LogicalPlan = {
    // For various commands (like DDL) and queries with side effects, we force query execution
    // to happen right away to let these side effects take place eagerly.
    queryExecution.analyzed match {
      case c: Command =>
        LocalRelation(c.output, withAction("command", queryExecution)(_.executeCollect()))
      case u @ Union(children) if children.forall(_.isInstanceOf[Command]) =>
        LocalRelation(u.output, withAction("command", queryExecution)(_.executeCollect()))
      case _ =>
        queryExecution.analyzed
    }
  }

  /**
   * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the
   * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use
   * it when constructing new Dataset objects that have the same object type (that will be
   * possibly resolved to a different schema).
   */
  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)

  // The deserializer expression which can be used to build a projection and turn rows to objects
  // of type T, after collecting rows to the driver side.
  private lazy val deserializer =
    exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer).deserializer

  private implicit def classTag = exprEnc.clsTag

  // sqlContext must be val because a stable identifier is expected when you import implicits
  @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext

  private[sql] def resolve(colName: String): NamedExpression = {
    queryExecution.analyzed.resolveQuoted(colName, sparkSession.sessionState.analyzer.resolver)
      .getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
  }

  private[sql] def numericColumns: Seq[Expression] = {
    schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n =>
      queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get
    }
  }

  /**
   * Get rows represented in Sequence by specific truncate and vertical requirement.
   *
   * @param numRows Number of rows to return
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   */
  private[sql] def getRows(
      numRows: Int,
      truncate: Int): Seq[Seq[String]] = {
    val newDf = toDF()
    val castCols = newDf.logicalPlan.output.map { col =>
      // Since binary types in top-level schema fields have a specific format to print,
      // so we do not cast them to strings here.
      if (col.dataType == BinaryType) {
        Column(col)
      } else {
        Column(col).cast(StringType)
      }
    }
    val data = newDf.select(castCols: _*).take(numRows + 1)

    // For array values, replace Seq and Array with square brackets
    // For cells that are beyond `truncate` characters, replace it with the
    // first `truncate-3` and "..."
    schema.fieldNames.toSeq +: data.map { row =>
      row.toSeq.map { cell =>
        val str = cell match {
          case null => "null"
          case binary: Array[Byte] => binary.map("%02X".format(_)).mkString("[", " ", "]")
          case _ => cell.toString
        }
        if (truncate > 0 && str.length > truncate) {
          // do not show ellipses for strings shorter than 4 characters.
          if (truncate < 4) str.substring(0, truncate)
          else str.substring(0, truncate - 3) + "..."
        } else {
          str
        }
      }: Seq[String]
    }
  }

  /**
   * Compose the string representing rows for output
   *
   * @param _numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   */
  private[sql] def showString(
      _numRows: Int,
      truncate: Int = 20,
      vertical: Boolean = false): String = {
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data.
    val tmpRows = getRows(numRows, truncate)

    val hasMoreData = tmpRows.length - 1 > numRows
    val rows = tmpRows.take(numRows + 1)

    val sb = new StringBuilder
    val numCols = schema.fieldNames.length
    // We set a minimum column width at '3'
    val minimumColWidth = 3

    if (!vertical) {
      // Initialise the width of each column to a minimum value
      val colWidths = Array.fill(numCols)(minimumColWidth)

      // Compute the width of each column
      for (row <- rows) {
        for ((cell, i) <- row.zipWithIndex) {
          colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell))
        }
      }

      val paddedRows = rows.map { row =>
        row.zipWithIndex.map { case (cell, i) =>
          if (truncate > 0) {
            StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          } else {
            StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          }
        }
      }

      // Create SeparateLine
      val sep: String = colWidths.map("-" * _).addString(sb, "+", "+", "+\n").toString()

      // column names
      paddedRows.head.addString(sb, "|", "|", "|\n")
      sb.append(sep)

      // data
      paddedRows.tail.foreach(_.addString(sb, "|", "|", "|\n"))
      sb.append(sep)
    } else {
      // Extended display mode enabled
      val fieldNames = rows.head
      val dataRows = rows.tail

      // Compute the width of field name and data columns
      val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) =>
        math.max(curMax, Utils.stringHalfWidth(fieldName))
      }
      val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) =>
        math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max)
      }

      dataRows.zipWithIndex.foreach { case (row, i) =>
        // "+ 5" in size means a character length except for padded names and data
        val rowHeader = StringUtils.rightPad(
          s"-RECORD $i", fieldNameColWidth + dataColWidth + 5, "-")
        sb.append(rowHeader).append("\n")
        row.zipWithIndex.map { case (cell, j) =>
          val fieldName = StringUtils.rightPad(fieldNames(j),
            fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length)
          val data = StringUtils.rightPad(cell,
            dataColWidth - Utils.stringHalfWidth(cell) + cell.length)
          s" $fieldName | $data "
        }.addString(sb, "", "\n", "\n")
      }
    }

    // Print a footer
    if (vertical && rows.tail.isEmpty) {
      // In a vertical mode, print an empty row set explicitly
      sb.append("(0 rows)\n")
    } else if (hasMoreData) {
      // For Data that has more than "numRows" records
      val rowsString = if (numRows == 1) "row" else "rows"
      sb.append(s"only showing top $numRows $rowsString\n")
    }

    sb.toString()
  }

  override def toString: String = {
    try {
      val builder = new StringBuilder
      val fields = schema.take(2).map {
        case f => s"${f.name}: ${f.dataType.simpleString(2)}"
      }
      builder.append("[")
      builder.append(fields.mkString(", "))
      if (schema.length > 2) {
        if (schema.length - fields.size == 1) {
          builder.append(" ... 1 more field")
        } else {
          builder.append(" ... " + (schema.length - 2) + " more fields")
        }
      }
      builder.append("]").toString()
    } catch {
      case NonFatal(e) =>
        s"Invalid tree; ${e.getMessage}:\n$queryExecution"
    }
  }

  /**
   * Converts this strongly typed collection of data to generic Dataframe. In contrast to the
   * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]
   * objects that allow fields to be accessed by ordinal or name.
   *
   * @group basic
   * @since 1.6.0
   */
  // This is declared with parentheses to prevent the Scala compiler from treating
  // `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))

  /**
   * :: Experimental ::
   * Returns a new Dataset where each record has been mapped on to the specified type. The
   * method used to map columns depend on the type of `U`:
   *  - When `U` is a class, fields for the class will be mapped to columns of the same name
   *    (case sensitivity is determined by `spark.sql.caseSensitive`).
   *  - When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will
   *    be assigned to `_1`).
   *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the
   *    `DataFrame` will be used.
   *
   * If the schema of the Dataset does not match the desired `U` type, you can use `select`
   * along with `alias` or `as` to rearrange or rename as required.
   *
   * Note that `as[]` only changes the view of the data that is passed into typed operations,
   * such as `map()`, and does not eagerly project away any columns that are not present in
   * the specified class.
   *
   * @group basic
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan)

  /**
   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.
   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with
   * meaningful names. For example:
   * {{{
   *   val rdd: RDD[(Int, String)] = ...
   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`
   *   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
   * }}}
   *
   * @group basic
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def toDF(colNames: String*): DataFrame = {
    require(schema.size == colNames.size,
      "The number of columns doesn't match.\n" +
        s"Old column names (${schema.size}): " + schema.fields.map(_.name).mkString(", ") + "\n" +
        s"New column names (${colNames.size}): " + colNames.mkString(", "))

    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>
      Column(oldAttribute).as(newName)
    }
    select(newCols : _*)
  }

  /**
   * Returns the schema of this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def schema: StructType = queryExecution.analyzed.schema

  /**
   * Prints the schema to the console in a nice tree format.
   *
   * @group basic
   * @since 1.6.0
   */
  // scalastyle:off println
  def printSchema(): Unit = println(schema.treeString)
  // scalastyle:on println

  /**
   * Prints the plans (logical and physical) to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(extended: Boolean): Unit = {
    val explain = ExplainCommand(queryExecution.logical, extended = extended)
    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {
      // scalastyle:off println
      r => println(r.getString(0))
      // scalastyle:on println
    }
  }

  /**
   * Prints the physical plan to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(): Unit = explain(extended = false)

  /**
   * Returns all column names and their data types as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def dtypes: Array[(String, String)] = schema.fields.map { field =>
    (field.name, field.dataType.toString)
  }

  /**
   * Returns all column names as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def columns: Array[String] = schema.fields.map(_.name)

  /**
   * Returns true if the `collect` and `take` methods can be run locally
   * (without any Spark executors).
   *
   * @group basic
   * @since 1.6.0
   */
  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]

  /**
   * Returns true if the `Dataset` is empty.
   *
   * @group basic
   * @since 2.4.0
   */
  def isEmpty: Boolean = withAction("isEmpty", limit(1).groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0) == 0
  }

  /**
   * Returns true if this Dataset contains one or more sources that continuously
   * return data as it arrives. A Dataset that reads data from a streaming source
   * must be executed as a `StreamingQuery` using the `start()` method in
   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or
   * `collect()`, will throw an [[AnalysisException]] when there is a streaming
   * source present.
   *
   * @group streaming
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def isStreaming: Boolean = logicalPlan.isStreaming

  /**
   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true)

  /**
   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
   * logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true)

  /**
   * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be
   * used to truncate the logical plan of this Dataset, which is especially useful in iterative
   * algorithms where the plan may grow exponentially. Local checkpoints are written to executor
   * storage and despite potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false)

  /**
   * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. Local checkpoints are written to executor storage and despite
   * potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint(
    eager = eager,
    reliableCheckpoint = false
  )

  /**
   * Returns a checkpointed version of this Dataset.
   *
   * @param eager Whether to checkpoint this dataframe immediately
   * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the
   *                           checkpoint directory. If false creates a local checkpoint using
   *                           the caching subsystem
   */
  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = {
    val internalRdd = queryExecution.toRdd.map(_.copy())
    if (reliableCheckpoint) {
      internalRdd.checkpoint()
    } else {
      internalRdd.localCheckpoint()
    }

    if (eager) {
      internalRdd.count()
    }

    val physicalPlan = queryExecution.executedPlan

    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the
    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner
    // joins.
    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {
      partitioning match {
        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)
        case p => p
      }
    }

    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)

    Dataset.ofRows(
      sparkSession,
      LogicalRDD(
        logicalPlan.output,
        internalRdd,
        outputPartitioning,
        physicalPlan.outputOrdering,
        isStreaming
      )(sparkSession)).as[T]
  }

  /**
   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time
   * before which we assume no more late data is going to arrive.
   *
   * Spark will use this watermark for several purposes:
   *  - To know when a given time window aggregation can be finalized and thus can be emitted when
   *    using output modes that do not allow updates.
   *  - To minimize the amount of state that we need to keep for on-going aggregations,
   *    `mapGroupsWithState` and `dropDuplicates` operators.
   *
   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across
   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost
   *  of coordinating this value across partitions, the actual watermark used is only guaranteed
   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still
   *  process records that arrive more than `delayThreshold` late.
   *
   * @param eventTime the name of the column that contains the event time of the row.
   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest
   *                       record that has been processed in the form of an interval
   *                       (e.g. "1 minute" or "5 hours"). NOTE: This should not be negative.
   *
   * @group streaming
   * @since 2.1.0
   */
  @InterfaceStability.Evolving
  // We only accept an existing column name, not a derived column here as a watermark that is
  // defined on a derived column cannot referenced elsewhere in the plan.
  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {
    val parsedDelay =
      try {
        CalendarInterval.fromCaseInsensitiveString(delayThreshold)
      } catch {
        case e: IllegalArgumentException =>
          throw new AnalysisException(
            s"Unable to parse time delay '$delayThreshold'",
            cause = Some(e))
      }
    require(parsedDelay.milliseconds >= 0 && parsedDelay.months >= 0,
      s"delay threshold ($delayThreshold) should not be negative.")
    EliminateEventTimeWatermark(
      EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan))
  }

  /**
   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
   * and all cells will be aligned right. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   *
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int): Unit = show(numRows, truncate = true)

  /**
   * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters
   * will be truncated, and all cells will be aligned right.
   *
   * @group action
   * @since 1.6.0
   */
  def show(): Unit = show(20)

  /**
   * Displays the top 20 rows of Dataset in a tabular form.
   *
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *                 be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  def show(truncate: Boolean): Unit = show(20, truncate)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   * @param numRows Number of rows to show
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *              be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {
    println(showString(numRows, truncate = 20))
  } else {
    println(showString(numRows, truncate = 0))
  }

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * If `vertical` enabled, this command prints output rows vertically (one line per column value)?
   *
   * {{{
   * -RECORD 0-------------------
   *  year            | 1980
   *  month           | 12
   *  AVG('Adj Close) | 0.503218
   *  AVG('Adj Close) | 0.595103
   * -RECORD 1-------------------
   *  year            | 1981
   *  month           | 01
   *  AVG('Adj Close) | 0.523289
   *  AVG('Adj Close) | 0.570307
   * -RECORD 2-------------------
   *  year            | 1982
   *  month           | 02
   *  AVG('Adj Close) | 0.436504
   *  AVG('Adj Close) | 0.475256
   * -RECORD 3-------------------
   *  year            | 1983
   *  month           | 03
   *  AVG('Adj Close) | 0.410516
   *  AVG('Adj Close) | 0.442194
   * -RECORD 4-------------------
   *  year            | 1984
   *  month           | 04
   *  AVG('Adj Close) | 0.450090
   *  AVG('Adj Close) | 0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   * @group action
   * @since 2.3.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Int, vertical: Boolean): Unit =
    println(showString(numRows, truncate, vertical))
  // scalastyle:on println

  /**
   * Returns a [[DataFrameNaFunctions]] for working with missing data.
   * {{{
   *   // Dropping rows containing any null values.
   *   ds.na.drop()
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())

  /**
   * Returns a [[DataFrameStatFunctions]] for working statistic functions support.
   * {{{
   *   // Finding frequent items in column with name 'a'.
   *   ds.stat.freqItems(Seq("a"))
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())

  /**
   * Join with another `DataFrame`.
   *
   * Behaves as an INNER JOIN and requires a subsequent join predicate.
   *
   * @param right Right side of the join operation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)
  }

  /**
   * Inner equi-join with another `DataFrame` using the given column.
   *
   * Different from other join functions, the join column will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the column "user_id"
   *   df1.join(df2, "user_id")
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumn Name of the column to join on. This column must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumn: String): DataFrame = {
    join(right, Seq(usingColumn))
  }

  /**
   * Inner equi-join with another `DataFrame` using the given columns.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the columns "user_id" and "user_name"
   *   df1.join(df2, Seq("user_id", "user_name"))
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {
    join(right, usingColumns, "inner")
  }

  /**
   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate
   * is specified as an inner join. If you would explicitly like to perform a cross join use the
   * `crossJoin` method.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {
    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right
    // by creating a new instance for one of the branch.
    val joined = sparkSession.sessionState.executePlan(
      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))
      .analyzed.asInstanceOf[Join]

    withPlan {
      Join(
        joined.left,
        joined.right,
        UsingJoin(JoinType(joinType), usingColumns),
        None)
    }
  }

  /**
   * Inner join with another `DataFrame`, using the given join expression.
   *
   * {{{
   *   // The following two are equivalent:
   *   df1.join(df2, $"df1Key" === $"df2Key")
   *   df1.join(df2).where($"df1Key" === $"df2Key")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, "inner")

  /**
   * Join with another `DataFrame`, using the given join expression. The following performs
   * a full outer join between `df1` and `df2`.
   *
   * {{{
   *   // Scala:
   *   import org.apache.spark.sql.functions._
   *   df1.join(df2, $"df1Key" === $"df2Key", "outer")
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
   * }}}
   *
   * @param right Right side of the join.
   * @param joinExprs Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {
    // Note that in this function, we introduce a hack in the case of self-join to automatically
    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].
    // Consider this case: df.join(df, df("key") === df("key"))
    // Since df("key") === df("key") is a trivially true condition, this actually becomes a
    // cartesian join. However, most likely users expect to perform a self join using "key".
    // With that assumption, this hack turns the trivially true condition into equality on join
    // keys that are resolved to both sides.

    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.
    // After the cloning, left and right side will have distinct expression ids.
    val plan = withPlan(
      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))
      .queryExecution.analyzed.asInstanceOf[Join]

    // If auto self join alias is disabled, return the plan.
    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {
      return withPlan(plan)
    }

    // If left/right have no output set intersection, return the plan.
    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed
    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed
    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {
      return withPlan(plan)
    }

    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.
    // By the time we get here, since we have already run analysis, all attributes should've been
    // resolved and become AttributeReference.
    val cond = plan.condition.map { _.transform {
      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)
          if a.sameRef(b) =>
        catalyst.expressions.EqualTo(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
      case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference)
        if a.sameRef(b) =>
        catalyst.expressions.EqualNullSafe(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
    }}

    withPlan {
      plan.copy(condition = cond)
    }
  }

  /**
   * Explicit cartesian join with another `DataFrame`.
   *
   * @param right Right side of the join operation.
   *
   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.
   *
   * @group untypedrel
   * @since 2.1.0
   */
  def crossJoin(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)
  }

  /**
   * :: Experimental ::
   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to
   * true.
   *
   * This is similar to the relation `join` function with one important difference in the
   * result schema. Since `joinWith` preserves objects present on either side of the join, the
   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.
   *
   * This type of join can be useful both for preserving type-safety with the original object
   * types as well as working with relational data where either side of the join has column
   * names in common.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {
    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,
    // etc.
    val joined = sparkSession.sessionState.executePlan(
      Join(
        this.logicalPlan,
        other.logicalPlan,
        JoinType(joinType),
        Some(condition.expr))).analyzed.asInstanceOf[Join]

    if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) {
      throw new AnalysisException("Invalid join type in joinWith: " + joined.joinType.sql)
    }

    // For both join side, combine all outputs into a single column and alias it with "_1" or "_2",
    // to match the schema for the encoder of the join result.
    // Note that we do this before joining them, to enable the join operator to return null for one
    // side, in cases like outer-join.
    val left = {
      val combined = if (this.exprEnc.flat) {
        assert(joined.left.output.length == 1)
        Alias(joined.left.output.head, "_1")()
      } else {
        Alias(CreateStruct(joined.left.output), "_1")()
      }
      Project(combined :: Nil, joined.left)
    }

    val right = {
      val combined = if (other.exprEnc.flat) {
        assert(joined.right.output.length == 1)
        Alias(joined.right.output.head, "_2")()
      } else {
        Alias(CreateStruct(joined.right.output), "_2")()
      }
      Project(combined :: Nil, joined.right)
    }

    // Rewrites the join condition to make the attribute point to correct column/field, after we
    // combine the outputs of each join side.
    val conditionExpr = joined.condition.get transformUp {
      case a: Attribute if joined.left.outputSet.contains(a) =>
        if (this.exprEnc.flat) {
          left.output.head
        } else {
          val index = joined.left.output.indexWhere(_.exprId == a.exprId)
          GetStructField(left.output.head, index)
        }
      case a: Attribute if joined.right.outputSet.contains(a) =>
        if (other.exprEnc.flat) {
          right.output.head
        } else {
          val index = joined.right.output.indexWhere(_.exprId == a.exprId)
          GetStructField(right.output.head, index)
        }
    }

    implicit val tuple2Encoder: Encoder[(T, U)] =
      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)

    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))
  }

  /**
   * :: Experimental ::
   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair
   * where `condition` evaluates to true.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {
    joinWith(other, condition, "inner")
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {
    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = false, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the specified column, all in ascending order.
   * {{{
   *   // The following 3 are equivalent
   *   ds.sort("sortcol")
   *   ds.sort($"sortcol")
   *   ds.sort($"sortcol".asc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortCol: String, sortCols: String*): Dataset[T] = {
    sort((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset sorted by the given expressions. For example:
   * {{{
   *   ds.sort($"col1", $"col2".desc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = true, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def apply(colName: String): Column = col(colName)

  /**
   * Specifies some hint on the current Dataset. As an example, the following code specifies
   * that one of the plan can be broadcasted:
   *
   * {{{
   *   df1.join(df2.hint("broadcast"))
   * }}}
   *
   * @group basic
   * @since 2.2.0
   */
  @scala.annotation.varargs
  def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan {
    UnresolvedHint(name, parameters, logicalPlan)
  }

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def col(colName: String): Column = colName match {
    case "*" =>
      Column(ResolvedStar(queryExecution.analyzed.output))
    case _ =>
      if (sqlContext.conf.supportQuotedRegexColumnName) {
        colRegex(colName)
      } else {
        val expr = resolve(colName)
        Column(expr)
      }
  }

  /**
   * Selects column based on the column name specified as a regex and returns it as [[Column]].
   * @group untypedrel
   * @since 2.3.0
   */
  def colRegex(colName: String): Column = {
    val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis
    colName match {
      case ParserUtils.escapedIdentifier(columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, None, caseSensitive))
      case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive))
      case _ =>
        Column(resolve(colName))
    }
  }

  /**
   * Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def as(alias: String): Dataset[T] = withTypedPlan {
    SubqueryAlias(alias, logicalPlan)
  }

  /**
   * (Scala-specific) Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def as(alias: Symbol): Dataset[T] = as(alias.name)

  /**
   * Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: String): Dataset[T] = as(alias)

  /**
   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: Symbol): Dataset[T] = as(alias)

  /**
   * Selects a set of column based expressions.
   * {{{
   *   ds.select($"colA", $"colB" + 1)
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(cols: Column*): DataFrame = withPlan {
    Project(cols.map(_.named), logicalPlan)
  }

  /**
   * Selects a set of columns. This is a variant of `select` that can only select
   * existing columns using column names (i.e. cannot construct expressions).
   *
   * {{{
   *   // The following two are equivalent:
   *   ds.select("colA", "colB")
   *   ds.select($"colA", $"colB")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)

  /**
   * Selects a set of SQL expressions. This is a variant of `select` that accepts
   * SQL expressions.
   *
   * {{{
   *   // The following are equivalent:
   *   ds.selectExpr("colA", "colB as newName", "abs(colC)")
   *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def selectExpr(exprs: String*): DataFrame = {
    select(exprs.map { expr =>
      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))
    }: _*)
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expression for each element.
   *
   * {{{
   *   val ds = Seq(1, 2, 3).toDS()
   *   val newDS = ds.select(expr("value + 1").as[Int])
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {
    implicit val encoder = c1.encoder
    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan)

    if (encoder.flat) {
      new Dataset[U1](sparkSession, project, encoder)
    } else {
      // Flattens inner fields of U1
      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)
    }
  }

  /**
   * Internal helper function for building typed selects that return tuples. For simplicity and
   * code reuse, we do this without the help of the type system and then use helper functions
   * that cast appropriately for the user facing interface.
   */
  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {
    val encoders = columns.map(_.encoder)
    val namedColumns =
      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)
    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))
    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =
    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =
    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =
    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4, U5](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4],
      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =
    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]

  /**
   * Filters rows using the given condition.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(condition: Column): Dataset[T] = withTypedPlan {
    Filter(condition.expr, logicalPlan)
  }

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.filter("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Filters rows using the given condition. This is an alias for `filter`.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(condition: Column): Dataset[T] = filter(condition)

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.where("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Groups the Dataset using the specified columns, so we can run aggregation on them. See
   * [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy($"department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)
  }

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)
  }

  /**
   * Groups the Dataset using the specified columns, so that we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of groupBy that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy("department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: (T, T) => T): T = withNewRDDExecutionId {
    rdd.reduce(func)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {
    val withGroupingKey = AppendColumns(func, logicalPlan)
    val executed = sparkSession.sessionState.executePlan(withGroupingKey)

    new KeyValueGroupedDataset(
      encoderFor[K],
      encoderFor[T],
      executed,
      logicalPlan.output,
      withGroupingKey.newColumns)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =
    groupByKey(func.call(_))(encoder)

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of rollup that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup("department", "group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of cube that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube("department", "group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg("age" -> "max", "salary" -> "avg")
   *   ds.groupBy().agg("age" -> "max", "salary" -> "avg")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
    groupBy().agg(aggExpr, aggExprs : _*)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * (Java-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(max($"age"), avg($"salary"))
   *   ds.groupBy().agg(max($"age"), avg($"salary"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)

  /**
   * Returns a new Dataset by taking the first `n` rows. The difference between this function
   * and `head` is that `head` is an action and returns an array (by triggering query execution)
   * while `limit` returns a new Dataset.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def limit(n: Int): Dataset[T] = withTypedPlan {
    Limit(Literal(n), logicalPlan)
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @deprecated("use union()", "2.0.0")
  def unionAll(other: Dataset[T]): Dataset[T] = union(other)

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.union(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   4|   5|   6|
   *   // +----+----+----+
   * }}}
   *
   * Notice that the column positions in the schema aren't necessarily matched with the
   * fields in the strongly typed objects in a Dataset. This function resolves columns
   * by their positions in the schema, not the fields in the strongly typed objects. Use
   * [[unionByName]] to resolve columns by field name in the typed objects.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def union(other: Dataset[T]): Dataset[T] = withSetOperator {
    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, other.logicalPlan))
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set
   * union (that does deduplication of elements), use this function followed by a [[distinct]].
   *
   * The difference between this function and [[union]] is that this function
   * resolves columns by name (not by position):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.unionByName(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   6|   4|   5|
   *   // +----+----+----+
   * }}}
   *
   * @group typedrel
   * @since 2.3.0
   */
  def unionByName(other: Dataset[T]): Dataset[T] = withSetOperator {
    // Check column name duplication
    val resolver = sparkSession.sessionState.analyzer.resolver
    val leftOutputAttrs = logicalPlan.output
    val rightOutputAttrs = other.logicalPlan.output

    SchemaUtils.checkColumnNameDuplication(
      leftOutputAttrs.map(_.name),
      "in the left attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)
    SchemaUtils.checkColumnNameDuplication(
      rightOutputAttrs.map(_.name),
      "in the right attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    // Builds a project list for `other` based on `logicalPlan` output names
    val rightProjectList = leftOutputAttrs.map { lattr =>
      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "${lattr.name}" among """ +
            s"""(${rightOutputAttrs.map(_.name).mkString(", ")})""")
      }
    }

    // Delegates failure checks to `CheckAnalysis`
    val notFoundAttrs = rightOutputAttrs.diff(rightProjectList)
    val rightChild = Project(rightProjectList ++ notFoundAttrs, other.logicalPlan)

    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, rightChild))
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.
   * This is equivalent to `INTERSECT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset while
   * preserving the duplicates.
   * This is equivalent to `INTERSECT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard
   * in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = true)
  }


  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.
   * This is equivalent to `EXCEPT DISTINCT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def except(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset while
   * preserving the duplicates.
   * This is equivalent to `EXCEPT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in
   * SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = true)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a user-supplied seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double, seed: Long): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction, seed = seed)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a random seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {
    withTypedPlan {
      Sample(0.0, fraction, withReplacement, seed, logicalPlan)
    }
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the total count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {
    sample(withReplacement, fraction, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * For Java API, use [[randomSplitAsList]].
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its
    // constituent partitions each time a split is materialized which could result in
    // overlapping splits. To prevent this, we explicitly sort each input partition to make the
    // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out
    // from the sort order.
    val sortOrder = logicalPlan.output
      .filter(attr => RowOrdering.isOrderable(attr.dataType))
      .map(SortOrder(_, Ascending))
    val plan = if (sortOrder.nonEmpty) {
      Sort(sortOrder, global = false, logicalPlan)
    } else {
      // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism
      cache()
      logicalPlan
    }
    val sum = weights.sum
    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
    normalizedCumWeights.sliding(2).map { x =>
      new Dataset[T](
        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder)
    }.toArray
  }

  /**
   * Returns a Java list that contains randomly split Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {
    val values = randomSplit(weights, seed)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {
    randomSplit(weights, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   */
  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {
    randomSplit(weights.toArray, seed)
  }

  /**
   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of
   * the input row are implicitly joined with each row that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count
   * the number of books that contain a given word:
   *
   * {{{
   *   case class Book(title: String, words: String)
   *   val ds: Dataset[Book]
   *
   *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))
   *
   *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
   * }}}
   *
   * Using `flatMap()` this can similarly be exploded as:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {
    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]

    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)

    val rowFunction =
      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))
    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero
   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All
   * columns of the input row are implicitly joined with each value that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()`:
   *
   * {{{
   *   ds.select(explode(split('words, " ")).as("word"))
   * }}}
   *
   * or `flatMap()`:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])
    : DataFrame = {
    val dataType = ScalaReflection.schemaFor[B].dataType
    val attributes = AttributeReference(outputColumn, dataType)() :: Nil
    // TODO handle the metadata?
    val elementSchema = attributes.toStructType

    def rowFunction(row: Row): TraversableOnce[InternalRow] = {
      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)
      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))
    }
    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * Returns a new Dataset by adding a column or replacing the existing column that has
   * the same name.
   *
   * `column`'s expression must only refer to attributes supplied by this Dataset. It is an
   * error to add a column that refers to some other Dataset.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col))

  /**
   * Returns a new Dataset by adding columns or replacing the existing columns that has
   * the same names.
   */
  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
    require(colNames.size == cols.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of columns: ${cols.size}")
    SchemaUtils.checkColumnNameDuplication(
      colNames,
      "in given column names",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output

    val columnMap = colNames.zip(cols).toMap

    val replacedAndExistingColumns = output.map { field =>
      columnMap.find { case (colName, _) =>
        resolver(field.name, colName)
      } match {
        case Some((colName: String, col: Column)) => col.as(colName)
        case _ => Column(field)
      }
    }

    val newColumns = columnMap.filter { case (colName, col) =>
      !output.exists(f => resolver(f.name, colName))
    }.map { case (colName, col) => col.as(colName) }

    select(replacedAndExistingColumns ++ newColumns : _*)
  }

  /**
   * Returns a new Dataset by adding columns with metadata.
   */
  private[spark] def withColumns(
      colNames: Seq[String],
      cols: Seq[Column],
      metadata: Seq[Metadata]): DataFrame = {
    require(colNames.size == metadata.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of metadata elements: ${metadata.size}")
    val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) =>
      col.as(colName, metadata)
    }
    withColumns(colNames, newCols)
  }

  /**
   * Returns a new Dataset by adding a column with metadata.
   */
  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame =
    withColumns(Seq(colName), Seq(col), Seq(metadata))

  /**
   * Returns a new Dataset with a column renamed.
   * This is a no-op if schema doesn't contain existingName.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumnRenamed(existingName: String, newName: String): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output
    val shouldRename = output.exists(f => resolver(f.name, existingName))
    if (shouldRename) {
      val columns = output.map { col =>
        if (resolver(col.name, existingName)) {
          Column(col).as(newName)
        } else {
          Column(col)
        }
      }
      select(columns : _*)
    } else {
      toDF()
    }
  }

  /**
   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain
   * column name.
   *
   * This method can only be used to drop top level columns. the colName string is treated
   * literally without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(colName: String): DataFrame = {
    drop(Seq(colName) : _*)
  }

  /**
   * Returns a new Dataset with columns dropped.
   * This is a no-op if schema doesn't contain column name(s).
   *
   * This method can only be used to drop top level columns. the colName string is treated literally
   * without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def drop(colNames: String*): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val remainingCols = allColumns.filter { attribute =>
      colNames.forall(n => !resolver(attribute.name, n))
    }.map(attribute => Column(attribute))
    if (remainingCols.size == allColumns.size) {
      toDF()
    } else {
      this.select(remainingCols: _*)
    }
  }

  /**
   * Returns a new Dataset with a column dropped.
   * This version of drop accepts a [[Column]] rather than a name.
   * This is a no-op if the Dataset doesn't have a column
   * with an equivalent expression.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(col: Column): DataFrame = {
    val expression = col match {
      case Column(u: UnresolvedAttribute) =>
        queryExecution.analyzed.resolveQuoted(
          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)
      case Column(expr: Expression) => expr
    }
    val attrs = this.logicalPlan.output
    val colsAfterDrop = attrs.filter { attr =>
      attr != expression
    }.map(attr => Column(attr))
    select(colsAfterDrop : _*)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `distinct`.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)

  /**
   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>
      // It is possibly there are more than one columns with the same name,
      // so we call filter instead of find.
      val cols = allColumns.filter(col => resolver(col.name, colName))
      if (cols.isEmpty) {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
      cols
    }
    Deduplicate(groupCols, logicalPlan)
  }

  /**
   * Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)

  /**
   * Returns a new [[Dataset]] with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {
    val colNames: Seq[String] = col1 +: cols
    dropDuplicates(colNames)
  }

  /**
   * Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
   * and max. If no columns are given, this function computes statistics for all numerical or
   * string columns.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.describe("age", "height").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // max     92.0  192.0
   * }}}
   *
   * Use [[summary]] for expanded statistics and control over which statistics to compute.
   *
   * @param cols Columns to compute statistics on.
   *
   * @group action
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def describe(cols: String*): DataFrame = {
    val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*)
    selected.summary("count", "mean", "stddev", "min", "max")
  }

  /**
   * Computes specified statistics for numeric and string columns. Available statistics are:
   *
   * - count
   * - mean
   * - stddev
   * - min
   * - max
   * - arbitrary approximate percentiles specified as a percentage (eg, 75%)
   *
   * If no statistics are given, this function computes count, mean, stddev, min,
   * approximate quartiles (percentiles at 25%, 50%, and 75%), and max.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.summary().show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 50%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * {{{
   *   ds.summary("count", "min", "25%", "75%", "max").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * To do a summary for specific columns first select them:
   *
   * {{{
   *   ds.select("age", "height").summary().show()
   * }}}
   *
   * See also [[describe]] for basic statistics.
   *
   * @param statistics Statistics from above list to be computed.
   *
   * @group action
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq)

  /**
   * Returns the first `n` rows.
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @group action
   * @since 1.6.0
   */
  def head(n: Int): Array[T] = withAction("head", limit(n).queryExecution)(collectFromPlan)

  /**
   * Returns the first row.
   * @group action
   * @since 1.6.0
   */
  def head(): T = head(1).head

  /**
   * Returns the first row. Alias for head().
   * @group action
   * @since 1.6.0
   */
  def first(): T = head()

  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: Dataset[T]): Dataset[U] = ...
   *
   *   ds
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: T => Boolean): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: FilterFunction[T]): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {
    MapElements[T, U](func, logicalPlan)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    implicit val uEnc = encoder
    withTypedPlan(MapElements[T, U](func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {
    new Dataset[U](
      sparkSession,
      MapPartitions[T, U](func, logicalPlan),
      implicitly[Encoder[U]])
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `f` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala
    mapPartitions(func)(encoder)
  }

  /**
   * Returns a new `DataFrame` that contains the result of applying a serialized R function
   * `func` to each partition.
   */
  private[sql] def mapPartitionsInR(
      func: Array[Byte],
      packageNames: Array[Byte],
      broadcastVars: Array[Broadcast[Object]],
      schema: StructType): DataFrame = {
    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]
    Dataset.ofRows(
      sparkSession,
      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =
    mapPartitions(_.flatMap(func))

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (T) => Iterator[U] = x => f.call(x).asScala
    flatMap(func)(encoder)
  }

  /**
   * Applies a function `f` to all rows.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(f: T => Unit): Unit = withNewRDDExecutionId {
    rdd.foreach(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each element of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))

  /**
   * Applies a function `f` to each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId {
    rdd.foreachPartition(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = {
    foreachPartition((it: Iterator[T]) => func.call(it.asJava))
  }

  /**
   * Returns the first `n` rows in the Dataset.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def take(n: Int): Array[T] = head(n)

  /**
   * Returns the first `n` rows in the Dataset as a list.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)

  /**
   * Returns an array that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * For Java API, use [[collectAsList]].
   *
   * @group action
   * @since 1.6.0
   */
  def collect(): Array[T] = withAction("collect", queryExecution)(collectFromPlan)

  /**
   * Returns a Java list that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def collectAsList(): java.util.List[T] = withAction("collectAsList", queryExecution) { plan =>
    val values = collectFromPlan(plan)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Returns an iterator that contains all rows in this Dataset.
   *
   * The iterator will consume as much memory as the largest partition in this Dataset.
   *
   * @note this results in multiple Spark jobs, and if the input Dataset is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input Dataset should be cached first.
   *
   * @group action
   * @since 2.0.0
   */
  def toLocalIterator(): java.util.Iterator[T] = {
    withAction("toLocalIterator", queryExecution) { plan =>
      // This projection writes output to a `InternalRow`, which means applying this projection is
      // not thread-safe. Here we create the projection inside this method to make `Dataset`
      // thread-safe.
      val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
      plan.executeToIterator().map { row =>
        // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
        // parameter of its `get` method, so it's safe to use null here.
        objProj(row).get(0, null).asInstanceOf[T]
      }.asJava
    }
  }

  /**
   * Returns the number of rows in the Dataset.
   * @group action
   * @since 1.6.0
   */
  def count(): Long = withAction("count", groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = true, logicalPlan)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments.
    // However, we don't want to complicate the semantics of this API method.
    // Instead, let's give users a friendly error message, pointing them to the new method.
    val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder])
    if (sortOrders.nonEmpty) throw new IllegalArgumentException(
      s"""Invalid partitionExprs specified: $sortOrders
         |For range partitioning use repartitionByRange(...) instead.
       """.stripMargin)
    withTypedPlan {
      RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(partitionExprs: Column*): Dataset[T] = {
    repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    require(partitionExprs.nonEmpty, "At least one partition-by expression must be specified.")
    val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match {
      case expr: SortOrder => expr
      case expr: Expression => SortOrder(expr, Ascending)
    })
    withTypedPlan {
      RepartitionByExpression(sortOrder, logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(partitionExprs: Column*): Dataset[T] = {
    repartitionByRange(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions
   * are requested. If a larger number of partitions is requested, it will stay at the current
   * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in
   * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not
   * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can call repartition. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @group typedrel
   * @since 1.6.0
   */
  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = false, logicalPlan)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `dropDuplicates`.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def distinct(): Dataset[T] = dropDuplicates()

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this)
    this
  }

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def cache(): this.type = persist()

  /**
   * Persist this Dataset with the given storage level.
   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,
   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,
   *                 `MEMORY_AND_DISK_2`, etc.
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(newLevel: StorageLevel): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)
    this
  }

  /**
   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
   *
   * @group basic
   * @since 2.1.0
   */
  def storageLevel: StorageLevel = {
    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>
      cachedData.cachedRepresentation.cacheBuilder.storageLevel
    }.getOrElse(StorageLevel.NONE)
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @param blocking Whether to block until all blocks are deleted.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(blocking: Boolean): this.type = {
    sparkSession.sharedState.cacheManager.uncacheQuery(this, cascade = false, blocking)
    this
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(): this.type = unpersist(blocking = false)

  // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`.
  @transient private lazy val rddQueryExecution: QueryExecution = {
    val deserialized = CatalystSerde.deserialize[T](logicalPlan)
    sparkSession.sessionState.executePlan(deserialized)
  }

  /**
   * Represents the content of the Dataset as an `RDD` of `T`.
   *
   * @group basic
   * @since 1.6.0
   */
  lazy val rdd: RDD[T] = {
    val objectType = exprEnc.deserializer.dataType
    rddQueryExecution.toRdd.mapPartitions { rows =>
      rows.map(_.get(0, objectType).asInstanceOf[T])
    }
  }

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def javaRDD: JavaRDD[T] = toJavaRDD

  /**
   * Registers this Dataset as a temporary table using the given name. The lifetime of this
   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  @deprecated("Use createOrReplaceTempView(viewName) instead.", "2.0.0")
  def registerTempTable(tableName: String): Unit = {
    createOrReplaceTempView(tableName)
  }

  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
   * created it, i.e. it will be automatically dropped when the session terminates. It's not
   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.0.0
   */
  @throws[AnalysisException]
  def createTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = false)
  }



  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 2.0.0
   */
  def createOrReplaceTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = false)
  }

  /**
   * Creates a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.1.0
   */
  @throws[AnalysisException]
  def createGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = true)
  }

  /**
   * Creates or replaces a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @group basic
   * @since 2.2.0
   */
  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = true)
  }

  private def createTempViewCommand(
      viewName: String,
      replace: Boolean,
      global: Boolean): CreateViewCommand = {
    val viewType = if (global) GlobalTempView else LocalTempView

    val tableIdentifier = try {
      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)
    } catch {
      case _: ParseException => throw new AnalysisException(s"Invalid view name: $viewName")
    }
    CreateViewCommand(
      name = tableIdentifier,
      userSpecifiedColumns = Nil,
      comment = None,
      properties = Map.empty,
      originalText = None,
      child = logicalPlan,
      allowExisting = false,
      replace = replace,
      viewType = viewType)
  }

  /**
   * Interface for saving the content of the non-streaming Dataset out into external storage.
   *
   * @group basic
   * @since 1.6.0
   */
  def write: DataFrameWriter[T] = {
    if (isStreaming) {
      logicalPlan.failAnalysis(
        "'write' can not be called on streaming Dataset/DataFrame")
    }
    new DataFrameWriter[T](this)
  }

  /**
   * Interface for saving the content of the streaming Dataset out into external storage.
   *
   * @group basic
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def writeStream: DataStreamWriter[T] = {
    if (!isStreaming) {
      logicalPlan.failAnalysis(
        "'writeStream' can be called only on streaming Dataset/DataFrame")
    }
    new DataStreamWriter[T](this)
  }


  /**
   * Returns the content of the Dataset as a Dataset of JSON strings.
   * @since 2.0.0
   */
  def toJSON: Dataset[String] = {
    val rowSchema = this.schema
    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone
    mapPartitions { iter =>
      val writer = new CharArrayWriter()
      // create the Generator without separator inserted between 2 records
      val gen = new JacksonGenerator(rowSchema, writer,
        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))

      new Iterator[String] {
        override def hasNext: Boolean = iter.hasNext
        override def next(): String = {
          gen.write(exprEnc.toRow(iter.next()))
          gen.flush()

          val json = writer.toString
          if (hasNext) {
            writer.reset()
          } else {
            gen.close()
          }

          json
        }
      }
    } (Encoders.STRING)
  }

  /**
   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply
   * asks each constituent BaseRelation for its respective files and takes the union of all results.
   * Depending on the source relations, this may not find all input files. Duplicates are removed.
   *
   * @group basic
   * @since 2.0.0
   */
  def inputFiles: Array[String] = {
    val files: Seq[String] = queryExecution.optimizedPlan.collect {
      case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) =>
        fsBasedRelation.inputFiles
      case fr: FileRelation =>
        fr.inputFiles
      case r: HiveTableRelation =>
        r.tableMeta.storage.locationUri.map(_.toString).toArray
    }.flatten
    files.toSet.toArray
  }

  ////////////////////////////////////////////////////////////////////////////
  // For Python API
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Converts a JavaRDD to a PythonRDD.
   */
  private[sql] def javaToPython: JavaRDD[Array[Byte]] = {
    val structType = schema  // capture it for closure
    val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType))
    EvaluatePython.javaToPython(rdd)
  }

  private[sql] def collectToPython(): Array[Any] = {
    EvaluatePython.registerPicklers()
    withAction("collectToPython", queryExecution) { plan =>
      val toJava: (Any) => Any = EvaluatePython.toJava(_, schema)
      val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
        plan.executeCollect().iterator.map(toJava))
      PythonRDD.serveIterator(iter, "serve-DataFrame")
    }
  }

  private[sql] def getRowsToPython(
      _numRows: Int,
      truncate: Int): Array[Any] = {
    EvaluatePython.registerPicklers()
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    val rows = getRows(numRows, truncate).map(_.toArray).toArray
    val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType)))
    val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
      rows.iterator.map(toJava))
    PythonRDD.serveIterator(iter, "serve-GetRows")
  }

  /**
   * Collect a Dataset as Arrow batches and serve stream to PySpark.
   */
  private[sql] def collectAsArrowToPython(): Array[Any] = {
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone

    PythonRDD.serveToStreamWithSync("serve-Arrow") { out =>
      withAction("collectAsArrowToPython", queryExecution) { plan =>
        val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId)
        val arrowBatchRdd = toArrowBatchRdd(plan)
        val numPartitions = arrowBatchRdd.partitions.length

        // Store collection results for worst case of 1 to N-1 partitions
        val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1))
        var lastIndex = -1  // index of last partition written

        // Handler to eagerly write partitions to Python in order
        def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = {
          // If result is from next partition in order
          if (index - 1 == lastIndex) {
            batchWriter.writeBatches(arrowBatches.iterator)
            lastIndex += 1
            // Write stored partitions that come next in order
            while (lastIndex < results.length && results(lastIndex) != null) {
              batchWriter.writeBatches(results(lastIndex).iterator)
              results(lastIndex) = null
              lastIndex += 1
            }
            // After last batch, end the stream
            if (lastIndex == results.length) {
              batchWriter.end()
            }
          } else {
            // Store partitions received out of order
            results(index - 1) = arrowBatches
          }
        }

        sparkSession.sparkContext.runJob(
          arrowBatchRdd,
          (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray,
          0 until numPartitions,
          handlePartitionBatches)
      }
    }
  }

  private[sql] def toPythonIterator(): Array[Any] = {
    withNewExecutionId {
      PythonRDD.toLocalIteratorAndServe(javaToPython.rdd)
    }
  }

  ////////////////////////////////////////////////////////////////////////////
  // Private Helpers
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with
   * an execution.
   */
  private def withNewExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, queryExecution)(body)
  }

  /**
   * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect
   * them with an execution. Before performing the action, the metrics of the executed plan will be
   * reset.
   */
  private def withNewRDDExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, rddQueryExecution) {
      rddQueryExecution.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      body
    }
  }

  /**
   * Wrap a Dataset action to track the QueryExecution and time cost, then report to the
   * user-registered callback functions.
   */
  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {
    try {
      qe.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      val start = System.nanoTime()
      val result = SQLExecution.withNewExecutionId(sparkSession, qe) {
        action(qe.executedPlan)
      }
      val end = System.nanoTime()
      sparkSession.listenerManager.onSuccess(name, qe, end - start)
      result
    } catch {
      case e: Throwable =>
        sparkSession.listenerManager.onFailure(name, qe, e)
        throw e
    }
  }

  /**
   * Collect all elements from a spark plan.
   */
  private def collectFromPlan(plan: SparkPlan): Array[T] = {
    // This projection writes output to a `InternalRow`, which means applying this projection is not
    // thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe.
    val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
    plan.executeCollect().map { row =>
      // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
      // parameter of its `get` method, so it's safe to use null here.
      objProj(row).get(0, null).asInstanceOf[T]
    }
  }

  private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = {
    val sortOrder: Seq[SortOrder] = sortExprs.map { col =>
      col.expr match {
        case expr: SortOrder =>
          expr
        case expr: Expression =>
          SortOrder(expr, Ascending)
      }
    }
    withTypedPlan {
      Sort(sortOrder, global = global, logicalPlan)
    }
  }

  /** A convenient function to wrap a logical plan and produce a DataFrame. */
  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = {
    Dataset.ofRows(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a logical plan and produce a Dataset. */
  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    Dataset(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a set based logical plan and produce a Dataset. */
  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) {
      // Set operators widen types (change the schema), so we cannot reuse the row encoder.
      Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]]
    } else {
      Dataset(sparkSession, logicalPlan)
    }
  }

  /** Convert to an RDD of serialized ArrowRecordBatches. */
  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = {
    val schemaCaptured = this.schema
    val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone
    plan.execute().mapPartitionsInternal { iter =>
      val context = TaskContext.get()
      ArrowConverters.toBatchIterator(
        iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context)
    }
  }

  // This is only used in tests, for now.
  private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = {
    toArrowBatchRdd(queryExecution.executedPlan)
  }
}

[0m2021.02.26 09:38:16 INFO  time: compiled root in 2.16s[0m
[0m2021.02.26 09:39:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 09:39:50 INFO  time: compiled root in 2.07s[0m
Feb 26, 2021 9:39:55 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 588
[0m2021.02.26 09:39:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 09:39:58 INFO  time: compiled root in 1.14s[0m
[0m2021.02.26 09:40:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 09:40:11 INFO  time: compiled root in 1.18s[0m
[0m2021.02.26 09:55:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 09:55:57 INFO  time: compiled root in 1.02s[0m
[0m2021.02.26 10:02:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:02:28 INFO  time: compiled root in 1.03s[0m
[0m2021.02.26 10:07:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:07:18 INFO  time: compiled root in 0.98s[0m
[0m2021.02.26 10:10:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:10:44 INFO  time: compiled root in 1.26s[0m
[0m2021.02.26 10:13:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:13:58 INFO  time: compiled root in 1.08s[0m
[0m2021.02.26 10:14:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:14:06 INFO  time: compiled root in 1.42s[0m
[0m2021.02.26 10:15:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:15:35 INFO  time: compiled root in 0.95s[0m
[0m2021.02.26 10:31:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:31:38 INFO  time: compiled root in 0.9s[0m
[0m2021.02.26 10:32:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:32:16 INFO  time: compiled root in 0.91s[0m
[0m2021.02.26 10:32:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:32:23 INFO  time: compiled root in 0.77s[0m
[0m2021.02.26 10:34:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:34:01 INFO  time: compiled root in 0.8s[0m
[0m2021.02.26 10:35:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:35:05 INFO  time: compiled root in 0.87s[0m
[0m2021.02.26 10:43:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:43:15 INFO  time: compiled root in 0.77s[0m
[0m2021.02.26 10:45:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:45:53 INFO  time: compiled root in 0.76s[0m
[0m2021.02.26 10:50:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 10:50:16 INFO  time: compiled root in 0.97s[0m
Feb 26, 2021 11:04:57 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1139
Feb 26, 2021 11:05:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1154
Feb 26, 2021 11:05:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1156
[0m2021.02.26 11:05:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:05:02 INFO  time: compiled root in 0.21s[0m
[0m2021.02.26 11:05:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:05:48 INFO  time: compiled root in 0.21s[0m
[0m2021.02.26 11:05:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:05:51 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 11:06:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:06:47 INFO  time: compiled root in 0.28s[0m
Feb 26, 2021 11:07:08 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1247
[0m2021.02.26 11:07:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:07:18 INFO  time: compiled root in 0.5s[0m
[0m2021.02.26 11:07:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:07:22 INFO  time: compiled root in 1.07s[0m
[0m2021.02.26 11:10:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:10:05 INFO  time: compiled root in 0.24s[0m
[0m2021.02.26 11:11:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:11:04 INFO  time: compiled root in 0.24s[0m
[0m2021.02.26 11:11:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:11:05 INFO  time: compiled root in 0.39s[0m
[0m2021.02.26 11:11:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:11:08 INFO  time: compiled root in 0.22s[0m
[0m2021.02.26 11:11:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:11:53 INFO  time: compiled root in 0.22s[0m
[0m2021.02.26 11:14:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:14:31 INFO  time: compiled root in 1.5s[0m
[0m2021.02.26 11:15:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:15:52 INFO  time: compiled root in 0.3s[0m
[0m2021.02.26 11:17:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:17:38 INFO  time: compiled root in 0.59s[0m
[0m2021.02.26 11:18:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:18:10 INFO  time: compiled root in 0.33s[0m
[0m2021.02.26 11:18:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:18:27 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 11:18:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:18:35 INFO  time: compiled root in 0.3s[0m
[0m2021.02.26 11:18:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:18:47 INFO  time: compiled root in 0.22s[0m
[0m2021.02.26 11:20:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:20:17 INFO  time: compiled root in 1.32s[0m
[0m2021.02.26 11:21:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:21:30 INFO  time: compiled root in 0.76s[0m
[0m2021.02.26 11:21:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:21:59 INFO  time: compiled root in 0.74s[0m
[0m2021.02.26 11:22:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:22:35 INFO  time: compiled root in 0.85s[0m
[0m2021.02.26 11:22:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:22:48 INFO  time: compiled root in 0.92s[0m
[0m2021.02.26 11:25:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:25:53 INFO  time: compiled root in 0.79s[0m
[0m2021.02.26 11:26:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:26:02 INFO  time: compiled root in 0.89s[0m
[0m2021.02.26 11:28:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:28:32 INFO  time: compiled root in 0.28s[0m
[0m2021.02.26 11:29:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:29:00 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 11:29:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:29:28 INFO  time: compiled root in 0.85s[0m
[0m2021.02.26 11:29:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:29:35 INFO  time: compiled root in 0.76s[0m
[0m2021.02.26 11:30:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:30:06 INFO  time: compiled root in 0.8s[0m
[0m2021.02.26 11:30:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:30:25 INFO  time: compiled root in 0.77s[0m
[0m2021.02.26 11:33:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:33:14 INFO  time: compiled root in 0.76s[0m
[0m2021.02.26 11:34:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:34:13 INFO  time: compiled root in 0.82s[0m
[0m2021.02.26 11:35:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:35:10 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 11:47:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:47:23 INFO  time: compiled root in 0.82s[0m
[0m2021.02.26 11:52:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:52:59 INFO  time: compiled root in 0.85s[0m
[0m2021.02.26 11:53:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:53:07 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 11:53:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:53:24 INFO  time: compiled root in 0.89s[0m
[0m2021.02.26 11:53:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:53:31 INFO  time: compiled root in 0.93s[0m
Feb 26, 2021 11:53:33 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.InterruptedException
java.util.concurrent.CompletionException: java.lang.InterruptedException
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:673)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:42)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.pc.VirtualFileParams.checkCanceled(VirtualFileParams.java:25)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:76)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:75)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.toStream(Iterator.scala:1320)
	at scala.collection.AbstractIterator.toStream(Iterator.scala:1334)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)
	at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:134)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:40)
	at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:69)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.write(ReflectiveTypeAdapterFactory.java:125)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.write(ReflectiveTypeAdapterFactory.java:243)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:423)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:55)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at com.google.gson.Gson.toJson(Gson.java:648)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:145)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:140)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:59)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.lambda$handleRequest$1(RemoteEndpoint.java:281)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
	... 9 more

[0m2021.02.26 11:53:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:53:33 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 11:54:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 11:54:03 INFO  time: compiled root in 1.26s[0m
[0m2021.02.26 12:00:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:00:43 INFO  time: compiled root in 0.22s[0m
[0m2021.02.26 12:00:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:00:53 INFO  time: compiled root in 1.03s[0m
[0m2021.02.26 12:01:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:01:08 INFO  time: compiled root in 1.28s[0m
[0m2021.02.26 12:04:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:04:33 INFO  time: compiled root in 0.89s[0m
[0m2021.02.26 12:12:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:12:40 INFO  time: compiled root in 0.97s[0m
[0m2021.02.26 12:20:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:20:25 INFO  time: compiled root in 0.89s[0m
[0m2021.02.26 12:22:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:22:26 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 12:22:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:22:30 INFO  time: compiled root in 0.93s[0m
[0m2021.02.26 12:23:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:23:56 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 12:24:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:24:02 INFO  time: compiled root in 0.85s[0m
[0m2021.02.26 12:25:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:25:14 INFO  time: compiled root in 0.93s[0m
[0m2021.02.26 12:25:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:25:49 INFO  time: compiled root in 0.78s[0m
[0m2021.02.26 12:25:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:25:51 INFO  time: compiled root in 0.86s[0m
[0m2021.02.26 12:27:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:27:11 INFO  time: compiled root in 0.82s[0m
[0m2021.02.26 12:28:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:28:10 INFO  time: compiled root in 0.92s[0m
Feb 26, 2021 12:28:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
java.util.concurrent.CompletionException: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:673)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:42)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
	at scala.reflect.internal.Symbols$Symbol$$anonfun$info$3.apply(Symbols.scala:1523)
	at scala.reflect.internal.Symbols$Symbol$$anonfun$info$3.apply(Symbols.scala:1521)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.reflect.internal.Symbols$Symbol.lock(Symbols.scala:567)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1521)
	at scala.tools.nsc.typechecker.Contexts$ImportInfo.qual(Contexts.scala:1416)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.pre$lzycompute$1(Completions.scala:719)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.scala$meta$internal$pc$completions$Completions$class$$anonfun$$pre$1(Completions.scala:719)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1$$anonfun$apply$1.apply(Completions.scala:722)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1$$anonfun$apply$1.apply(Completions.scala:720)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.apply(Completions.scala:720)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.apply(Completions.scala:718)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.meta.internal.pc.completions.Completions$class.renamedSymbols(Completions.scala:718)
	at scala.meta.internal.pc.MetalsGlobal.renamedSymbols(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.Signatures$ShortenedNames$.synthesize(Signatures.scala:56)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:151)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:75)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.toStream(Iterator.scala:1320)
	at scala.collection.AbstractIterator.toStream(Iterator.scala:1334)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)
	at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:134)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:40)
	at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:69)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.write(ReflectiveTypeAdapterFactory.java:125)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.write(ReflectiveTypeAdapterFactory.java:243)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:423)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:55)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at com.google.gson.Gson.toJson(Gson.java:648)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:145)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:140)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:59)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.lambda$handleRequest$1(RemoteEndpoint.java:281)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
	... 9 more

[0m2021.02.26 12:28:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:28:37 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 12:28:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:28:48 INFO  time: compiled root in 1s[0m
[0m2021.02.26 12:30:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:30:41 INFO  time: compiled root in 0.92s[0m
[0m2021.02.26 12:30:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:30:45 INFO  time: compiled root in 1s[0m
[0m2021.02.26 12:32:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:32:18 INFO  time: compiled root in 0.84s[0m
[0m2021.02.26 12:34:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:34:22 INFO  time: compiled root in 0.84s[0m
Feb 26, 2021 12:35:38 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.02.26 12:35:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:35:40 INFO  time: compiled root in 0.89s[0m
[0m2021.02.26 12:37:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 12:37:12 INFO  time: compiled root in 0.96s[0m
[0m2021.02.26 13:59:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:59:49 INFO  time: compiled root in 0.17s[0m
Feb 26, 2021 1:59:52 PM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTraceNotification
[0m2021.02.26 13:59:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 13:59:59 INFO  time: compiled root in 0.99s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.types

import scala.collection.mutable.ArrayBuffer
import scala.util.Try
import scala.util.control.NonFatal

import org.json4s.JsonDSL._

import org.apache.spark.SparkException
import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.sql.catalyst.expressions.{Attribute, AttributeReference, InterpretedOrdering}
import org.apache.spark.sql.catalyst.parser.{CatalystSqlParser, LegacyTypeStringParser}
import org.apache.spark.sql.catalyst.util.{escapeSingleQuotedString, quoteIdentifier}
import org.apache.spark.util.Utils

/**
 * A [[StructType]] object can be constructed by
 * {{{
 * StructType(fields: Seq[StructField])
 * }}}
 * For a [[StructType]] object, one or multiple [[StructField]]s can be extracted by names.
 * If multiple [[StructField]]s are extracted, a [[StructType]] object will be returned.
 * If a provided name does not have a matching field, it will be ignored. For the case
 * of extracting a single [[StructField]], a `null` will be returned.
 *
 * Scala Example:
 * {{{
 * import org.apache.spark.sql._
 * import org.apache.spark.sql.types._
 *
 * val struct =
 *   StructType(
 *     StructField("a", IntegerType, true) ::
 *     StructField("b", LongType, false) ::
 *     StructField("c", BooleanType, false) :: Nil)
 *
 * // Extract a single StructField.
 * val singleField = struct("b")
 * // singleField: StructField = StructField(b,LongType,false)
 *
 * // If this struct does not have a field called "d", it throws an exception.
 * struct("d")
 * // java.lang.IllegalArgumentException: Field "d" does not exist.
 * //   ...
 *
 * // Extract multiple StructFields. Field names are provided in a set.
 * // A StructType object will be returned.
 * val twoFields = struct(Set("b", "c"))
 * // twoFields: StructType =
 * //   StructType(StructField(b,LongType,false), StructField(c,BooleanType,false))
 *
 * // Any names without matching fields will throw an exception.
 * // For the case shown below, an exception is thrown due to "d".
 * struct(Set("b", "c", "d"))
 * // java.lang.IllegalArgumentException: Field "d" does not exist.
 * //    ...
 * }}}
 *
 * A [[org.apache.spark.sql.Row]] object is used as a value of the [[StructType]].
 *
 * Scala Example:
 * {{{
 * import org.apache.spark.sql._
 * import org.apache.spark.sql.types._
 *
 * val innerStruct =
 *   StructType(
 *     StructField("f1", IntegerType, true) ::
 *     StructField("f2", LongType, false) ::
 *     StructField("f3", BooleanType, false) :: Nil)
 *
 * val struct = StructType(
 *   StructField("a", innerStruct, true) :: Nil)
 *
 * // Create a Row with the schema defined by struct
 * val row = Row(Row(1, 2, true))
 * }}}
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
case class StructType(fields: Array[StructField]) extends DataType with Seq[StructField] {

  /** No-arg constructor for kryo. */
  def this() = this(Array.empty[StructField])

  /** Returns all field names in an array. */
  def fieldNames: Array[String] = fields.map(_.name)

  /**
   * Returns all field names in an array. This is an alias of `fieldNames`.
   *
   * @since 2.4.0
   */
  def names: Array[String] = fieldNames

  private lazy val fieldNamesSet: Set[String] = fieldNames.toSet
  private lazy val nameToField: Map[String, StructField] = fields.map(f => f.name -> f).toMap
  private lazy val nameToIndex: Map[String, Int] = fieldNames.zipWithIndex.toMap

  override def equals(that: Any): Boolean = {
    that match {
      case StructType(otherFields) =>
        java.util.Arrays.equals(
          fields.asInstanceOf[Array[AnyRef]], otherFields.asInstanceOf[Array[AnyRef]])
      case _ => false
    }
  }

  private lazy val _hashCode: Int = java.util.Arrays.hashCode(fields.asInstanceOf[Array[AnyRef]])
  override def hashCode(): Int = _hashCode

  /**
   * Creates a new [[StructType]] by adding a new field.
   * {{{
   * val struct = (new StructType)
   *   .add(StructField("a", IntegerType, true))
   *   .add(StructField("b", LongType, false))
   *   .add(StructField("c", StringType, true))
   *}}}
   */
  def add(field: StructField): StructType = {
    StructType(fields :+ field)
  }

  /**
   * Creates a new [[StructType]] by adding a new nullable field with no metadata.
   *
   * val struct = (new StructType)
   *   .add("a", IntegerType)
   *   .add("b", LongType)
   *   .add("c", StringType)
   */
  def add(name: String, dataType: DataType): StructType = {
    StructType(fields :+ StructField(name, dataType, nullable = true, Metadata.empty))
  }

  /**
   * Creates a new [[StructType]] by adding a new field with no metadata.
   *
   * val struct = (new StructType)
   *   .add("a", IntegerType, true)
   *   .add("b", LongType, false)
   *   .add("c", StringType, true)
   */
  def add(name: String, dataType: DataType, nullable: Boolean): StructType = {
    StructType(fields :+ StructField(name, dataType, nullable, Metadata.empty))
  }

  /**
   * Creates a new [[StructType]] by adding a new field and specifying metadata.
   * {{{
   * val struct = (new StructType)
   *   .add("a", IntegerType, true, Metadata.empty)
   *   .add("b", LongType, false, Metadata.empty)
   *   .add("c", StringType, true, Metadata.empty)
   * }}}
   */
  def add(
      name: String,
      dataType: DataType,
      nullable: Boolean,
      metadata: Metadata): StructType = {
    StructType(fields :+ StructField(name, dataType, nullable, metadata))
  }

  /**
   * Creates a new [[StructType]] by adding a new field and specifying metadata.
   * {{{
   * val struct = (new StructType)
   *   .add("a", IntegerType, true, "comment1")
   *   .add("b", LongType, false, "comment2")
   *   .add("c", StringType, true, "comment3")
   * }}}
   */
  def add(
      name: String,
      dataType: DataType,
      nullable: Boolean,
      comment: String): StructType = {
    StructType(fields :+ StructField(name, dataType, nullable).withComment(comment))
  }

  /**
   * Creates a new [[StructType]] by adding a new nullable field with no metadata where the
   * dataType is specified as a String.
   *
   * {{{
   * val struct = (new StructType)
   *   .add("a", "int")
   *   .add("b", "long")
   *   .add("c", "string")
   * }}}
   */
  def add(name: String, dataType: String): StructType = {
    add(name, CatalystSqlParser.parseDataType(dataType), nullable = true, Metadata.empty)
  }

  /**
   * Creates a new [[StructType]] by adding a new field with no metadata where the
   * dataType is specified as a String.
   *
   * {{{
   * val struct = (new StructType)
   *   .add("a", "int", true)
   *   .add("b", "long", false)
   *   .add("c", "string", true)
   * }}}
   */
  def add(name: String, dataType: String, nullable: Boolean): StructType = {
    add(name, CatalystSqlParser.parseDataType(dataType), nullable, Metadata.empty)
  }

  /**
   * Creates a new [[StructType]] by adding a new field and specifying metadata where the
   * dataType is specified as a String.
   * {{{
   * val struct = (new StructType)
   *   .add("a", "int", true, Metadata.empty)
   *   .add("b", "long", false, Metadata.empty)
   *   .add("c", "string", true, Metadata.empty)
   * }}}
   */
  def add(
      name: String,
      dataType: String,
      nullable: Boolean,
      metadata: Metadata): StructType = {
    add(name, CatalystSqlParser.parseDataType(dataType), nullable, metadata)
  }

  /**
   * Creates a new [[StructType]] by adding a new field and specifying metadata where the
   * dataType is specified as a String.
   * {{{
   * val struct = (new StructType)
   *   .add("a", "int", true, "comment1")
   *   .add("b", "long", false, "comment2")
   *   .add("c", "string", true, "comment3")
   * }}}
   */
  def add(
      name: String,
      dataType: String,
      nullable: Boolean,
      comment: String): StructType = {
    add(name, CatalystSqlParser.parseDataType(dataType), nullable, comment)
  }

  /**
   * Extracts the [[StructField]] with the given name.
   *
   * @throws IllegalArgumentException if a field with the given name does not exist
   */
  def apply(name: String): StructField = {
    nameToField.getOrElse(name,
      throw new IllegalArgumentException(
        s"""Field "$name" does not exist.
           |Available fields: ${fieldNames.mkString(", ")}""".stripMargin))
  }

  /**
   * Returns a [[StructType]] containing [[StructField]]s of the given names, preserving the
   * original order of fields.
   *
   * @throws IllegalArgumentException if a field cannot be found for any of the given names
   */
  def apply(names: Set[String]): StructType = {
    val nonExistFields = names -- fieldNamesSet
    if (nonExistFields.nonEmpty) {
      throw new IllegalArgumentException(
        s"""Nonexistent field(s): ${nonExistFields.mkString(", ")}.
           |Available fields: ${fieldNames.mkString(", ")}""".stripMargin)
    }
    // Preserve the original order of fields.
    StructType(fields.filter(f => names.contains(f.name)))
  }

  /**
   * Returns the index of a given field.
   *
   * @throws IllegalArgumentException if a field with the given name does not exist
   */
  def fieldIndex(name: String): Int = {
    nameToIndex.getOrElse(name,
      throw new IllegalArgumentException(
        s"""Field "$name" does not exist.
           |Available fields: ${fieldNames.mkString(", ")}""".stripMargin))
  }

  private[sql] def getFieldIndex(name: String): Option[Int] = {
    nameToIndex.get(name)
  }

  protected[sql] def toAttributes: Seq[AttributeReference] =
    map(f => AttributeReference(f.name, f.dataType, f.nullable, f.metadata)())

  def treeString: String = {
    val builder = new StringBuilder
    builder.append("root\n")
    val prefix = " |"
    fields.foreach(field => field.buildFormattedString(prefix, builder))

    builder.toString()
  }

  // scalastyle:off println
  def printTreeString(): Unit = println(treeString)
  // scalastyle:on println

  private[sql] def buildFormattedString(prefix: String, builder: StringBuilder): Unit = {
    fields.foreach(field => field.buildFormattedString(prefix, builder))
  }

  override private[sql] def jsonValue =
    ("type" -> typeName) ~
      ("fields" -> map(_.jsonValue))

  override def apply(fieldIndex: Int): StructField = fields(fieldIndex)

  override def length: Int = fields.length

  override def iterator: Iterator[StructField] = fields.iterator

  /**
   * The default size of a value of the StructType is the total default sizes of all field types.
   */
  override def defaultSize: Int = fields.map(_.dataType.defaultSize).sum

  override def simpleString: String = {
    val fieldTypes = fields.view.map(field => s"${field.name}:${field.dataType.simpleString}")
    Utils.truncatedString(fieldTypes, "struct<", ",", ">")
  }

  override def catalogString: String = {
    // in catalogString, we should not truncate
    val fieldTypes = fields.map(field => s"${field.name}:${field.dataType.catalogString}")
    s"struct<${fieldTypes.mkString(",")}>"
  }

  override def sql: String = {
    val fieldTypes = fields.map(f => s"${quoteIdentifier(f.name)}: ${f.dataType.sql}")
    s"STRUCT<${fieldTypes.mkString(", ")}>"
  }

  /**
   * Returns a string containing a schema in DDL format. For example, the following value:
   * `StructType(Seq(StructField("eventId", IntegerType), StructField("s", StringType)))`
   * will be converted to `eventId` INT, `s` STRING.
   * The returned DDL schema can be used in a table creation.
   *
   * @since 2.4.0
   */
  def toDDL: String = fields.map(_.toDDL).mkString(",")

  private[sql] override def simpleString(maxNumberFields: Int): String = {
    val builder = new StringBuilder
    val fieldTypes = fields.take(maxNumberFields).map {
      f => s"${f.name}: ${f.dataType.simpleString(maxNumberFields)}"
    }
    builder.append("struct<")
    builder.append(fieldTypes.mkString(", "))
    if (fields.length > 2) {
      if (fields.length - fieldTypes.length == 1) {
        builder.append(" ... 1 more field")
      } else {
        builder.append(" ... " + (fields.length - 2) + " more fields")
      }
    }
    builder.append(">").toString()
  }

  /**
   * Merges with another schema (`StructType`).  For a struct field A from `this` and a struct field
   * B from `that`,
   *
   * 1. If A and B have the same name and data type, they are merged to a field C with the same name
   *    and data type.  C is nullable if and only if either A or B is nullable.
   * 2. If A doesn't exist in `that`, it's included in the result schema.
   * 3. If B doesn't exist in `this`, it's also included in the result schema.
   * 4. Otherwise, `this` and `that` are considered as conflicting schemas and an exception would be
   *    thrown.
   */
  private[sql] def merge(that: StructType): StructType =
    StructType.merge(this, that).asInstanceOf[StructType]

  override private[spark] def asNullable: StructType = {
    val newFields = fields.map {
      case StructField(name, dataType, nullable, metadata) =>
        StructField(name, dataType.asNullable, nullable = true, metadata)
    }

    StructType(newFields)
  }

  override private[spark] def existsRecursively(f: (DataType) => Boolean): Boolean = {
    f(this) || fields.exists(field => field.dataType.existsRecursively(f))
  }

  @transient
  private[sql] lazy val interpretedOrdering =
    InterpretedOrdering.forSchema(this.fields.map(_.dataType))
}

/**
 * @since 1.3.0
 */
@InterfaceStability.Stable
object StructType extends AbstractDataType {

  override private[sql] def defaultConcreteType: DataType = new StructType

  override private[sql] def acceptsType(other: DataType): Boolean = {
    other.isInstanceOf[StructType]
  }

  override private[sql] def simpleString: String = "struct"

  private[sql] def fromString(raw: String): StructType = {
    Try(DataType.fromJson(raw)).getOrElse(LegacyTypeStringParser.parse(raw)) match {
      case t: StructType => t
      case _ => throw new RuntimeException(s"Failed parsing ${StructType.simpleString}: $raw")
    }
  }

  /**
   * Creates StructType for a given DDL-formatted string, which is a comma separated list of field
   * definitions, e.g., a INT, b STRING.
   *
   * @since 2.2.0
   */
  def fromDDL(ddl: String): StructType = CatalystSqlParser.parseTableSchema(ddl)

  def apply(fields: Seq[StructField]): StructType = StructType(fields.toArray)

  def apply(fields: java.util.List[StructField]): StructType = {
    import scala.collection.JavaConverters._
    StructType(fields.asScala)
  }

  private[sql] def fromAttributes(attributes: Seq[Attribute]): StructType =
    StructType(attributes.map(a => StructField(a.name, a.dataType, a.nullable, a.metadata)))

  private[sql] def removeMetadata(key: String, dt: DataType): DataType =
    dt match {
      case StructType(fields) =>
        val newFields = fields.map { f =>
          val mb = new MetadataBuilder()
          f.copy(dataType = removeMetadata(key, f.dataType),
            metadata = mb.withMetadata(f.metadata).remove(key).build())
        }
        StructType(newFields)
      case _ => dt
    }

  private[sql] def merge(left: DataType, right: DataType): DataType =
    (left, right) match {
      case (ArrayType(leftElementType, leftContainsNull),
      ArrayType(rightElementType, rightContainsNull)) =>
        ArrayType(
          merge(leftElementType, rightElementType),
          leftContainsNull || rightContainsNull)

      case (MapType(leftKeyType, leftValueType, leftContainsNull),
      MapType(rightKeyType, rightValueType, rightContainsNull)) =>
        MapType(
          merge(leftKeyType, rightKeyType),
          merge(leftValueType, rightValueType),
          leftContainsNull || rightContainsNull)

      case (StructType(leftFields), StructType(rightFields)) =>
        val newFields = ArrayBuffer.empty[StructField]

        val rightMapped = fieldsMap(rightFields)
        leftFields.foreach {
          case leftField @ StructField(leftName, leftType, leftNullable, _) =>
            rightMapped.get(leftName)
              .map { case rightField @ StructField(rightName, rightType, rightNullable, _) =>
                try {
                  leftField.copy(
                    dataType = merge(leftType, rightType),
                    nullable = leftNullable || rightNullable)
                } catch {
                  case NonFatal(e) =>
                    throw new SparkException(s"Failed to merge fields '$leftName' and " +
                      s"'$rightName'. " + e.getMessage)
                }
              }
              .orElse {
                Some(leftField)
              }
              .foreach(newFields += _)
        }

        val leftMapped = fieldsMap(leftFields)
        rightFields
          .filterNot(f => leftMapped.get(f.name).nonEmpty)
          .foreach { f =>
            newFields += f
          }

        StructType(newFields)

      case (DecimalType.Fixed(leftPrecision, leftScale),
        DecimalType.Fixed(rightPrecision, rightScale)) =>
        if ((leftPrecision == rightPrecision) && (leftScale == rightScale)) {
          DecimalType(leftPrecision, leftScale)
        } else if ((leftPrecision != rightPrecision) && (leftScale != rightScale)) {
          throw new SparkException("Failed to merge decimal types with incompatible " +
            s"precision $leftPrecision and $rightPrecision & scale $leftScale and $rightScale")
        } else if (leftPrecision != rightPrecision) {
          throw new SparkException("Failed to merge decimal types with incompatible " +
            s"precision $leftPrecision and $rightPrecision")
        } else {
          throw new SparkException("Failed to merge decimal types with incompatible " +
            s"scala $leftScale and $rightScale")
        }

      case (leftUdt: UserDefinedType[_], rightUdt: UserDefinedType[_])
        if leftUdt.userClass == rightUdt.userClass => leftUdt

      case (leftType, rightType) if leftType == rightType =>
        leftType

      case _ =>
        throw new SparkException(s"Failed to merge incompatible data types ${left.catalogString}" +
          s" and ${right.catalogString}")
    }

  private[sql] def fieldsMap(fields: Array[StructField]): Map[String, StructField] = {
    import scala.collection.breakOut
    fields.map(s => (s.name, s))(breakOut)
  }
}

[0m2021.02.26 14:04:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:04:54 INFO  time: compiled root in 1s[0m
[0m2021.02.26 14:05:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:05:49 INFO  time: compiled root in 1.93s[0m
[0m2021.02.26 14:07:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:07:22 INFO  time: compiled root in 0.96s[0m
[0m2021.02.26 14:09:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:09:24 INFO  time: compiled root in 0.92s[0m
[0m2021.02.26 14:11:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:11:33 INFO  time: compiled root in 0.95s[0m
[0m2021.02.26 14:13:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:13:08 INFO  time: compiled root in 0.84s[0m
Feb 26, 2021 2:13:09 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.02.26 14:23:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:23:04 INFO  time: compiled root in 0.92s[0m
[0m2021.02.26 14:23:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:23:29 INFO  time: compiled root in 0.87s[0m
[0m2021.02.26 14:23:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:23:33 INFO  time: compiled root in 1.04s[0m
Feb 26, 2021 2:23:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5169
[0m2021.02.26 14:24:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:24:26 INFO  time: compiled root in 0.85s[0m
[0m2021.02.26 14:26:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:26:53 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 14:27:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:27:05 INFO  time: compiled root in 1.83s[0m
[0m2021.02.26 14:27:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:27:17 INFO  time: compiled root in 1.17s[0m
[0m2021.02.26 14:27:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:27:22 INFO  time: compiled root in 1.35s[0m
[0m2021.02.26 14:28:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:28:19 INFO  time: compiled root in 1.13s[0m
[0m2021.02.26 14:31:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:31:13 INFO  time: compiled root in 1.14s[0m
[0m2021.02.26 14:31:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:31:16 INFO  time: compiled root in 1.1s[0m
[0m2021.02.26 14:31:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:31:18 INFO  time: compiled root in 1.05s[0m
[0m2021.02.26 14:31:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:32:00 INFO  time: compiled root in 1.49s[0m
[0m2021.02.26 14:32:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:32:11 INFO  time: compiled root in 1.16s[0m
[0m2021.02.26 14:32:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:32:20 INFO  time: compiled root in 1.05s[0m
[0m2021.02.26 14:34:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:34:05 INFO  time: compiled root in 0.21s[0m
[0m2021.02.26 14:34:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:34:25 INFO  time: compiled root in 1.21s[0m
[0m2021.02.26 14:42:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:42:09 INFO  time: compiled root in 1.11s[0m
[0m2021.02.26 14:42:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:42:11 INFO  time: compiled root in 0.96s[0m
[0m2021.02.26 14:42:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:42:16 INFO  time: compiled root in 1.21s[0m
[0m2021.02.26 14:45:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:45:06 INFO  time: compiled root in 1s[0m
[0m2021.02.26 14:45:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:45:47 INFO  time: compiled root in 1.04s[0m
[0m2021.02.26 14:47:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:47:47 INFO  time: compiled root in 0.94s[0m
[0m2021.02.26 14:47:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:47:55 INFO  time: compiled root in 1.6s[0m
[0m2021.02.26 14:59:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:59:20 INFO  time: compiled root in 1.01s[0m
[0m2021.02.26 14:59:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 14:59:57 INFO  time: compiled root in 1.31s[0m
[0m2021.02.26 15:00:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:00:26 INFO  time: compiled root in 1.1s[0m
[0m2021.02.26 15:03:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:03:12 INFO  time: compiled root in 0.22s[0m
[0m2021.02.26 15:03:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:03:22 INFO  time: compiled root in 1.16s[0m
[0m2021.02.26 15:04:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:04:09 INFO  time: compiled root in 1.06s[0m
[0m2021.02.26 15:04:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:04:19 INFO  time: compiled root in 1.07s[0m
[0m2021.02.26 15:07:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:07:28 INFO  time: compiled root in 0.75s[0m
[0m2021.02.26 15:07:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:07:43 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 15:10:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:10:55 INFO  time: compiled root in 1.3s[0m
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[<error>]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1033, 1033, 1047)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[<error>]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1033, 1033, 1040)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[<error>]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1033, 1033, 1043)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[String]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1033, 1033, 1045)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[String]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1033, 1033, 1046)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[String]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1033, 1033, 1046)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[String]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1033, 1033, 1045)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[String]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1033, 1033, 1045)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[String]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1033, 1033, 1045)
[0m2021.02.26 15:15:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:15:25 INFO  time: compiled root in 0.28s[0m
[0m2021.02.26 15:15:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:15:32 INFO  time: compiled root in 1.02s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.types

import org.json4s.JsonAST.JValue
import org.json4s.JsonDSL._

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.sql.catalyst.util.{escapeSingleQuotedString, quoteIdentifier}

/**
 * A field inside a StructType.
 * @param name The name of this field.
 * @param dataType The data type of this field.
 * @param nullable Indicates if values of this field can be `null` values.
 * @param metadata The metadata of this field. The metadata should be preserved during
 *                 transformation if the content of the column is not modified, e.g, in selection.
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
case class StructField(
    name: String,
    dataType: DataType,
    nullable: Boolean = true,
    metadata: Metadata = Metadata.empty) {

  /** No-arg constructor for kryo. */
  protected def this() = this(null, null)

  private[sql] def buildFormattedString(prefix: String, builder: StringBuilder): Unit = {
    builder.append(s"$prefix-- $name: ${dataType.typeName} (nullable = $nullable)\n")
    DataType.buildFormattedString(dataType, s"$prefix    |", builder)
  }

  // override the default toString to be compatible with legacy parquet files.
  override def toString: String = s"StructField($name,$dataType,$nullable)"

  private[sql] def jsonValue: JValue = {
    ("name" -> name) ~
      ("type" -> dataType.jsonValue) ~
      ("nullable" -> nullable) ~
      ("metadata" -> metadata.jsonValue)
  }

  /**
   * Updates the StructField with a new comment value.
   */
  def withComment(comment: String): StructField = {
    val newMetadata = new MetadataBuilder()
      .withMetadata(metadata)
      .putString("comment", comment)
      .build()
    copy(metadata = newMetadata)
  }

  /**
   * Return the comment of this StructField.
   */
  def getComment(): Option[String] = {
    if (metadata.contains("comment")) Option(metadata.getString("comment")) else None
  }

  /**
   * Returns a string containing a schema in DDL format. For example, the following value:
   * `StructField("eventId", IntegerType)` will be converted to `eventId` INT.
   *
   * @since 2.4.0
   */
  def toDDL: String = {
    val comment = getComment()
      .map(escapeSingleQuotedString)
      .map(" COMMENT '" + _ + "'")

    s"${quoteIdentifier(name)} ${dataType.sql}${comment.getOrElse("")}"
  }
}

[0m2021.02.26 15:16:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:16:39 INFO  time: compiled root in 0.23s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql.types

import scala.math.Ordering
import scala.reflect.runtime.universe.typeTag

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.unsafe.types.UTF8String

/**
 * The data type representing `String` values. Please use the singleton `DataTypes.StringType`.
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class StringType private() extends AtomicType {
  // The companion object and this class is separated so the companion object also subclasses
  // this type. Otherwise, the companion object would be of type "StringType$" in byte code.
  // Defined with a private constructor so the companion object is the only possible instantiation.
  private[sql] type InternalType = UTF8String
  @transient private[sql] lazy val tag = typeTag[InternalType]
  private[sql] val ordering = implicitly[Ordering[InternalType]]

  /**
   * The default size of a value of the StringType is 20 bytes.
   */
  override def defaultSize: Int = 20

  private[spark] override def asNullable: StringType = this
}

/**
 * @since 1.3.0
 */
@InterfaceStability.Stable
case object StringType extends StringType


[0m2021.02.26 15:17:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:17:20 INFO  time: compiled root in 1.12s[0m
[0m2021.02.26 15:17:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:17:25 INFO  time: compiled root in 1.04s[0m
[0m2021.02.26 15:17:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:17:34 INFO  time: compiled root in 1.08s[0m
Feb 26, 2021 3:17:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8345
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[<error>]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1833, 1833, 1847)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[<error>]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1833, 1833, 1840)
Feb 26, 2021 3:18:02 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[String]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1833, 1833, 1845)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[String]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1833, 1833, 1846)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[String]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1833, 1833, 1846)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[String]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1833, 1833, 1845)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[String]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1833, 1833, 1845)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in List[String]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 1833, 1833, 1845)
Feb 26, 2021 3:18:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8411
Feb 26, 2021 3:18:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8413
/*
 * Scala (https://www.scala-lang.org)
 *
 * Copyright EPFL and Lightbend, Inc.
 *
 * Licensed under Apache License 2.0
 * (http://www.apache.org/licenses/LICENSE-2.0).
 *
 * See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.
 */

package scala
package collection

import scala.annotation.tailrec

/** A template trait for linear sequences of type `LinearSeq[A]` which optimizes
 *  the implementation of various methods under the assumption of fast linear access.
 *
 *  $linearSeqOptim
 *
 *  @define  linearSeqOptim
 *  Linear-optimized sequences implement most operations in in terms of three methods,
 *  which are assumed to have efficient implementations. These are:
 *  {{{
 *     def isEmpty: Boolean
 *     def head: A
 *     def tail: Repr
 *  }}}
 *  Here, `A` is the type of the sequence elements and `Repr` is the type of the sequence itself.
 *  Note that default implementations are provided via inheritance, but these
 *  should be overridden for performance.
 *
 *
 */
trait LinearSeqOptimized[+A, +Repr <: LinearSeqOptimized[A, Repr]] extends LinearSeqLike[A, Repr] { self: Repr =>

  def isEmpty: Boolean

  def head: A

  def tail: Repr

  /** The length of the $coll.
   *
   *  $willNotTerminateInf
   *
   *  Note: the execution of `length` may take time proportional to the length of the sequence.
   */
  def length: Int = {
    var these = self
    var len = 0
    while (!these.isEmpty) {
      len += 1
      these = these.tail
    }
    len
  }

  /** Selects an element by its index in the $coll.
   *  Note: the execution of `apply` may take time proportional to the index value.
   *  @throws IndexOutOfBoundsException if `idx` does not satisfy `0 <= idx < length`.
   */
  def apply(n: Int): A = {
    val rest = drop(n)
    if (n < 0 || rest.isEmpty) throw new IndexOutOfBoundsException("" + n)
    rest.head
  }

  override /*IterableLike*/
  def foreach[U](f: A => U) {
    var these = this
    while (!these.isEmpty) {
      f(these.head)
      these = these.tail
    }
  }


  override /*IterableLike*/
  def forall(p: A => Boolean): Boolean = {
    var these = this
    while (!these.isEmpty) {
      if (!p(these.head)) return false
      these = these.tail
    }
    true
  }

  override /*IterableLike*/
  def exists(p: A => Boolean): Boolean = {
    var these = this
    while (!these.isEmpty) {
      if (p(these.head)) return true
      these = these.tail
    }
    false
  }

  override /*SeqLike*/
  def contains[A1 >: A](elem: A1): Boolean = {
    var these = this
    while (!these.isEmpty) {
      if (these.head == elem) return true
      these = these.tail
    }
    false
  }

  override /*IterableLike*/
  def find(p: A => Boolean): Option[A] = {
    var these = this
    while (!these.isEmpty) {
      if (p(these.head)) return Some(these.head)
      these = these.tail
    }
    None
  }

  override /*TraversableLike*/
  def foldLeft[B](z: B)(@deprecatedName('f) op: (B, A) => B): B = {
    var acc = z
    var these = this
    while (!these.isEmpty) {
      acc = op(acc, these.head)
      these = these.tail
    }
    acc
  }

  override /*IterableLike*/
  def foldRight[B](z: B)(@deprecatedName('f) op: (A, B) => B): B =
    if (this.isEmpty) z
    else op(head, tail.foldRight(z)(op))

  override /*TraversableLike*/
  def reduceLeft[B >: A](@deprecatedName('f) op: (B, A) => B): B =
    if (isEmpty) throw new UnsupportedOperationException("empty.reduceLeft")
    else tail.foldLeft[B](head)(op)

  override /*IterableLike*/
  def reduceRight[B >: A](op: (A, B) => B): B =
    if (isEmpty) throw new UnsupportedOperationException("Nil.reduceRight")
    else if (tail.isEmpty) head
    else op(head, tail.reduceRight(op))

  override /*TraversableLike*/
  def last: A = {
    if (isEmpty) throw new NoSuchElementException
    var these = this
    var nx = these.tail
    while (!nx.isEmpty) {
      these = nx
      nx = nx.tail
    }
    these.head
  }

  override /*IterableLike*/
  def take(n: Int): Repr = {
    val b = newBuilder
    var i = 0
    var these = repr
    while (!these.isEmpty && i < n) {
      i += 1
      b += these.head
      these = these.tail
    }
    b.result()
  }

  override /*TraversableLike*/
  def drop(n: Int): Repr = {
    var these: Repr = repr
    var count = n
    while (!these.isEmpty && count > 0) {
      these = these.tail
      count -= 1
    }
    // !!! This line should actually be something like:
    //   newBuilder ++= these result
    // since we are in collection.*, not immutable.*.
    // However making that change will pessimize all the
    // immutable linear seqs (like list) which surely expect
    // drop to share.  (Or at least it would penalize List if
    // it didn't override drop.  It would be a lot better if
    // the leaf collections didn't override so many methods.)
    //
    // Upshot: MutableList is broken and passes part of the
    // original list as the result of drop.
    these
  }

  override /*IterableLike*/
  def dropRight(n: Int): Repr = {
    val b = newBuilder
    var these = this
    var lead = this drop n
    while (!lead.isEmpty) {
      b += these.head
      these = these.tail
      lead = lead.tail
    }
    b.result()
  }

  override /*IterableLike*/
  def slice(from: Int, until: Int): Repr = {
    var these: Repr = repr
    var count = from max 0
    if (until <= count)
      return newBuilder.result()

    val b = newBuilder
    var sliceElems = until - count
    while (these.nonEmpty && count > 0) {
      these = these.tail
      count -= 1
    }
    while (these.nonEmpty && sliceElems > 0) {
      sliceElems -= 1
      b += these.head
      these = these.tail
    }
    b.result()
  }

  override /*IterableLike*/
  def takeWhile(p: A => Boolean): Repr = {
    val b = newBuilder
    var these = this
    while (!these.isEmpty && p(these.head)) {
      b += these.head
      these = these.tail
    }
    b.result()
  }

  override /*TraversableLike*/
  def span(p: A => Boolean): (Repr, Repr) = {
    var these: Repr = repr
    val b = newBuilder
    while (!these.isEmpty && p(these.head)) {
      b += these.head
      these = these.tail
    }
    (b.result(), these)
  }

  override /*IterableLike*/
  def sameElements[B >: A](that: GenIterable[B]): Boolean = that match {
    case that1: LinearSeq[_] =>
      // Probably immutable, so check reference identity first (it's quick anyway)
      (this eq that1) || {
        var these = this
        var those = that1
        while (!these.isEmpty && !those.isEmpty && these.head == those.head) {
          these = these.tail
          those = those.tail
        }
        these.isEmpty && those.isEmpty
      }
    case _ =>
      super.sameElements(that)
  }

  override /*SeqLike*/
  def lengthCompare(len: Int): Int = {
    @tailrec def loop(i: Int, xs: Repr): Int = {
      if (i == len)
        if (xs.isEmpty) 0 else 1
      else if (xs.isEmpty)
        -1
      else
        loop(i + 1, xs.tail)
    }
    if (len < 0) 1
    else loop(0, this)
  }

  override /*SeqLike*/
  def isDefinedAt(x: Int): Boolean = x >= 0 && lengthCompare(x) > 0

  override /*SeqLike*/
  def segmentLength(p: A => Boolean, from: Int): Int = {
    var i = 0
    var these = this drop from
    while (!these.isEmpty && p(these.head)) {
      i += 1
      these = these.tail
    }
    i
  }

  override /*SeqLike*/
  def indexWhere(p: A => Boolean, from: Int): Int = {
    var i = math.max(from, 0)
    var these = this drop from
    while (these.nonEmpty) {
      if (p(these.head))
        return i

      i += 1
      these = these.tail
    }
    -1
  }

  override /*SeqLike*/
  def lastIndexWhere(p: A => Boolean, end: Int): Int = {
    var i = 0
    var these = this
    var last = -1
    while (!these.isEmpty && i <= end) {
      if (p(these.head)) last = i
      these = these.tail
      i += 1
    }
    last
  }

  override /*TraversableLike*/
  def tails: Iterator[Repr] = Iterator.iterate(repr)(_.tail).takeWhile(_.nonEmpty) ++ Iterator(newBuilder.result)
}

[0m2021.02.26 15:19:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:19:36 INFO  time: compiled root in 0.23s[0m
[0m2021.02.26 15:19:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:19:47 INFO  time: compiled root in 0.22s[0m
[0m2021.02.26 15:20:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:20:21 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 15:20:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:20:22 INFO  time: compiled root in 0.18s[0m
[0m2021.02.26 15:20:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:20:46 INFO  time: compiled root in 0.23s[0m
[0m2021.02.26 15:20:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:20:50 INFO  time: compiled root in 0.23s[0m
[0m2021.02.26 15:21:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:21:58 INFO  time: compiled root in 0.22s[0m
[0m2021.02.26 15:22:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:22:26 INFO  time: compiled root in 0.26s[0m
[0m2021.02.26 15:22:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:22:52 INFO  time: compiled root in 1.11s[0m
[0m2021.02.26 15:25:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:25:45 INFO  time: compiled root in 0.25s[0m
[0m2021.02.26 15:26:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:26:20 INFO  time: compiled root in 0.25s[0m
[0m2021.02.26 15:26:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:26:28 INFO  time: compiled root in 0.24s[0m
[0m2021.02.26 15:26:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:26:34 INFO  time: compiled root in 1.21s[0m
[0m2021.02.26 15:27:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:27:13 INFO  time: compiled root in 0.23s[0m
[0m2021.02.26 15:28:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:28:39 INFO  time: compiled root in 1.26s[0m
[0m2021.02.26 15:31:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:31:40 INFO  time: compiled root in 1.1s[0m
[0m2021.02.26 15:37:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:37:27 INFO  time: compiled root in 0.27s[0m
[0m2021.02.26 15:37:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:37:56 INFO  time: compiled root in 0.29s[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:21 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:38:57 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:39:14: stale bloop error: not found: value warc
      .toDF( warc: String, 
             ^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value warcTpye
    warcTpye: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value warcTargetUri
    warcTargetUri: String, 
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value warcDate
    warcDate: String, 
    ^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:5: stale bloop error: not found: value warcRecordID
    warcRecordID: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:44:5: stale bloop error: not found: value warcRefersTo
    warcRefersTo: String,
    ^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:45:5: stale bloop error: not found: value warcBlockDigest
    warcBlockDigest: String,
    ^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: not found: value warcIdentifiedContentLang
    warcIdentifiedContentLang: String,
    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:47:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:48:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:08 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:39:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:39:18 INFO  time: compiled root in 0.28s[0m
[0m2021.02.26 15:39:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:39:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:39:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:39:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:39:28 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:39:28 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:28 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:28 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:39:28 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:39:28 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:28 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:28 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:39:32 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:39:32 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:32 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:32 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:39:32 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:5: stale bloop error: not found: value contentType
    contentType: String, 
    ^^^^^^^^^^^[0m
[0m2021.02.26 15:39:32 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: not found: value contentLength
    contentLength: Int,
    ^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:32 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:41:5: stale bloop error: type mismatch;
 found   : Int
 required: String
    contentLength: Int,
    ^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.26 15:39:32 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:42:5: stale bloop error: not found: value content
    content: String)
    ^^^^^^^[0m
[0m2021.02.26 15:39:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:39:34 INFO  time: compiled root in 0.24s[0m
[0m2021.02.26 15:39:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:39:46 INFO  time: compiled root in 1.15s[0m
[0m2021.02.26 15:39:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:39:54 INFO  time: compiled root in 1.17s[0m
[0m2021.02.26 15:40:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:40:13 INFO  time: compiled root in 1.16s[0m
[0m2021.02.26 15:41:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:41:56 INFO  time: compiled root in 1.18s[0m
[0m2021.02.26 15:44:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:44:21 INFO  time: compiled root in 0.25s[0m
[0m2021.02.26 15:48:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:48:21 INFO  time: compiled root in 0.99s[0m
[0m2021.02.26 15:52:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:52:04 INFO  time: compiled root in 0.98s[0m
[0m2021.02.26 15:52:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:52:20 INFO  time: compiled root in 1.01s[0m
[0m2021.02.26 15:52:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:52:26 INFO  time: compiled root in 1s[0m
[0m2021.02.26 15:53:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:53:23 INFO  time: compiled root in 0.98s[0m
[0m2021.02.26 15:57:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:57:58 INFO  time: compiled root in 0.98s[0m
[0m2021.02.26 15:58:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 15:58:05 INFO  time: compiled root in 1.02s[0m
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in Array[<error>]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 2417, 2417, 2432)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in Array[<error>]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 2417, 2417, 2425)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in Array[<error>]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 2417, 2417, 2427)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in Array[<error>]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 2417, 2417, 2429)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in Array[com.google.flatbuffers.Struct]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 2417, 2417, 2430)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in Array[<error>]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 2417, 2417, 2431)
something's wrong: no file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala in Array[org.apache.spark.sql.types.StructField]RangePosition(file:///home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala, 2417, 2417, 2435)
[0m2021.02.26 16:00:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 16:00:05 INFO  time: compiled root in 0.21s[0m
[0m2021.02.26 16:00:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 16:00:19 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 16:04:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 16:04:39 INFO  time: compiled root in 0.23s[0m
[0m2021.02.26 16:04:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 16:04:55 INFO  time: compiled root in 0.21s[0m
[0m2021.02.26 16:05:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 16:05:01 INFO  time: compiled root in 0.24s[0m
[0m2021.02.26 16:05:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 16:05:15 INFO  time: compiled root in 1.04s[0m
[0m2021.02.26 16:05:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 16:05:19 INFO  time: compiled root in 0.96s[0m
[0m2021.02.26 16:07:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 16:07:33 INFO  time: compiled root in 1.17s[0m
[0m2021.02.26 16:10:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 16:10:51 INFO  time: compiled root in 1.03s[0m
[0m2021.02.26 16:26:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 16:26:54 INFO  time: compiled root in 1.13s[0m
Feb 26, 2021 4:27:39 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.02.26 16:27:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 16:27:40 INFO  time: compiled root in 0.24s[0m
[0m2021.02.26 16:27:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 16:27:44 INFO  time: compiled root in 0.28s[0m
[0m2021.02.26 16:28:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 16:28:06 INFO  time: compiled root in 1.11s[0m
[0m2021.02.26 16:51:35 INFO  shutting down Metals[0m
[0m2021.02.26 16:51:35 INFO  Shut down connection with build server.[0m
[0m2021.02.26 16:51:35 INFO  Shut down connection with build server.[0m
[0m2021.02.26 16:51:35 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.01 14:44:46 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.53.2.[0m
[0m2021.03.01 14:44:47 INFO  time: initialize in 0.49s[0m
[0m2021.03.01 14:44:48 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.01 14:44:47 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6690888558580290599/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.01 14:44:48 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.03.01 14:44:50 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val df = spark.read.load(
        "s3a://commoncrawl/cc-index/table/cc-main/warc/"
        )

    val commonCrawl = spark
        .read
        .textFile(
          "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
          ).flatMap(_.split("\n"))


        commonCrawl.take(100)
      //  "warc", 
      // "warcTpye", 
      // "warcTargetUri", 
      // "warcDate", 
      // "warcRecordID", 
      // "warcRefersTo", 
      // "warcBlockDigest", 
      // "warcIdentifiedContentLang",
      // "contentType", 
      // "contentLength",
      // "content
        
    // commonCrawl.as[Wet]
    
    // commonCrawl
    //   .printSchema()
      // .map(row => row.split("\n"))
      // .toDF()
      // .printSchema()
      // .take(100).foreach(println)
    
    // df.printSchema()
    // df
    // .select("url")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    //rddFromFile
    //.filter(!functions.isnull($"envelope.format"))
    //.select(rddFromFile.col("envelope.format"), rddFromFile.col("envelope.payload-metadata.actual-content-length"))
    //.show()
    
  }

  case class wet(
    warc: String, 
    warcTpye: String, 
    warcTargetUri: String, 
    warcDate: String, 
    warcRecordID: String,
    warcRefersTo: String,
    warcBlockDigest: String,
    warcIdentifiedContentLang: String,
    contentType: String, 
    contentLength: Int,
    content: String
    ) {}

}
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/delaneylekien/project3/scalas3read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6690888558580290599/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6690888558580290599/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.01 14:44:53 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val df = spark.read.load(
        "s3a://commoncrawl/cc-index/table/cc-main/warc/"
        )

    val commonCrawl = spark
        .read
        .textFile(
          "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
          ).flatMap(_.split("\n"))


        commonCrawl.take(100)
      //  "warc", 
      // "warcTpye", 
      // "warcTargetUri", 
      // "warcDate", 
      // "warcRecordID", 
      // "warcRefersTo", 
      // "warcBlockDigest", 
      // "warcIdentifiedContentLang",
      // "contentType", 
      // "contentLength",
      // "content
        
    // commonCrawl.as[Wet]
    
    // commonCrawl
    //   .printSchema()
      // .map(row => row.split("\n"))
      // .toDF()
      // .printSchema()
      // .take(100).foreach(println)
    
    // df.printSchema()
    // df
    // .select("url")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    //rddFromFile
    //.filter(!functions.isnull($"envelope.format"))
    //.select(rddFromFile.col("envelope.format"), rddFromFile.col("envelope.payload-metadata.actual-content-length"))
    //.show()
    
  }

  case class wet(
    warc: String, 
    warcTpye: String, 
    warcTargetUri: String, 
    warcDate: String, 
    warcRecordID: String,
    warcRefersTo: String,
    warcBlockDigest: String,
    warcIdentifiedContentLang: String,
    contentType: String, 
    contentLength: Int,
    content: String
    ) {}

}
[0m2021.03.01 14:44:53 INFO  time: code lens generation in 5.58s[0m
[0m2021.03.01 14:44:53 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1408900262952788394/bsp.socket'...[0m
2021.03.01 14:44:53 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7481655866034195639/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1408900262952788394/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1408900262952788394/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7481655866034195639/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7481655866034195639/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.01 14:44:54 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:44:54 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:44:54 INFO  time: Connected to build server in 6.2s[0m
[0m2021.03.01 14:44:54 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.01 14:44:54 INFO  time: Imported build in 0.39s[0m
[0m2021.03.01 14:44:57 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.01 14:44:57 INFO  time: indexed workspace in 3.57s[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._, _root_.sbtassembly.AssemblyPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin, _root_.sbtassembly.AssemblyPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.11.12"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7"
  )

  assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.03.01 14:46:23 INFO  time: code lens generation in 3.54s[0m
[0m2021.03.01 14:46:23 INFO  time: code lens generation in 2.84s[0m
[0m2021.03.01 14:46:23 INFO  time: code lens generation in 1.54s[0m
[0m2021.03.01 14:46:23 INFO  time: code lens generation in 4.45s[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._, _root_.sbtassembly.AssemblyPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin, _root_.sbtassembly.AssemblyPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.11.12"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7",
    // https://mvnrepository.com/artifact/uk.bl.wa.discovery/warc-hadoop-recordreaders
    libraryDependencies += "uk.bl.wa.discovery" % "warc-hadoop-recordreaders" % "3.1.0",
    // https://mvnrepository.com/artifact/uk.bl.wa.discovery/warc-hadoop-indexer
    libraryDependencies += "uk.bl.wa.discovery" % "warc-hadoop-indexer" % "3.1.0"



  )

  assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.03.01 14:48:55 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals8416366229814355041/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.03.01 14:48:57 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_265)[0m
[0m2021.03.01 14:48:58 INFO  [info] loading settings for project scalas3read-build-build-build from metals.sbt ...[0m
[0m2021.03.01 14:48:58 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project/project[0m
[0m2021.03.01 14:49:00 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.03.01 14:49:00 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.03.01 14:49:03 INFO  [success] Generated .bloop/scalas3read-build-build.json[0m
[0m2021.03.01 14:49:03 INFO  [success] Total time: 2 s, completed Mar 1, 2021 2:49:03 PM[0m
[0m2021.03.01 14:49:03 INFO  [info] loading settings for project scalas3read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.03.01 14:49:03 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.03.01 14:49:04 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.03.01 14:49:04 INFO  [success] Total time: 1 s, completed Mar 1, 2021 2:49:05 PM[0m
[0m2021.03.01 14:49:09 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.03.01 14:49:09 INFO  [info] set current project to scalas3read (in build file:/home/delaneylekien/project3/scalas3read/)[0m
[0m2021.03.01 14:49:15 INFO  [warn] [0m
[0m2021.03.01 14:49:15 INFO  [warn] 	Note: Unresolved dependencies path:[0m
[0m2021.03.01 14:49:15 INFO  [error] sbt.librarymanagement.ResolveException: Error downloading org.restlet.jee:org.restlet.ext.servlet:2.3.0[0m
[0m2021.03.01 14:49:15 INFO  [error]   Not found[0m
[0m2021.03.01 14:49:15 INFO  [error]   Not found[0m
[0m2021.03.01 14:49:15 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.restlet.jee/org.restlet.ext.servlet/2.3.0/ivys/ivy.xml[0m
[0m2021.03.01 14:49:15 INFO  [error]   not found: https://repo1.maven.org/maven2/org/restlet/jee/org.restlet.ext.servlet/2.3.0/org.restlet.ext.servlet-2.3.0.pom[0m
[0m2021.03.01 14:49:15 INFO  [error] Error downloading org.restlet.jee:org.restlet:2.3.0[0m
[0m2021.03.01 14:49:15 INFO  [error]   Not found[0m
[0m2021.03.01 14:49:15 INFO  [error]   Not found[0m
[0m2021.03.01 14:49:15 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.restlet.jee/org.restlet/2.3.0/ivys/ivy.xml[0m
[0m2021.03.01 14:49:15 INFO  [error]   not found: https://repo1.maven.org/maven2/org/restlet/jee/org.restlet/2.3.0/org.restlet-2.3.0.pom[0m
[0m2021.03.01 14:49:15 INFO  [error] Error downloading org.apache.hadoop:hadoop-core:0.20.2-cdh3u4[0m
[0m2021.03.01 14:49:15 INFO  [error]   Not found[0m
[0m2021.03.01 14:49:15 INFO  [error]   Not found[0m
[0m2021.03.01 14:49:15 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.apache.hadoop/hadoop-core/0.20.2-cdh3u4/ivys/ivy.xml[0m
[0m2021.03.01 14:49:15 INFO  [error]   not found: https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/0.20.2-cdh3u4/hadoop-core-0.20.2-cdh3u4.pom[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at lmcoursier.CoursierDependencyResolution.unresolvedWarningOrThrow(CoursierDependencyResolution.scala:258)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at lmcoursier.CoursierDependencyResolution.$anonfun$update$38(CoursierDependencyResolution.scala:227)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at scala.util.Either$LeftProjection.map(Either.scala:573)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at lmcoursier.CoursierDependencyResolution.update(CoursierDependencyResolution.scala:227)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.librarymanagement.DependencyResolution.update(DependencyResolution.scala:60)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.internal.LibraryManagement$.resolve$1(LibraryManagement.scala:53)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$12(LibraryManagement.scala:103)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.util.Tracked$.$anonfun$lastOutput$1(Tracked.scala:73)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$20(LibraryManagement.scala:116)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at scala.util.control.Exception$Catch.apply(Exception.scala:228)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11(LibraryManagement.scala:116)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11$adapted(LibraryManagement.scala:97)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.util.Tracked$.$anonfun$inputChangedW$1(Tracked.scala:219)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.internal.LibraryManagement$.cachedUpdate(LibraryManagement.scala:130)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.Classpaths$.$anonfun$updateTask0$5(Defaults.scala:3486)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:68)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:282)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:23)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.Execute.work(Execute.scala:291)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:282)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:64)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m2021.03.01 14:49:15 INFO  [error] 	at java.lang.Thread.run(Thread.java:748)[0m
[0m2021.03.01 14:49:15 INFO  [error] (update) sbt.librarymanagement.ResolveException: Error downloading org.restlet.jee:org.restlet.ext.servlet:2.3.0[0m
[0m2021.03.01 14:49:15 INFO  [error]   Not found[0m
[0m2021.03.01 14:49:15 INFO  [error]   Not found[0m
[0m2021.03.01 14:49:15 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.restlet.jee/org.restlet.ext.servlet/2.3.0/ivys/ivy.xml[0m
[0m2021.03.01 14:49:15 INFO  [error]   not found: https://repo1.maven.org/maven2/org/restlet/jee/org.restlet.ext.servlet/2.3.0/org.restlet.ext.servlet-2.3.0.pom[0m
[0m2021.03.01 14:49:15 INFO  [error] Error downloading org.restlet.jee:org.restlet:2.3.0[0m
[0m2021.03.01 14:49:15 INFO  [error]   Not found[0m
[0m2021.03.01 14:49:15 INFO  [error]   Not found[0m
[0m2021.03.01 14:49:15 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.restlet.jee/org.restlet/2.3.0/ivys/ivy.xml[0m
[0m2021.03.01 14:49:15 INFO  [error]   not found: https://repo1.maven.org/maven2/org/restlet/jee/org.restlet/2.3.0/org.restlet-2.3.0.pom[0m
[0m2021.03.01 14:49:15 INFO  [error] Error downloading org.apache.hadoop:hadoop-core:0.20.2-cdh3u4[0m
[0m2021.03.01 14:49:15 INFO  [error]   Not found[0m
[0m2021.03.01 14:49:15 INFO  [error]   Not found[0m
[0m2021.03.01 14:49:15 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.apache.hadoop/hadoop-core/0.20.2-cdh3u4/ivys/ivy.xml[0m
[0m2021.03.01 14:49:15 INFO  [error]   not found: https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/0.20.2-cdh3u4/hadoop-core-0.20.2-cdh3u4.pom[0m
[0m2021.03.01 14:49:15 INFO  [error] Total time: 5 s, completed Mar 1, 2021 2:49:15 PM[0m
[0m2021.03.01 14:49:15 INFO  time: ran 'sbt bloopInstall' in 20s[0m
[0m2021.03.01 14:49:15 INFO  sbt bloopInstall exit: 1[0m
[0m2021.03.01 14:49:15 ERROR sbt command failed: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals8416366229814355041/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall[0m
[0m2021.03.01 14:49:15 INFO  Disconnecting from Bloop session...[0m
[0m2021.03.01 14:49:15 INFO  Shut down connection with build server.[0m
[0m2021.03.01 14:49:15 INFO  Shut down connection with build server.[0m
[0m2021.03.01 14:49:15 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.03.01 14:49:15 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2841949114228156847/bsp.socket'...
Waiting for the bsp connection to come up...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.01 14:49:15 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:49:15 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 14:49:15 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 14:49:15 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:49:15 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:49:15 INFO  time: Connected to build server in 0.23s[0m
[0m2021.03.01 14:49:15 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.01 14:49:17 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.01 14:49:17 INFO  time: indexed workspace in 1.92s[0m
[0m2021.03.01 14:52:42 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals2183940538935678657/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.03.01 14:52:44 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_265)[0m
[0m2021.03.01 14:52:45 INFO  [info] loading settings for project scalas3read-build-build-build from metals.sbt ...[0m
[0m2021.03.01 14:52:45 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project/project[0m
[0m2021.03.01 14:52:47 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.03.01 14:52:47 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.03.01 14:52:50 INFO  [success] Generated .bloop/scalas3read-build-build.json[0m
[0m2021.03.01 14:52:50 INFO  [success] Total time: 3 s, completed Mar 1, 2021 2:52:50 PM[0m
[0m2021.03.01 14:52:50 INFO  [info] loading settings for project scalas3read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.03.01 14:52:50 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.03.01 14:52:51 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.03.01 14:52:51 INFO  [success] Total time: 1 s, completed Mar 1, 2021 2:52:52 PM[0m
[0m2021.03.01 14:52:56 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.03.01 14:52:56 INFO  [info] set current project to scalas3read (in build file:/home/delaneylekien/project3/scalas3read/)[0m
[0m2021.03.01 14:52:59 INFO  [warn] [0m
[0m2021.03.01 14:52:59 INFO  [warn] 	Note: Unresolved dependencies path:[0m
[0m2021.03.01 14:52:59 INFO  [error] sbt.librarymanagement.ResolveException: Error downloading org.restlet.jee:org.restlet.ext.servlet:2.3.0[0m
[0m2021.03.01 14:52:59 INFO  [error]   Not found[0m
[0m2021.03.01 14:52:59 INFO  [error]   Not found[0m
[0m2021.03.01 14:52:59 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.restlet.jee/org.restlet.ext.servlet/2.3.0/ivys/ivy.xml[0m
[0m2021.03.01 14:52:59 INFO  [error]   not found: https://repo1.maven.org/maven2/org/restlet/jee/org.restlet.ext.servlet/2.3.0/org.restlet.ext.servlet-2.3.0.pom[0m
[0m2021.03.01 14:52:59 INFO  [error] Error downloading org.restlet.jee:org.restlet:2.3.0[0m
[0m2021.03.01 14:52:59 INFO  [error]   Not found[0m
[0m2021.03.01 14:52:59 INFO  [error]   Not found[0m
[0m2021.03.01 14:52:59 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.restlet.jee/org.restlet/2.3.0/ivys/ivy.xml[0m
[0m2021.03.01 14:52:59 INFO  [error]   not found: https://repo1.maven.org/maven2/org/restlet/jee/org.restlet/2.3.0/org.restlet-2.3.0.pom[0m
[0m2021.03.01 14:52:59 INFO  [error] Error downloading org.apache.hadoop:hadoop-core:0.20.2-cdh3u4[0m
[0m2021.03.01 14:52:59 INFO  [error]   Not found[0m
[0m2021.03.01 14:52:59 INFO  [error]   Not found[0m
[0m2021.03.01 14:52:59 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.apache.hadoop/hadoop-core/0.20.2-cdh3u4/ivys/ivy.xml[0m
[0m2021.03.01 14:52:59 INFO  [error]   not found: https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/0.20.2-cdh3u4/hadoop-core-0.20.2-cdh3u4.pom[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at lmcoursier.CoursierDependencyResolution.unresolvedWarningOrThrow(CoursierDependencyResolution.scala:258)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at lmcoursier.CoursierDependencyResolution.$anonfun$update$38(CoursierDependencyResolution.scala:227)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at scala.util.Either$LeftProjection.map(Either.scala:573)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at lmcoursier.CoursierDependencyResolution.update(CoursierDependencyResolution.scala:227)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.librarymanagement.DependencyResolution.update(DependencyResolution.scala:60)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.internal.LibraryManagement$.resolve$1(LibraryManagement.scala:53)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$12(LibraryManagement.scala:103)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.util.Tracked$.$anonfun$lastOutput$1(Tracked.scala:73)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$20(LibraryManagement.scala:116)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at scala.util.control.Exception$Catch.apply(Exception.scala:228)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11(LibraryManagement.scala:116)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.internal.LibraryManagement$.$anonfun$cachedUpdate$11$adapted(LibraryManagement.scala:97)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.util.Tracked$.$anonfun$inputChangedW$1(Tracked.scala:219)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.internal.LibraryManagement$.cachedUpdate(LibraryManagement.scala:130)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.Classpaths$.$anonfun$updateTask0$5(Defaults.scala:3486)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at scala.Function1.$anonfun$compose$1(Function1.scala:49)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.internal.util.$tilde$greater.$anonfun$$u2219$1(TypeFunctions.scala:62)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.std.Transform$$anon$4.work(Transform.scala:68)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.Execute.$anonfun$submit$2(Execute.scala:282)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:23)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.Execute.work(Execute.scala:291)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.Execute.$anonfun$submit$1(Execute.scala:282)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:265)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at sbt.CompletionService$$anon$2.call(CompletionService.scala:64)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)[0m
[0m2021.03.01 14:52:59 INFO  [error] 	at java.lang.Thread.run(Thread.java:748)[0m
[0m2021.03.01 14:52:59 INFO  [error] (update) sbt.librarymanagement.ResolveException: Error downloading org.restlet.jee:org.restlet.ext.servlet:2.3.0[0m
[0m2021.03.01 14:52:59 INFO  [error]   Not found[0m
[0m2021.03.01 14:52:59 INFO  [error]   Not found[0m
[0m2021.03.01 14:52:59 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.restlet.jee/org.restlet.ext.servlet/2.3.0/ivys/ivy.xml[0m
[0m2021.03.01 14:52:59 INFO  [error]   not found: https://repo1.maven.org/maven2/org/restlet/jee/org.restlet.ext.servlet/2.3.0/org.restlet.ext.servlet-2.3.0.pom[0m
[0m2021.03.01 14:52:59 INFO  [error] Error downloading org.restlet.jee:org.restlet:2.3.0[0m
[0m2021.03.01 14:52:59 INFO  [error]   Not found[0m
[0m2021.03.01 14:52:59 INFO  [error]   Not found[0m
[0m2021.03.01 14:52:59 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.restlet.jee/org.restlet/2.3.0/ivys/ivy.xml[0m
[0m2021.03.01 14:52:59 INFO  [error]   not found: https://repo1.maven.org/maven2/org/restlet/jee/org.restlet/2.3.0/org.restlet-2.3.0.pom[0m
[0m2021.03.01 14:52:59 INFO  [error] Error downloading org.apache.hadoop:hadoop-core:0.20.2-cdh3u4[0m
[0m2021.03.01 14:52:59 INFO  [error]   Not found[0m
[0m2021.03.01 14:52:59 INFO  [error]   Not found[0m
[0m2021.03.01 14:52:59 INFO  [error]   not found: /home/delaneylekien/.ivy2/localorg.apache.hadoop/hadoop-core/0.20.2-cdh3u4/ivys/ivy.xml[0m
[0m2021.03.01 14:52:59 INFO  [error]   not found: https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/0.20.2-cdh3u4/hadoop-core-0.20.2-cdh3u4.pom[0m
[0m2021.03.01 14:52:59 INFO  [error] Total time: 2 s, completed Mar 1, 2021 2:52:59 PM[0m
[0m2021.03.01 14:52:59 INFO  sbt bloopInstall exit: 1[0m
[0m2021.03.01 14:52:59 INFO  time: ran 'sbt bloopInstall' in 17s[0m
[0m2021.03.01 14:52:59 ERROR sbt command failed: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals2183940538935678657/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall[0m
[0m2021.03.01 14:52:59 INFO  Disconnecting from Bloop session...[0m
[0m2021.03.01 14:52:59 INFO  Shut down connection with build server.[0m
[0m2021.03.01 14:52:59 INFO  Shut down connection with build server.[0m
[0m2021.03.01 14:52:59 INFO  Shut down connection with build server.[0m
[0m2021.03.01 14:52:59 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 14:52:59 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:52:59 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 14:52:59 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 14:52:59 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:52:59 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:52:59 INFO  time: Connected to build server in 0.19s[0m
[0m2021.03.01 14:52:59 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.01 14:53:00 INFO  time: Imported build in 0.17s[0m
[0m2021.03.01 14:53:02 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.01 14:53:02 INFO  time: indexed workspace in 2.22s[0m
[0m2021.03.01 14:54:15 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals8172200786648836992/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.03.01 14:54:17 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_265)[0m
[0m2021.03.01 14:54:18 INFO  [info] loading settings for project scalas3read-build-build-build from metals.sbt ...[0m
[0m2021.03.01 14:54:18 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project/project[0m
[0m2021.03.01 14:54:20 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.03.01 14:54:20 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.03.01 14:54:23 INFO  [success] Generated .bloop/scalas3read-build-build.json[0m
[0m2021.03.01 14:54:23 INFO  [success] Total time: 3 s, completed Mar 1, 2021 2:54:23 PM[0m
[0m2021.03.01 14:54:23 INFO  [info] loading settings for project scalas3read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.03.01 14:54:23 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.03.01 14:54:24 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.03.01 14:54:24 INFO  [success] Total time: 1 s, completed Mar 1, 2021 2:54:25 PM[0m
[0m2021.03.01 14:54:29 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.03.01 14:54:29 INFO  [info] set current project to scalas3read (in build file:/home/delaneylekien/project3/scalas3read/)[0m
[0m2021.03.01 14:54:34 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.03.01 14:54:35 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.03.01 14:54:35 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.03.01 14:54:35 INFO  [success] Generated .bloop/root.json[0m
[0m2021.03.01 14:54:35 INFO  [success] Total time: 6 s, completed Mar 1, 2021 2:54:35 PM[0m
[0m2021.03.01 14:54:35 INFO  sbt bloopInstall exit: 0[0m
[0m2021.03.01 14:54:36 INFO  time: ran 'sbt bloopInstall' in 20s[0m
[0m2021.03.01 14:54:36 INFO  Disconnecting from Bloop session...[0m
[0m2021.03.01 14:54:36 INFO  Shut down connection with build server.[0m
[0m2021.03.01 14:54:36 INFO  Shut down connection with build server.[0m
[0m2021.03.01 14:54:36 INFO  Shut down connection with build server.[0m
[0m2021.03.01 14:54:36 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 14:54:36 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:54:36 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 14:54:36 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 14:54:36 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:54:36 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:54:36 INFO  time: Connected to build server in 0.2s[0m
[0m2021.03.01 14:54:36 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.01 14:54:36 INFO  time: Imported build in 0.18s[0m
[0m2021.03.01 14:54:38 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.01 14:54:38 INFO  time: indexed workspace in 2.12s[0m
[0m2021.03.01 14:54:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:54:47 INFO  time: compiled root in 7.49s[0m
[0m2021.03.01 14:55:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:55:09 INFO  time: compiled root in 2.73s[0m
[0m2021.03.01 14:55:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:55:12 INFO  time: compiled root in 1.61s[0m
[0m2021.03.01 14:57:17 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals4411409606790201486/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.03.01 14:57:19 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_265)[0m
[0m2021.03.01 14:57:20 INFO  [info] loading settings for project scalas3read-build-build-build from metals.sbt ...[0m
[0m2021.03.01 14:57:20 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project/project[0m
[0m2021.03.01 14:57:22 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.03.01 14:57:22 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.03.01 14:57:25 INFO  [success] Generated .bloop/scalas3read-build-build.json[0m
[0m2021.03.01 14:57:25 INFO  [success] Total time: 2 s, completed Mar 1, 2021 2:57:25 PM[0m
[0m2021.03.01 14:57:25 INFO  [info] loading settings for project scalas3read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.03.01 14:57:25 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.03.01 14:57:26 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.03.01 14:57:26 INFO  [success] Total time: 1 s, completed Mar 1, 2021 2:57:27 PM[0m
[0m2021.03.01 14:57:31 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.03.01 14:57:31 INFO  [info] set current project to scalas3read (in build file:/home/delaneylekien/project3/scalas3read/)[0m
[0m2021.03.01 14:57:35 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.03.01 14:57:35 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.03.01 14:57:37 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.03.01 14:57:37 INFO  [success] Generated .bloop/root.json[0m
[0m2021.03.01 14:57:37 INFO  [success] Total time: 5 s, completed Mar 1, 2021 2:57:37 PM[0m
[0m2021.03.01 14:57:37 INFO  sbt bloopInstall exit: 0[0m
[0m2021.03.01 14:57:37 INFO  time: ran 'sbt bloopInstall' in 19s[0m
[0m2021.03.01 14:57:37 INFO  Disconnecting from Bloop session...[0m
[0m2021.03.01 14:57:37 INFO  Shut down connection with build server.[0m
[0m2021.03.01 14:57:37 INFO  Shut down connection with build server.[0m
[0m2021.03.01 14:57:37 INFO  Shut down connection with build server.[0m
[0m2021.03.01 14:57:37 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 14:57:37 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:57:37 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 14:57:37 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 14:57:37 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:57:37 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:57:37 INFO  time: Connected to build server in 0.38s[0m
[0m2021.03.01 14:57:37 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.01 14:57:40 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.01 14:57:40 INFO  time: indexed workspace in 2.25s[0m
[0m2021.03.01 14:57:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:57:42 INFO  time: compiled root in 1.87s[0m
[0m2021.03.01 15:09:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:09:19 INFO  time: compiled root in 1.33s[0m
[0m2021.03.01 15:13:11 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals7844381224462859974/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.03.01 15:13:13 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_265)[0m
[0m2021.03.01 15:13:14 INFO  [info] loading settings for project scalas3read-build-build-build from metals.sbt ...[0m
[0m2021.03.01 15:13:14 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project/project[0m
[0m2021.03.01 15:13:16 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.03.01 15:13:16 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.03.01 15:13:19 INFO  [success] Generated .bloop/scalas3read-build-build.json[0m
[0m2021.03.01 15:13:19 INFO  [success] Total time: 2 s, completed Mar 1, 2021 3:13:19 PM[0m
[0m2021.03.01 15:13:19 INFO  [info] loading settings for project scalas3read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.03.01 15:13:19 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.03.01 15:13:20 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.03.01 15:13:20 INFO  [success] Total time: 1 s, completed Mar 1, 2021 3:13:20 PM[0m
[0m2021.03.01 15:13:25 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.03.01 15:13:25 INFO  [info] set current project to scalas3read (in build file:/home/delaneylekien/project3/scalas3read/)[0m
[0m2021.03.01 15:13:29 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.03.01 15:13:29 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.03.01 15:13:30 INFO  [success] Generated .bloop/root.json[0m
[0m2021.03.01 15:13:30 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.03.01 15:13:30 INFO  [success] Total time: 5 s, completed Mar 1, 2021 3:13:30 PM[0m
[0m2021.03.01 15:13:30 INFO  sbt bloopInstall exit: 0[0m
[0m2021.03.01 15:13:30 INFO  time: ran 'sbt bloopInstall' in 19s[0m
[0m2021.03.01 15:13:30 INFO  Disconnecting from Bloop session...[0m
[0m2021.03.01 15:13:30 INFO  Shut down connection with build server.[0m
[0m2021.03.01 15:13:30 INFO  Shut down connection with build server.[0m
[0m2021.03.01 15:13:30 INFO  Shut down connection with build server.[0m
[0m2021.03.01 15:13:30 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 15:13:30 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 15:13:30 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 15:13:30 INFO  Attempting to connect to the build server...[0m
[0m2021.03.01 15:13:30 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 15:13:31 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 15:13:31 INFO  time: Connected to build server in 0.32s[0m
[0m2021.03.01 15:13:31 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.01 15:13:33 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.01 15:13:33 INFO  time: indexed workspace in 2.39s[0m
[0m2021.03.01 15:14:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:14:16 INFO  time: compiled root in 1.88s[0m
[0m2021.03.01 15:24:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:24:02 INFO  time: compiled root in 1.36s[0m
[0m2021.03.01 15:25:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:25:58 INFO  time: compiled root in 0.66s[0m
[0m2021.03.01 15:26:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:26:00 INFO  time: compiled root in 0.25s[0m
[0m2021.03.01 15:27:34 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:38:36: stale bloop error: value split is not a member of Char
          ).toString().map(line => line.split("/n"))
                                   ^^^^^^^^^^[0m
[0m2021.03.01 15:27:34 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:38:36: stale bloop error: value split is not a member of Char
          ).toString().map(line => line.split("/n"))
                                   ^^^^^^^^^^[0m
[0m2021.03.01 15:27:34 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:38:36: stale bloop error: value split is not a member of Char
          ).toString().map(line => line.split("/n"))
                                   ^^^^^^^^^^[0m
[0m2021.03.01 15:27:35 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:38:36: stale bloop error: value split is not a member of Char
          ).toString().map(line => line.split("/n"))
                                   ^^^^^^^^^^[0m
[0m2021.03.01 15:27:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:27:38 INFO  time: compiled root in 1.19s[0m
[0m2021.03.01 15:31:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:31:26 INFO  time: compiled root in 1.92s[0m
[0m2021.03.01 15:34:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:34:30 INFO  time: compiled root in 1.02s[0m
[0m2021.03.01 15:35:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:35:42 INFO  time: compiled root in 1.03s[0m
[0m2021.03.01 15:36:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:36:32 INFO  time: compiled root in 1.02s[0m
[0m2021.03.01 15:38:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:38:11 INFO  time: compiled root in 1.25s[0m
[0m2021.03.01 15:44:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:44:19 INFO  time: compiled root in 1.22s[0m
[0m2021.03.01 15:44:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:44:48 INFO  time: compiled root in 1.02s[0m
[0m2021.03.01 15:44:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:44:50 INFO  time: compiled root in 1.23s[0m
[0m2021.03.01 15:46:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:46:28 INFO  time: compiled root in 0.24s[0m
[0m2021.03.01 15:46:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:46:34 INFO  time: compiled root in 1.06s[0m
[0m2021.03.01 15:48:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:48:16 INFO  time: compiled root in 1.01s[0m
[0m2021.03.01 15:54:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:54:14 INFO  time: compiled root in 0.31s[0m
[0m2021.03.01 15:55:45 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:44: stale bloop error: value spilt is not a member of String
    val cutCrawl = commonCrawl.map(line => line.spilt('\n'))
                                           ^^^^^^^^^^[0m
[0m2021.03.01 15:55:45 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:44: stale bloop error: value spilt is not a member of String
    val cutCrawl = commonCrawl.map(line => line.spilt('\n'))
                                           ^^^^^^^^^^[0m
[0m2021.03.01 15:55:45 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:44: stale bloop error: value spilt is not a member of String
    val cutCrawl = commonCrawl.map(line => line.spilt('\n'))
                                           ^^^^^^^^^^[0m
[0m2021.03.01 15:55:45 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:40:44: stale bloop error: value spilt is not a member of String
    val cutCrawl = commonCrawl.map(line => line.spilt('\n'))
                                           ^^^^^^^^^^[0m
[0m2021.03.01 15:55:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:55:47 INFO  time: compiled root in 0.15s[0m
[0m2021.03.01 15:55:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:55:54 INFO  time: compiled root in 1.17s[0m
[0m2021.03.01 16:05:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:05:04 INFO  time: compiled root in 1.56s[0m
[0m2021.03.01 16:07:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:07:42 INFO  time: compiled root in 1.96s[0m
[0m2021.03.01 16:12:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:12:31 INFO  time: compiled root in 1.45s[0m
[0m2021.03.01 16:12:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:12:35 INFO  time: compiled root in 1.44s[0m
[0m2021.03.01 16:12:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:12:43 INFO  time: compiled root in 1.23s[0m
[0m2021.03.01 16:13:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:13:32 INFO  time: compiled root in 1.22s[0m
[0m2021.03.01 16:14:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:14:58 INFO  time: compiled root in 1.26s[0m
[0m2021.03.01 16:24:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:24:59 INFO  time: compiled root in 1.2s[0m
[0m2021.03.01 16:25:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:25:39 INFO  time: compiled root in 0.34s[0m
[0m2021.03.01 16:26:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:26:17 INFO  time: compiled root in 1.24s[0m
[0m2021.03.01 16:30:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:30:58 INFO  time: compiled root in 1.24s[0m
[0m2021.03.01 16:31:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:31:19 INFO  time: compiled root in 1.38s[0m
[0m2021.03.01 16:32:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:32:38 INFO  time: compiled root in 1.29s[0m
[0m2021.03.01 16:37:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:37:50 INFO  time: compiled root in 0.3s[0m
[0m2021.03.01 16:38:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:38:31 INFO  time: compiled root in 1.22s[0m
[0m2021.03.01 16:40:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:40:30 INFO  time: compiled root in 1.29s[0m
[0m2021.03.01 16:40:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:40:37 INFO  time: compiled root in 1.27s[0m
[0m2021.03.01 16:41:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:41:57 INFO  time: compiled root in 1.24s[0m
[0m2021.03.01 17:23:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:23:06 INFO  time: compiled root in 3.49s[0m
[0m2021.03.01 17:24:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:24:30 INFO  time: compiled root in 2.55s[0m
[0m2021.03.01 17:26:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:12 INFO  time: compiled root in 1.12s[0m
[0m2021.03.01 17:26:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:22 INFO  time: compiled root in 2.08s[0m
[0m2021.03.01 17:26:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:30 INFO  time: compiled root in 1.51s[0m
[0m2021.03.01 17:28:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:28:08 INFO  time: compiled root in 1.31s[0m
[0m2021.03.01 17:30:10 INFO  shutting down Metals[0m
[0m2021.03.01 17:30:10 INFO  Shut down connection with build server.[0m
[0m2021.03.01 17:30:10 INFO  Shut down connection with build server.[0m
[0m2021.03.01 17:30:11 INFO  Shut down connection with build server.[0m
[0m2021.03.01 17:30:40 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.53.2.[0m
[0m2021.03.01 17:30:41 INFO  time: initialize in 0.5s[0m
[0m2021.03.01 17:30:41 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4879417258813823153/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.01 17:30:41 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
[0m2021.03.01 17:30:42 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.03.01 17:30:44 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val df = spark.read.load(
        "s3a://commoncrawl/cc-index/table/cc-main/warc/"
        )
    
    val commonCrawl = spark.sparkContext.textFile(
          "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    )

    // .take(100).foreach(println)

    // val cutCrawl = commonCrawl.map(_.split("WARC/1.0")).show()
    // val cutCrawl = commonCrawl.flatMap(line => line.split('\n')).toDF().show()

    
      //  "warc", 
      // "warcTpye", 
      // "warcTargetUri", 
      // "warcDate", 
      // "warcRecordID", 
      // "warcRefersTo", 
      // "warcBlockDigest", 
      // "warcIdentifiedContentLang",
      // "contentType", 
      // "contentLength",
      // "content
        
    // commonCrawl.as[Wet]
    
    // commonCrawl
    //   .printSchema()
      // .map(row => row.split("\n"))
      // .toDF()
      // .printSchema()
      // .take(100).foreach(println)
    
    // df.printSchema()
    df
    .select("*")
    .filter($"crawl" === "CC-MAIN-2021-04")
    .filter($"subset" === "warc")
    .filter($"url_path".contains("job"))
    .show(100, false)

    //rddFromFile
    //.filter(!functions.isnull($"envelope.format"))
    //.select(rddFromFile.col("envelope.format"), rddFromFile.col("envelope.payload-metadata.actual-content-length"))
    //.show()
    
  }

  case class wet(
    warc: String, 
    warcTpye: String, 
    warcTargetUri: String, 
    warcDate: String, 
    warcRecordID: String,
    warcRefersTo: String,
    warcBlockDigest: String,
    warcIdentifiedContentLang: String,
    contentType: String, 
    contentLength: Int,
    content: String
    ) {}

}
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/delaneylekien/project3/scalas3read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4879417258813823153/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4879417258813823153/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.01 17:30:47 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val df = spark.read.load(
        "s3a://commoncrawl/cc-index/table/cc-main/warc/"
        )
    
    val commonCrawl = spark.sparkContext.textFile(
          "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    )

    // .take(100).foreach(println)

    // val cutCrawl = commonCrawl.map(_.split("WARC/1.0")).show()
    // val cutCrawl = commonCrawl.flatMap(line => line.split('\n')).toDF().show()

    
      //  "warc", 
      // "warcTpye", 
      // "warcTargetUri", 
      // "warcDate", 
      // "warcRecordID", 
      // "warcRefersTo", 
      // "warcBlockDigest", 
      // "warcIdentifiedContentLang",
      // "contentType", 
      // "contentLength",
      // "content
        
    // commonCrawl.as[Wet]
    
    // commonCrawl
    //   .printSchema()
      // .map(row => row.split("\n"))
      // .toDF()
      // .printSchema()
      // .take(100).foreach(println)
    
    // df.printSchema()
    df
    .select("*")
    .filter($"crawl" === "CC-MAIN-2021-04")
    .filter($"subset" === "warc")
    .filter($"url_path".contains("job"))
    .show(100, false)

    //rddFromFile
    //.filter(!functions.isnull($"envelope.format"))
    //.select(rddFromFile.col("envelope.format"), rddFromFile.col("envelope.payload-metadata.actual-content-length"))
    //.show()
    
  }

  case class wet(
    warc: String, 
    warcTpye: String, 
    warcTargetUri: String, 
    warcDate: String, 
    warcRecordID: String,
    warcRefersTo: String,
    warcBlockDigest: String,
    warcIdentifiedContentLang: String,
    contentType: String, 
    contentLength: Int,
    content: String
    ) {}

}
[0m2021.03.01 17:30:47 INFO  time: code lens generation in 5.06s[0m
[0m2021.03.01 17:30:47 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4755857946139208912/bsp.socket'...
[0m2021.03.01 17:30:47 Waiting for the bsp connection to come up...INFO 
 Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4885491063126270229/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4755857946139208912/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4755857946139208912/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4885491063126270229/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4885491063126270229/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.01 17:30:47 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 17:30:47 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 17:30:47 INFO  time: Connected to build server in 5.89s[0m
[0m2021.03.01 17:30:47 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.01 17:30:48 INFO  time: Imported build in 0.33s[0m
[0m2021.03.01 17:30:51 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.01 17:30:51 INFO  time: indexed workspace in 4.06s[0m
[0m2021.03.01 17:31:42 INFO  shutting down Metals[0m
[0m2021.03.01 17:31:42 INFO  Shut down connection with build server.[0m
[0m2021.03.01 17:31:42 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...[0m
2021.03.01 17:31:42 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.02 10:41:56 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.53.2.[0m
[0m2021.03.02 10:41:56 INFO  time: initialize in 0.49s[0m
[0m2021.03.02 10:41:57 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.02 10:41:56 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5456028253551959795/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.02 10:41:56 WARN  no build target for: /home/delaneylekien/project3/scalas3read/build.sbt[0m
[0m2021.03.02 10:41:56 INFO  skipping build import with status 'Installed'[0m
Mar 02, 2021 10:41:58 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 11
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.03.02 10:42:00 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val df = spark.read.load(
        "s3a://commoncrawl/cc-index/table/cc-main/warc/"
        )
    
    val commonCrawl = spark.sparkContext.textFile(
          "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    )

    // .take(100).foreach(println)

    // val cutCrawl = commonCrawl.map(_.split("WARC/1.0")).show()
    // val cutCrawl = commonCrawl.flatMap(line => line.split('\n')).toDF().show()

    
      //  "warc", 
      // "warcTpye", 
      // "warcTargetUri", 
      // "warcDate", 
      // "warcRecordID", 
      // "warcRefersTo", 
      // "warcBlockDigest", 
      // "warcIdentifiedContentLang",
      // "contentType", 
      // "contentLength",
      // "content
        
    // commonCrawl.as[Wet]
    
    // commonCrawl
    //   .printSchema()
      // .map(row => row.split("\n"))
      // .toDF()
      // .printSchema()
      // .take(100).foreach(println)
    
    // df.printSchema()
    df
    .select("*")
    .filter($"crawl" === "CC-MAIN-2021-04")
    .filter($"subset" === "warc")
    .filter($"url_path".contains("job"))
    .show(100, false)

    //rddFromFile
    //.filter(!functions.isnull($"envelope.format"))
    //.select(rddFromFile.col("envelope.format"), rddFromFile.col("envelope.payload-metadata.actual-content-length"))
    //.show()
    
  }

  case class wet(
    warc: String, 
    warcTpye: String, 
    warcTargetUri: String, 
    warcDate: String, 
    warcRecordID: String,
    warcRefersTo: String,
    warcBlockDigest: String,
    warcIdentifiedContentLang: String,
    contentType: String, 
    contentLength: Int,
    content: String
    ) {}

}
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/delaneylekien/project3/scalas3read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5456028253551959795/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5456028253551959795/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.02 10:42:02 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
import Dependencies._

ThisBuild / scalaVersion     := "2.11.12"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7"




  )

  assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.03.02 10:42:02 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8227010402121118320/bsp.socket'...
[0m2021.03.02 10:42:02 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3195159714118858439/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3195159714118858439/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3195159714118858439/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.02 10:42:03 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8227010402121118320/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8227010402121118320/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.02 10:42:03 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 10:42:03 INFO  time: Connected to build server in 5.97s[0m
[0m2021.03.02 10:42:03 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.02 10:42:03 INFO  time: code lens generation in 2.93s[0m
[0m2021.03.02 10:42:03 INFO  time: code lens generation in 2.68s[0m
[0m2021.03.02 10:42:03 INFO  time: code lens generation in 5.8s[0m
[0m2021.03.02 10:42:03 INFO  time: code lens generation in 4.63s[0m
[0m2021.03.02 10:42:03 INFO  time: code lens generation in 5.67s[0m
[0m2021.03.02 10:42:03 INFO  time: code lens generation in 1.38s[0m
[0m2021.03.02 10:42:03 INFO  time: code lens generation in 4.18s[0m
[0m2021.03.02 10:42:03 INFO  time: code lens generation in 5.84s[0m
[0m2021.03.02 10:42:03 INFO  time: code lens generation in 3.76s[0m
[0m2021.03.02 10:42:03 INFO  time: code lens generation in 3.3s[0m
[0m2021.03.02 10:42:03 INFO  time: Imported build in 0.31s[0m
[0m2021.03.02 10:42:06 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.02 10:42:07 INFO  time: indexed workspace in 3.77s[0m
[0m2021.03.02 10:42:08 INFO  Deduplicating compilation of scalas3read-build from bsp client 'Metals 0.10.0' (since 5.499s)[0m
[0m2021.03.02 10:42:09 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals270069522732714679/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._, _root_.sbtassembly.AssemblyPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin, _root_.sbtassembly.AssemblyPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.11.12"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7"




  )

  assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.03.02 10:42:12 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_265)[0m
[0m2021.03.02 10:42:14 INFO  [info] loading settings for project scalas3read-build-build-build from metals.sbt ...[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._, _root_.sbtassembly.AssemblyPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin, _root_.sbtassembly.AssemblyPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.11.12"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "scalas3read",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    // https://mvnrepository.com/artifact/org.apache.httpcomponents/httpclient
    libraryDependencies += "org.apache.httpcomponents" % "httpclient" % "4.5.12",
    // https://mvnrepository.com/artifact/commons-io/commons-io
    libraryDependencies += "commons-io" % "commons-io" % "2.8.0",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7",
    libraryDependencies += "org.apache.spark/spark-core_2.10/1.0.2, SURFsara/warcutils/2.1"




  )

  assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.03.02 10:42:16 INFO  time: code lens generation in 10s[0m
[0m2021.03.02 10:42:16 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project/project[0m
[0m2021.03.02 10:42:17 INFO  time: code lens generation in 7.47s[0m
[0m2021.03.02 10:42:17 INFO  time: code lens generation in 9.12s[0m
[0m2021.03.02 10:42:17 INFO  time: code lens generation in 8.29s[0m
[0m2021.03.02 10:42:18 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.03.02 10:42:18 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.03.02 10:42:21 INFO  [success] Generated .bloop/scalas3read-build-build.json[0m
[0m2021.03.02 10:42:21 INFO  [success] Total time: 3 s, completed Mar 2, 2021 10:42:21 AM[0m
[0m2021.03.02 10:42:21 INFO  [info] loading settings for project scalas3read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.03.02 10:42:21 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.03.02 10:42:23 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.03.02 10:42:23 INFO  [success] Total time: 2 s, completed Mar 2, 2021 10:42:24 AM[0m
[0m2021.03.02 10:42:26 ERROR /home/delaneylekien/project3/scalas3read/build.sbt:20: error: No Append.Value[Seq[sbt.librarymanagement.ModuleID], String] found, so String cannot be appended to Seq[sbt.librarymanagement.ModuleID][0m
[0m2021.03.02 10:42:26 ERROR     libraryDependencies += "org.apache.spark/spark-core_2.10/1.0.2, SURFsara/warcutils/2.1"[0m
[0m2021.03.02 10:42:26 ERROR                         ^[0m
[0m2021.03.02 10:42:26 ERROR sbt.compiler.EvalException: Type error in expression[0m
[0m2021.03.02 10:42:26 INFO  [error] sbt.compiler.EvalException: Type error in expression[0m
[0m2021.03.02 10:42:26 INFO  [error] Use 'last' for the full log.[0m
[0m2021.03.02 10:42:26 INFO  [warn] Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? (default: r)[0m
[0m2021.03.02 10:42:26 INFO  sbt bloopInstall exit: 1[0m
[0m2021.03.02 10:42:26 INFO  time: ran 'sbt bloopInstall' in 16s[0m
[0m2021.03.02 10:42:26 ERROR sbt command failed: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals270069522732714679/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall[0m
[0m2021.03.02 10:42:26 INFO  Disconnecting from Bloop session...[0m
[0m2021.03.02 10:42:26 INFO  Shut down connection with build server.[0m
[0m2021.03.02 10:42:26 INFO  Shut down connection with build server.[0m
[0m2021.03.02 10:42:26 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.03.02 10:42:26 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6814221023814022622/bsp.socket'...
No more data in the server stdin, exiting...
[0m2021.03.02 10:42:26 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 10:42:26 INFO  Attempting to connect to the build server...[0m
[0m2021.03.02 10:42:26 INFO  Attempting to connect to the build server...[0m
[0m2021.03.02 10:42:26 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 10:42:26 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 10:42:26 INFO  time: Connected to build server in 0.18s[0m
[0m2021.03.02 10:42:26 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.02 10:42:28 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.02 10:42:28 INFO  time: indexed workspace in 1.95s[0m
[0m2021.03.02 10:45:05 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals2747826109049519613/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.03.02 10:45:07 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_265)[0m
[0m2021.03.02 10:45:08 INFO  [info] loading settings for project scalas3read-build-build-build from metals.sbt ...[0m
[0m2021.03.02 10:45:08 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project/project[0m
[0m2021.03.02 10:45:10 INFO  [info] loading settings for project scalas3read-build-build from metals.sbt ...[0m
[0m2021.03.02 10:45:10 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project/project[0m
[0m2021.03.02 10:45:12 INFO  [success] Generated .bloop/scalas3read-build-build.json[0m
[0m2021.03.02 10:45:12 INFO  [success] Total time: 2 s, completed Mar 2, 2021 10:45:12 AM[0m
[0m2021.03.02 10:45:12 INFO  [info] loading settings for project scalas3read-build from metals.sbt,plugins.sbt ...[0m
[0m2021.03.02 10:45:12 INFO  [info] loading project definition from /home/delaneylekien/project3/scalas3read/project[0m
[0m2021.03.02 10:45:14 INFO  [success] Generated .bloop/scalas3read-build.json[0m
[0m2021.03.02 10:45:14 INFO  [success] Total time: 1 s, completed Mar 2, 2021 10:45:14 AM[0m
[0m2021.03.02 10:45:18 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.03.02 10:45:18 INFO  [info] set current project to scalas3read (in build file:/home/delaneylekien/project3/scalas3read/)[0m
[0m2021.03.02 10:45:22 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.03.02 10:45:22 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.03.02 10:45:24 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.03.02 10:45:24 INFO  [success] Generated .bloop/root.json[0m
[0m2021.03.02 10:45:24 INFO  [success] Total time: 5 s, completed Mar 2, 2021 10:45:24 AM[0m
[0m2021.03.02 10:45:24 INFO  sbt bloopInstall exit: 0[0m
[0m2021.03.02 10:45:24 INFO  time: ran 'sbt bloopInstall' in 19s[0m
[0m2021.03.02 10:45:24 INFO  Disconnecting from Bloop session...[0m
[0m2021.03.02 10:45:24 INFO  Shut down connection with build server.[0m
[0m2021.03.02 10:45:24 INFO  Shut down connection with build server.[0m
[0m2021.03.02 10:45:24 INFO  Shut down connection with build server.[0m
[0m2021.03.02 10:45:24 INFO  Attempting to connect to the build server...[0m
[0m2021.03.02 10:45:24 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 10:45:24 INFO  Attempting to connect to the build server...[0m
[0m2021.03.02 10:45:24 INFO  Attempting to connect to the build server...[0m
[0m2021.03.02 10:45:24 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 10:45:24 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 10:45:24 INFO  time: Connected to build server in 0.33s[0m
[0m2021.03.02 10:45:24 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.02 10:45:27 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.02 10:45:27 INFO  time: indexed workspace in 2.16s[0m
[0m2021.03.02 10:45:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:45:35 INFO  time: compiled root in 6.91s[0m
[0m2021.03.02 10:45:35 INFO  skipping build import with status 'Installed'[0m
[0m2021.03.02 10:45:56 INFO  compiling root-test (1 scala source)[0m
[0m2021.03.02 10:45:56 INFO  time: compiled root-test in 0.63s[0m
[0m2021.03.02 10:46:29 INFO  compiling root-test (1 scala source)[0m
[0m2021.03.02 10:46:29 INFO  time: compiled root-test in 0.61s[0m
[0m2021.03.02 11:12:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:12:26 INFO  time: compiled root in 1.74s[0m
[0m2021.03.02 11:13:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:13:10 INFO  time: compiled root in 1.68s[0m
[0m2021.03.02 11:15:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:15:18 INFO  time: compiled root in 1.45s[0m
[0m2021.03.02 11:17:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:17:17 INFO  time: compiled root in 1.35s[0m
[0m2021.03.02 11:21:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:21:35 INFO  time: compiled root in 1.96s[0m
[0m2021.03.02 11:25:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:25:34 INFO  time: compiled root in 1.27s[0m
[0m2021.03.02 11:28:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:28:32 INFO  time: compiled root in 1.42s[0m
[0m2021.03.02 11:34:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:34:31 INFO  time: compiled root in 1.14s[0m
[0m2021.03.02 11:34:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:34:58 INFO  time: compiled root in 1.16s[0m
[0m2021.03.02 11:36:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:36:13 INFO  time: compiled root in 1.38s[0m
[0m2021.03.02 11:36:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:36:39 INFO  time: compiled root in 1.12s[0m
[0m2021.03.02 11:40:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:40:06 INFO  time: compiled root in 1.04s[0m
[0m2021.03.02 11:40:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:40:19 INFO  time: compiled root in 1.32s[0m
[0m2021.03.02 11:43:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:43:42 INFO  time: compiled root in 1.11s[0m
[0m2021.03.02 11:47:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:47:51 INFO  time: compiled root in 1.09s[0m
[0m2021.03.02 11:48:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:48:31 INFO  time: compiled root in 0.33s[0m
[0m2021.03.02 11:48:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:48:37 INFO  time: compiled root in 1.13s[0m
[0m2021.03.02 11:52:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:52:32 INFO  time: compiled root in 1.47s[0m
[0m2021.03.02 11:54:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:54:19 INFO  time: compiled root in 1.06s[0m
[0m2021.03.02 11:55:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:55:16 INFO  time: compiled root in 1s[0m
[0m2021.03.02 11:58:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:58:23 INFO  time: compiled root in 1.06s[0m
[0m2021.03.02 11:58:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:58:26 INFO  time: compiled root in 1.11s[0m
[0m2021.03.02 12:11:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:11:07 INFO  time: compiled root in 0.24s[0m
[0m2021.03.02 12:11:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:11:17 INFO  time: compiled root in 0.22s[0m
[0m2021.03.02 12:48:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:48:22 INFO  time: compiled root in 1.15s[0m
[0m2021.03.02 14:18:59 INFO  shutting down Metals[0m
[0m2021.03.02 14:18:59 INFO  Shut down connection with build server.[0m
[0m2021.03.02 14:18:59 INFO  Shut down connection with build server.[0m
[0m2021.03.02 14:18:59 INFO  Shut down connection with build server.[0m
[0m2021.03.02 14:20:18 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.53.2.[0m
[0m2021.03.02 14:20:18 INFO  time: initialize in 0.48s[0m
[0m2021.03.02 14:20:19 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6330331727957832767/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.02 14:20:19 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
[0m2021.03.02 14:20:19 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.03.02 14:20:21 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val df = spark.read.load(
        "s3a://commoncrawl/cc-index/table/cc-main/warc/"
        )
    
    val commonCrawl = spark.sparkContext.textFile(
          "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    )
    
    // df.printSchema()
    df
    .select("url_host_name", "url_host_tld", "warc_record_offset")
    .filter($"crawl" === "CC-MAIN-2021-04")
    .filter($"subset" === "warc")
    .filter($"url_host_tld" === "us")
    .filter($"url_path".contains("job"))
    .show(100, false)

      // |-- url_surtkey: string (nullable = true)
      // |-- url: string (nullable = true)
      // |-- url_host_name: string (nullable = true)
      // |-- url_host_tld: string (nullable = true)
      // |-- url_host_2nd_last_part: string (nullable = true)
      // |-- url_host_3rd_last_part: string (nullable = true)
      // |-- url_host_4th_last_part: string (nullable = true)
      // |-- url_host_5th_last_part: string (nullable = true)
      // |-- url_host_registry_suffix: string (nullable = true)
      // |-- url_host_registered_domain: string (nullable = true)
      // |-- url_host_private_suffix: string (nullable = true)
      // |-- url_host_private_domain: string (nullable = true)
      // |-- url_protocol: string (nullable = true)
      // |-- url_port: integer (nullable = true)
      // |-- url_path: string (nullable = true)
      // |-- url_query: string (nullable = true)
      // |-- fetch_time: timestamp (nullable = true)
      // |-- fetch_status: short (nullable = true)
      // |-- content_digest: string (nullable = true)
      // |-- content_mime_type: string (nullable = true)
      // |-- content_mime_detected: string (nullable = true)
      // |-- warc_filename: string (nullable = true)
      // |-- warc_record_offset: integer (nullable = true)
      // |-- warc_record_length: integer (nullable = true)
      // |-- warc_segment: string (nullable = true)
      // |-- crawl: string (nullable = true)
      // |-- subset: string (nullable = true)
    
  }

  case class wet(
    warc: String, 
    warcTpye: String, 
    warcTargetUri: String, 
    warcDate: String, 
    warcRecordID: String,
    warcRefersTo: String,
    warcBlockDigest: String,
    warcIdentifiedContentLang: String,
    contentType: String, 
    contentLength: Int,
    content: String
    ) {}

}
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Loading previous analysis for 'root-test' from '/home/delaneylekien/project3/scalas3read/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/delaneylekien/project3/scalas3read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6330331727957832767/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6330331727957832767/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.02 14:20:24 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    val df = spark.read.load(
        "s3a://commoncrawl/cc-index/table/cc-main/warc/"
        )
    
    val commonCrawl = spark.sparkContext.textFile(
          "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    )
    
    // df.printSchema()
    df
    .select("url_host_name", "url_host_tld", "warc_record_offset")
    .filter($"crawl" === "CC-MAIN-2021-04")
    .filter($"subset" === "warc")
    .filter($"url_host_tld" === "us")
    .filter($"url_path".contains("job"))
    .show(100, false)

      // |-- url_surtkey: string (nullable = true)
      // |-- url: string (nullable = true)
      // |-- url_host_name: string (nullable = true)
      // |-- url_host_tld: string (nullable = true)
      // |-- url_host_2nd_last_part: string (nullable = true)
      // |-- url_host_3rd_last_part: string (nullable = true)
      // |-- url_host_4th_last_part: string (nullable = true)
      // |-- url_host_5th_last_part: string (nullable = true)
      // |-- url_host_registry_suffix: string (nullable = true)
      // |-- url_host_registered_domain: string (nullable = true)
      // |-- url_host_private_suffix: string (nullable = true)
      // |-- url_host_private_domain: string (nullable = true)
      // |-- url_protocol: string (nullable = true)
      // |-- url_port: integer (nullable = true)
      // |-- url_path: string (nullable = true)
      // |-- url_query: string (nullable = true)
      // |-- fetch_time: timestamp (nullable = true)
      // |-- fetch_status: short (nullable = true)
      // |-- content_digest: string (nullable = true)
      // |-- content_mime_type: string (nullable = true)
      // |-- content_mime_detected: string (nullable = true)
      // |-- warc_filename: string (nullable = true)
      // |-- warc_record_offset: integer (nullable = true)
      // |-- warc_record_length: integer (nullable = true)
      // |-- warc_segment: string (nullable = true)
      // |-- crawl: string (nullable = true)
      // |-- subset: string (nullable = true)
    
  }

  case class wet(
    warc: String, 
    warcTpye: String, 
    warcTargetUri: String, 
    warcDate: String, 
    warcRecordID: String,
    warcRefersTo: String,
    warcBlockDigest: String,
    warcIdentifiedContentLang: String,
    contentType: String, 
    contentLength: Int,
    content: String
    ) {}

}
[0m2021.03.02 14:20:25 INFO  time: code lens generation in 5.11s[0m
[0m2021.03.02 14:20:24 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6898574133099767740/bsp.socket'...
[0m2021.03.02 14:20:24 INFO  Attempting to connect to the build server...Waiting for the bsp connection to come up...[0m

Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7500092019097771724/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7500092019097771724/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7500092019097771724/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.02 14:20:25 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6898574133099767740/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6898574133099767740/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.02 14:20:25 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.02 14:20:25 INFO  time: Connected to build server in 5.7s[0m
[0m2021.03.02 14:20:25 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.02 14:20:25 INFO  time: Imported build in 0.37s[0m
[0m2021.03.02 14:20:28 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.02 14:20:29 INFO  time: indexed workspace in 3.95s[0m
Mar 02, 2021 2:21:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 33
Mar 02, 2021 2:22:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 54
Mar 02, 2021 2:22:19 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 123
[0m2021.03.02 14:25:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:25:36 INFO  time: compiled root in 5.2s[0m
[0m2021.03.02 14:25:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:25:45 INFO  time: compiled root in 1.9s[0m
[0m2021.03.02 14:27:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:27:07 INFO  time: compiled root in 2.02s[0m
[0m2021.03.02 14:33:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:33:30 INFO  time: compiled root in 0.41s[0m
[0m2021.03.02 14:33:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:33:38 INFO  time: compiled root in 1.25s[0m
[0m2021.03.02 14:46:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:46:16 INFO  time: compiled root in 1.08s[0m
[0m2021.03.02 14:48:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:48:19 INFO  time: compiled root in 0.27s[0m
[0m2021.03.02 14:48:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:48:33 INFO  time: compiled root in 1.23s[0m
Mar 02, 2021 2:49:28 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 703
[0m2021.03.02 14:51:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:51:27 INFO  time: compiled root in 1.6s[0m
[0m2021.03.02 14:52:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:52:29 INFO  time: compiled root in 1.37s[0m
Mar 02, 2021 2:54:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 993
[0m2021.03.02 14:54:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:54:17 INFO  time: compiled root in 1.31s[0m
[0m2021.03.02 14:55:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:55:29 INFO  time: compiled root in 1.24s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.util.{Locale, Properties}

import scala.collection.JavaConverters._

import com.fasterxml.jackson.databind.ObjectMapper
import com.univocity.parsers.csv.CsvParser

import org.apache.spark.Partition
import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.internal.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.json.{CreateJacksonParser, JacksonParser, JSONOptions}
import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap
import org.apache.spark.sql.execution.command.DDLUtils
import org.apache.spark.sql.execution.datasources.{DataSource, FailureSafeParser}
import org.apache.spark.sql.execution.datasources.csv._
import org.apache.spark.sql.execution.datasources.jdbc._
import org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource
import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation
import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils
import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport}
import org.apache.spark.sql.types.{StringType, StructType}
import org.apache.spark.unsafe.types.UTF8String

/**
 * Interface used to load a [[Dataset]] from external storage systems (e.g. file systems,
 * key-value stores, etc). Use `SparkSession.read` to access this.
 *
 * @since 1.4.0
 */
@InterfaceStability.Stable
class DataFrameReader private[sql](sparkSession: SparkSession) extends Logging {

  /**
   * Specifies the input data source format.
   *
   * @since 1.4.0
   */
  def format(source: String): DataFrameReader = {
    this.source = source
    this
  }

  /**
   * Specifies the input schema. Some data sources (e.g. JSON) can infer the input schema
   * automatically from data. By specifying the schema here, the underlying data source can
   * skip the schema inference step, and thus speed up data loading.
   *
   * @since 1.4.0
   */
  def schema(schema: StructType): DataFrameReader = {
    this.userSpecifiedSchema = Option(schema)
    this
  }

  /**
   * Specifies the schema by using the input DDL-formatted string. Some data sources (e.g. JSON) can
   * infer the input schema automatically from data. By specifying the schema here, the underlying
   * data source can skip the schema inference step, and thus speed up data loading.
   *
   * {{{
   *   spark.read.schema("a INT, b STRING, c DOUBLE").csv("test.csv")
   * }}}
   *
   * @since 2.3.0
   */
  def schema(schemaString: String): DataFrameReader = {
    this.userSpecifiedSchema = Option(StructType.fromDDL(schemaString))
    this
  }

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def option(key: String, value: String): DataFrameReader = {
    this.extraOptions += (key -> value)
    this
  }

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Boolean): DataFrameReader = option(key, value.toString)

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Long): DataFrameReader = option(key, value.toString)

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Double): DataFrameReader = option(key, value.toString)

  /**
   * (Scala-specific) Adds input options for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def options(options: scala.collection.Map[String, String]): DataFrameReader = {
    this.extraOptions ++= options
    this
  }

  /**
   * Adds input options for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def options(options: java.util.Map[String, String]): DataFrameReader = {
    this.options(options.asScala)
    this
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that don't require a path (e.g. external
   * key-value stores).
   *
   * @since 1.4.0
   */
  def load(): DataFrame = {
    load(Seq.empty: _*) // force invocation of `load(...varargs...)`
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that require a path (e.g. data backed by
   * a local or distributed file system).
   *
   * @since 1.4.0
   */
  def load(path: String): DataFrame = {
    // force invocation of `load(...varargs...)`
    option(DataSourceOptions.PATH_KEY, path).load(Seq.empty: _*)
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that support multiple paths.
   * Only works if the source is a HadoopFsRelationProvider.
   *
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def load(paths: String*): DataFrame = {
    if (source.toLowerCase(Locale.ROOT) == DDLUtils.HIVE_PROVIDER) {
      throw new AnalysisException("Hive data source can only be used with tables, you can not " +
        "read files of Hive data source directly.")
    }

    val cls = DataSource.lookupDataSource(source, sparkSession.sessionState.conf)
    if (classOf[DataSourceV2].isAssignableFrom(cls)) {
      val ds = cls.newInstance().asInstanceOf[DataSourceV2]
      if (ds.isInstanceOf[ReadSupport]) {
        val sessionOptions = DataSourceV2Utils.extractSessionConfigs(
          ds = ds, conf = sparkSession.sessionState.conf)
        val pathsOption = {
          val objectMapper = new ObjectMapper()
          DataSourceOptions.PATHS_KEY -> objectMapper.writeValueAsString(paths.toArray)
        }
        Dataset.ofRows(sparkSession, DataSourceV2Relation.create(
          ds, sessionOptions ++ extraOptions.toMap + pathsOption,
          userSpecifiedSchema = userSpecifiedSchema))
      } else {
        loadV1Source(paths: _*)
      }
    } else {
      loadV1Source(paths: _*)
    }
  }

  private def loadV1Source(paths: String*) = {
    // Code path for data source v1.
    sparkSession.baseRelationToDataFrame(
      DataSource.apply(
        sparkSession,
        paths = paths,
        userSpecifiedSchema = userSpecifiedSchema,
        className = source,
        options = extraOptions.toMap).resolveRelation())
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table and connection properties.
   *
   * @since 1.4.0
   */
  def jdbc(url: String, table: String, properties: Properties): DataFrame = {
    assertNoSpecifiedSchema("jdbc")
    // properties should override settings in extraOptions.
    this.extraOptions ++= properties.asScala
    // explicit url and dbtable should override all
    this.extraOptions ++= Seq(JDBCOptions.JDBC_URL -> url, JDBCOptions.JDBC_TABLE_NAME -> table)
    format("jdbc").load()
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table. Partitions of the table will be retrieved in parallel based on the parameters
   * passed to this function.
   *
   * Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
   * your external database systems.
   *
   * @param url JDBC database url of the form `jdbc:subprotocol:subname`.
   * @param table Name of the table in the external database.
   * @param columnName the name of a column of numeric, date, or timestamp type
   *                   that will be used for partitioning.
   * @param lowerBound the minimum value of `columnName` used to decide partition stride.
   * @param upperBound the maximum value of `columnName` used to decide partition stride.
   * @param numPartitions the number of partitions. This, along with `lowerBound` (inclusive),
   *                      `upperBound` (exclusive), form partition strides for generated WHERE
   *                      clause expressions used to split the column `columnName` evenly. When
   *                      the input is less than 1, the number is set to 1.
   * @param connectionProperties JDBC database connection arguments, a list of arbitrary string
   *                             tag/value. Normally at least a "user" and "password" property
   *                             should be included. "fetchsize" can be used to control the
   *                             number of rows per fetch and "queryTimeout" can be used to wait
   *                             for a Statement object to execute to the given number of seconds.
   * @since 1.4.0
   */
  def jdbc(
      url: String,
      table: String,
      columnName: String,
      lowerBound: Long,
      upperBound: Long,
      numPartitions: Int,
      connectionProperties: Properties): DataFrame = {
    // columnName, lowerBound, upperBound and numPartitions override settings in extraOptions.
    this.extraOptions ++= Map(
      JDBCOptions.JDBC_PARTITION_COLUMN -> columnName,
      JDBCOptions.JDBC_LOWER_BOUND -> lowerBound.toString,
      JDBCOptions.JDBC_UPPER_BOUND -> upperBound.toString,
      JDBCOptions.JDBC_NUM_PARTITIONS -> numPartitions.toString)
    jdbc(url, table, connectionProperties)
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table using connection properties. The `predicates` parameter gives a list
   * expressions suitable for inclusion in WHERE clauses; each one defines one partition
   * of the `DataFrame`.
   *
   * Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
   * your external database systems.
   *
   * @param url JDBC database url of the form `jdbc:subprotocol:subname`
   * @param table Name of the table in the external database.
   * @param predicates Condition in the where clause for each partition.
   * @param connectionProperties JDBC database connection arguments, a list of arbitrary string
   *                             tag/value. Normally at least a "user" and "password" property
   *                             should be included. "fetchsize" can be used to control the
   *                             number of rows per fetch.
   * @since 1.4.0
   */
  def jdbc(
      url: String,
      table: String,
      predicates: Array[String],
      connectionProperties: Properties): DataFrame = {
    assertNoSpecifiedSchema("jdbc")
    // connectionProperties should override settings in extraOptions.
    val params = extraOptions ++ connectionProperties.asScala
    val options = new JDBCOptions(url, table, params)
    val parts: Array[Partition] = predicates.zipWithIndex.map { case (part, i) =>
      JDBCPartition(part, i) : Partition
    }
    val relation = JDBCRelation(parts, options)(sparkSession)
    sparkSession.baseRelationToDataFrame(relation)
  }

  /**
   * Loads a JSON file and returns the results as a `DataFrame`.
   *
   * See the documentation on the overloaded `json()` method with varargs for more details.
   *
   * @since 1.4.0
   */
  def json(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    json(Seq(path): _*)
  }

  /**
   * Loads JSON files and returns the results as a `DataFrame`.
   *
   * <a href="http://jsonlines.org/">JSON Lines</a> (newline-delimited JSON) is supported by
   * default. For JSON (one record per file), set the `multiLine` option to true.
   *
   * This function goes through the input once to determine the input schema. If you know the
   * schema in advance, use the version that specifies the schema to avoid the extra scan.
   *
   * You can set the following JSON-specific options to deal with non-standard JSON files:
   * <ul>
   * <li>`primitivesAsString` (default `false`): infers all primitive values as a string type</li>
   * <li>`prefersDecimal` (default `false`): infers all floating-point values as a decimal
   * type. If the values do not fit in decimal, then it infers them as doubles.</li>
   * <li>`allowComments` (default `false`): ignores Java/C++ style comment in JSON records</li>
   * <li>`allowUnquotedFieldNames` (default `false`): allows unquoted JSON field names</li>
   * <li>`allowSingleQuotes` (default `true`): allows single quotes in addition to double quotes
   * </li>
   * <li>`allowNumericLeadingZeros` (default `false`): allows leading zeros in numbers
   * (e.g. 00012)</li>
   * <li>`allowBackslashEscapingAnyCharacter` (default `false`): allows accepting quoting of all
   * character using backslash quoting mechanism</li>
   * <li>`allowUnquotedControlChars` (default `false`): allows JSON Strings to contain unquoted
   * control characters (ASCII characters with value less than 32, including tab and line feed
   * characters) or not.</li>
   * <li>`mode` (default `PERMISSIVE`): allows a mode for dealing with corrupt records
   * during parsing.
   *   <ul>
   *     <li>`PERMISSIVE` : when it meets a corrupted record, puts the malformed string into a
   *     field configured by `columnNameOfCorruptRecord`, and sets other fields to `null`. To
   *     keep corrupt records, an user can set a string type field named
   *     `columnNameOfCorruptRecord` in an user-defined schema. If a schema does not have the
   *     field, it drops corrupt records during parsing. When inferring a schema, it implicitly
   *     adds a `columnNameOfCorruptRecord` field in an output schema.</li>
   *     <li>`DROPMALFORMED` : ignores the whole corrupted records.</li>
   *     <li>`FAILFAST` : throws an exception when it meets corrupted records.</li>
   *   </ul>
   * </li>
   * <li>`columnNameOfCorruptRecord` (default is the value specified in
   * `spark.sql.columnNameOfCorruptRecord`): allows renaming the new field having malformed string
   * created by `PERMISSIVE` mode. This overrides `spark.sql.columnNameOfCorruptRecord`.</li>
   * <li>`dateFormat` (default `yyyy-MM-dd`): sets the string that indicates a date format.
   * Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to
   * date type.</li>
   * <li>`timestampFormat` (default `yyyy-MM-dd'T'HH:mm:ss.SSSXXX`): sets the string that
   * indicates a timestamp format. Custom date formats follow the formats at
   * `java.text.SimpleDateFormat`. This applies to timestamp type.</li>
   * <li>`multiLine` (default `false`): parse one record, which may span multiple lines,
   * per file</li>
   * <li>`encoding` (by default it is not set): allows to forcibly set one of standard basic
   * or extended encoding for the JSON files. For example UTF-16BE, UTF-32LE. If the encoding
   * is not specified and `multiLine` is set to `true`, it will be detected automatically.</li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * <li>`samplingRatio` (default is 1.0): defines fraction of input JSON objects used
   * for schema inferring.</li>
   * <li>`dropFieldIfAllNull` (default `false`): whether to ignore column of all null values or
   * empty array/struct during schema inference.</li>
   * </ul>
   *
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def json(paths: String*): DataFrame = format("json").load(paths : _*)

  /**
   * Loads a `JavaRDD[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON
   * Lines text format or newline-delimited JSON</a>) and returns the result as
   * a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonRDD input RDD with one JSON object per record
   * @since 1.4.0
   */
  @deprecated("Use json(Dataset[String]) instead.", "2.2.0")
  def json(jsonRDD: JavaRDD[String]): DataFrame = json(jsonRDD.rdd)

  /**
   * Loads an `RDD[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON Lines
   * text format or newline-delimited JSON</a>) and returns the result as a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonRDD input RDD with one JSON object per record
   * @since 1.4.0
   */
  @deprecated("Use json(Dataset[String]) instead.", "2.2.0")
  def json(jsonRDD: RDD[String]): DataFrame = {
    json(sparkSession.createDataset(jsonRDD)(Encoders.STRING))
  }

  /**
   * Loads a `Dataset[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON Lines
   * text format or newline-delimited JSON</a>) and returns the result as a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonDataset input Dataset with one JSON object per record
   * @since 2.2.0
   */
  def json(jsonDataset: Dataset[String]): DataFrame = {
    val parsedOptions = new JSONOptions(
      extraOptions.toMap,
      sparkSession.sessionState.conf.sessionLocalTimeZone,
      sparkSession.sessionState.conf.columnNameOfCorruptRecord)

    val schema = userSpecifiedSchema.getOrElse {
      TextInputJsonDataSource.inferFromDataset(jsonDataset, parsedOptions)
    }

    verifyColumnNameOfCorruptRecord(schema, parsedOptions.columnNameOfCorruptRecord)
    val actualSchema =
      StructType(schema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord))

    val createParser = CreateJacksonParser.string _
    val parsed = jsonDataset.rdd.mapPartitions { iter =>
      val rawParser = new JacksonParser(actualSchema, parsedOptions)
      val parser = new FailureSafeParser[String](
        input => rawParser.parse(input, createParser, UTF8String.fromString),
        parsedOptions.parseMode,
        schema,
        parsedOptions.columnNameOfCorruptRecord)
      iter.flatMap(parser.parse)
    }
    sparkSession.internalCreateDataFrame(parsed, schema, isStreaming = jsonDataset.isStreaming)
  }

  /**
   * Loads a CSV file and returns the result as a `DataFrame`. See the documentation on the
   * other overloaded `csv()` method for more details.
   *
   * @since 2.0.0
   */
  def csv(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    csv(Seq(path): _*)
  }

  /**
   * Loads an `Dataset[String]` storing CSV rows and returns the result as a `DataFrame`.
   *
   * If the schema is not specified using `schema` function and `inferSchema` option is enabled,
   * this function goes through the input once to determine the input schema.
   *
   * If the schema is not specified using `schema` function and `inferSchema` option is disabled,
   * it determines the columns as string types and it reads only the first line to determine the
   * names and the number of fields.
   *
   * If the enforceSchema is set to `false`, only the CSV header in the first line is checked
   * to conform specified or inferred schema.
   *
   * @param csvDataset input Dataset with one CSV row per record
   * @since 2.2.0
   */
  def csv(csvDataset: Dataset[String]): DataFrame = {
    val parsedOptions: CSVOptions = new CSVOptions(
      extraOptions.toMap,
      sparkSession.sessionState.conf.csvColumnPruning,
      sparkSession.sessionState.conf.sessionLocalTimeZone)
    val filteredLines: Dataset[String] =
      CSVUtils.filterCommentAndEmpty(csvDataset, parsedOptions)
    val maybeFirstLine: Option[String] = filteredLines.take(1).headOption

    val schema = userSpecifiedSchema.getOrElse {
      TextInputCSVDataSource.inferFromDataset(
        sparkSession,
        csvDataset,
        maybeFirstLine,
        parsedOptions)
    }

    verifyColumnNameOfCorruptRecord(schema, parsedOptions.columnNameOfCorruptRecord)
    val actualSchema =
      StructType(schema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord))

    val linesWithoutHeader = if (parsedOptions.headerFlag && maybeFirstLine.isDefined) {
      val firstLine = maybeFirstLine.get
      val parser = new CsvParser(parsedOptions.asParserSettings)
      val columnNames = parser.parseLine(firstLine)
      CSVDataSource.checkHeaderColumnNames(
        actualSchema,
        columnNames,
        csvDataset.getClass.getCanonicalName,
        parsedOptions.enforceSchema,
        sparkSession.sessionState.conf.caseSensitiveAnalysis)
      filteredLines.rdd.mapPartitions(CSVUtils.filterHeaderLine(_, firstLine, parsedOptions))
    } else {
      filteredLines.rdd
    }

    val parsed = linesWithoutHeader.mapPartitions { iter =>
      val rawParser = new UnivocityParser(actualSchema, parsedOptions)
      val parser = new FailureSafeParser[String](
        input => Seq(rawParser.parse(input)),
        parsedOptions.parseMode,
        schema,
        parsedOptions.columnNameOfCorruptRecord)
      iter.flatMap(parser.parse)
    }
    sparkSession.internalCreateDataFrame(parsed, schema, isStreaming = csvDataset.isStreaming)
  }

  /**
   * Loads CSV files and returns the result as a `DataFrame`.
   *
   * This function will go through the input once to determine the input schema if `inferSchema`
   * is enabled. To avoid going through the entire data once, disable `inferSchema` option or
   * specify the schema explicitly using `schema`.
   *
   * You can set the following CSV-specific options to deal with CSV files:
   * <ul>
   * <li>`sep` (default `,`): sets a single character as a separator for each
   * field and value.</li>
   * <li>`encoding` (default `UTF-8`): decodes the CSV files by the given encoding
   * type.</li>
   * <li>`quote` (default `"`): sets a single character used for escaping quoted values where
   * the separator can be part of the value. If you would like to turn off quotations, you need to
   * set not `null` but an empty string. This behaviour is different from
   * `com.databricks.spark.csv`.</li>
   * <li>`escape` (default `\`): sets a single character used for escaping quotes inside
   * an already quoted value.</li>
   * <li>`charToEscapeQuoteEscaping` (default `escape` or `\0`): sets a single character used for
   * escaping the escape for the quote character. The default value is escape character when escape
   * and quote characters are different, `\0` otherwise.</li>
   * <li>`comment` (default empty string): sets a single character used for skipping lines
   * beginning with this character. By default, it is disabled.</li>
   * <li>`header` (default `false`): uses the first line as names of columns.</li>
   * <li>`enforceSchema` (default `true`): If it is set to `true`, the specified or inferred schema
   * will be forcibly applied to datasource files, and headers in CSV files will be ignored.
   * If the option is set to `false`, the schema will be validated against all headers in CSV files
   * in the case when the `header` option is set to `true`. Field names in the schema
   * and column names in CSV headers are checked by their positions taking into account
   * `spark.sql.caseSensitive`. Though the default value is true, it is recommended to disable
   * the `enforceSchema` option to avoid incorrect results.</li>
   * <li>`inferSchema` (default `false`): infers the input schema automatically from data. It
   * requires one extra pass over the data.</li>
   * <li>`samplingRatio` (default is 1.0): defines fraction of rows used for schema inferring.</li>
   * <li>`ignoreLeadingWhiteSpace` (default `false`): a flag indicating whether or not leading
   * whitespaces from values being read should be skipped.</li>
   * <li>`ignoreTrailingWhiteSpace` (default `false`): a flag indicating whether or not trailing
   * whitespaces from values being read should be skipped.</li>
   * <li>`nullValue` (default empty string): sets the string representation of a null value. Since
   * 2.0.1, this applies to all supported types including the string type.</li>
   * <li>`emptyValue` (default empty string): sets the string representation of an empty value.</li>
   * <li>`nanValue` (default `NaN`): sets the string representation of a non-number" value.</li>
   * <li>`positiveInf` (default `Inf`): sets the string representation of a positive infinity
   * value.</li>
   * <li>`negativeInf` (default `-Inf`): sets the string representation of a negative infinity
   * value.</li>
   * <li>`dateFormat` (default `yyyy-MM-dd`): sets the string that indicates a date format.
   * Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to
   * date type.</li>
   * <li>`timestampFormat` (default `yyyy-MM-dd'T'HH:mm:ss.SSSXXX`): sets the string that
   * indicates a timestamp format. Custom date formats follow the formats at
   * `java.text.SimpleDateFormat`. This applies to timestamp type.</li>
   * <li>`maxColumns` (default `20480`): defines a hard limit of how many columns
   * a record can have.</li>
   * <li>`maxCharsPerColumn` (default `-1`): defines the maximum number of characters allowed
   * for any given value being read. By default, it is -1 meaning unlimited length</li>
   * <li>`mode` (default `PERMISSIVE`): allows a mode for dealing with corrupt records
   *    during parsing. It supports the following case-insensitive modes. Note that Spark tries
   *    to parse only required columns in CSV under column pruning. Therefore, corrupt records
   *    can be different based on required set of fields. This behavior can be controlled by
   *    `spark.sql.csv.parser.columnPruning.enabled` (enabled by default).
   *   <ul>
   *     <li>`PERMISSIVE` : when it meets a corrupted record, puts the malformed string into a
   *     field configured by `columnNameOfCorruptRecord`, and sets other fields to `null`. To keep
   *     corrupt records, an user can set a string type field named `columnNameOfCorruptRecord`
   *     in an user-defined schema. If a schema does not have the field, it drops corrupt records
   *     during parsing. A record with less/more tokens than schema is not a corrupted record to
   *     CSV. When it meets a record having fewer tokens than the length of the schema, sets
   *     `null` to extra fields. When the record has more tokens than the length of the schema,
   *     it drops extra tokens.</li>
   *     <li>`DROPMALFORMED` : ignores the whole corrupted records.</li>
   *     <li>`FAILFAST` : throws an exception when it meets corrupted records.</li>
   *   </ul>
   * </li>
   * <li>`columnNameOfCorruptRecord` (default is the value specified in
   * `spark.sql.columnNameOfCorruptRecord`): allows renaming the new field having malformed string
   * created by `PERMISSIVE` mode. This overrides `spark.sql.columnNameOfCorruptRecord`.</li>
   * <li>`multiLine` (default `false`): parse one record, which may span multiple lines.</li>
   * </ul>
   *
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def csv(paths: String*): DataFrame = format("csv").load(paths : _*)

  /**
   * Loads a Parquet file, returning the result as a `DataFrame`. See the documentation
   * on the other overloaded `parquet()` method for more details.
   *
   * @since 2.0.0
   */
  def parquet(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    parquet(Seq(path): _*)
  }

  /**
   * Loads a Parquet file, returning the result as a `DataFrame`.
   *
   * You can set the following Parquet-specific option(s) for reading Parquet files:
   * <ul>
   * <li>`mergeSchema` (default is the value specified in `spark.sql.parquet.mergeSchema`): sets
   * whether we should merge schemas collected from all Parquet part-files. This will override
   * `spark.sql.parquet.mergeSchema`.</li>
   * </ul>
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def parquet(paths: String*): DataFrame = {
    format("parquet").load(paths: _*)
  }

  /**
   * Loads an ORC file and returns the result as a `DataFrame`.
   *
   * @param path input path
   * @since 1.5.0
   */
  def orc(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    orc(Seq(path): _*)
  }

  /**
   * Loads ORC files and returns the result as a `DataFrame`.
   *
   * @param paths input paths
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orc(paths: String*): DataFrame = format("orc").load(paths: _*)

  /**
   * Returns the specified table as a `DataFrame`.
   *
   * @since 1.4.0
   */
  def table(tableName: String): DataFrame = {
    assertNoSpecifiedSchema("table")
    sparkSession.table(tableName)
  }

  /**
   * Loads text files and returns a `DataFrame` whose schema starts with a string column named
   * "value", and followed by partitioned columns if there are any. See the documentation on
   * the other overloaded `text()` method for more details.
   *
   * @since 2.0.0
   */
  def text(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    text(Seq(path): _*)
  }

  /**
   * Loads text files and returns a `DataFrame` whose schema starts with a string column named
   * "value", and followed by partitioned columns if there are any.
   *
   * By default, each line in the text files is a new row in the resulting DataFrame. For example:
   * {{{
   *   // Scala:
   *   spark.read.text("/path/to/spark/README.md")
   *
   *   // Java:
   *   spark.read().text("/path/to/spark/README.md")
   * }}}
   *
   * You can set the following text-specific option(s) for reading text files:
   * <ul>
   * <li>`wholetext` (default `false`): If true, read a file as a single row and not split by "\n".
   * </li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * </ul>
   *
   * @param paths input paths
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def text(paths: String*): DataFrame = format("text").load(paths : _*)

  /**
   * Loads text files and returns a [[Dataset]] of String. See the documentation on the
   * other overloaded `textFile()` method for more details.
   * @since 2.0.0
   */
  def textFile(path: String): Dataset[String] = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    textFile(Seq(path): _*)
  }

  /**
   * Loads text files and returns a [[Dataset]] of String. The underlying schema of the Dataset
   * contains a single string column named "value".
   *
   * If the directory structure of the text files contains partitioning information, those are
   * ignored in the resulting Dataset. To include partitioning information as columns, use `text`.
   *
   * By default, each line in the text files is a new row in the resulting DataFrame. For example:
   * {{{
   *   // Scala:
   *   spark.read.textFile("/path/to/spark/README.md")
   *
   *   // Java:
   *   spark.read().textFile("/path/to/spark/README.md")
   * }}}
   *
   * You can set the following textFile-specific option(s) for reading text files:
   * <ul>
   * <li>`wholetext` (default `false`): If true, read a file as a single row and not split by "\n".
   * </li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * </ul>
   *
   * @param paths input path
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def textFile(paths: String*): Dataset[String] = {
    assertNoSpecifiedSchema("textFile")
    text(paths : _*).select("value").as[String](sparkSession.implicits.newStringEncoder)
  }

  /**
   * A convenient function for schema validation in APIs.
   */
  private def assertNoSpecifiedSchema(operation: String): Unit = {
    if (userSpecifiedSchema.nonEmpty) {
      throw new AnalysisException(s"User specified schema not supported with `$operation`")
    }
  }

  /**
   * A convenient function for schema validation in datasources supporting
   * `columnNameOfCorruptRecord` as an option.
   */
  private def verifyColumnNameOfCorruptRecord(
      schema: StructType,
      columnNameOfCorruptRecord: String): Unit = {
    schema.getFieldIndex(columnNameOfCorruptRecord).foreach { corruptFieldIndex =>
      val f = schema(corruptFieldIndex)
      if (f.dataType != StringType || !f.nullable) {
        throw new AnalysisException(
          "The field for corrupt records must be string type and nullable")
      }
    }
  }

  ///////////////////////////////////////////////////////////////////////////////////////
  // Builder pattern config options
  ///////////////////////////////////////////////////////////////////////////////////////

  private var source: String = sparkSession.sessionState.conf.defaultDataSourceName

  private var userSpecifiedSchema: Option[StructType] = None

  private var extraOptions = CaseInsensitiveMap[String](Map.empty)

}

[0m2021.03.02 14:56:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:56:13 INFO  time: compiled root in 2.09s[0m
[0m2021.03.02 14:57:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:57:45 INFO  time: compiled root in 0.66s[0m
[0m2021.03.02 14:57:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:57:47 INFO  time: compiled root in 1.2s[0m
[0m2021.03.02 14:57:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:57:51 INFO  time: compiled root in 1.15s[0m
[0m2021.03.02 14:59:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:59:08 INFO  time: compiled root in 1.13s[0m
[0m2021.03.02 15:01:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:01:12 INFO  time: compiled root in 1.22s[0m
[0m2021.03.02 15:01:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:01:59 INFO  time: compiled root in 1s[0m
[0m2021.03.02 15:28:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:28:11 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 15:30:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:30:28 INFO  time: compiled root in 1.09s[0m
[0m2021.03.02 15:31:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:31:07 INFO  time: compiled root in 1.12s[0m
[0m2021.03.02 15:31:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:31:59 INFO  time: compiled root in 1.72s[0m
[0m2021.03.02 15:32:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:32:09 INFO  time: compiled root in 1.39s[0m
[0m2021.03.02 15:34:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:34:23 INFO  time: compiled root in 0.33s[0m
[0m2021.03.02 15:36:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:36:34 INFO  time: compiled root in 0.39s[0m
[0m2021.03.02 15:36:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:36:47 INFO  time: compiled root in 0.34s[0m
[0m2021.03.02 15:38:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:38:02 INFO  time: compiled root in 0.28s[0m
[0m2021.03.02 15:38:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:17: stale bloop error: type mismatch;
 found   : org.apache.spark.sql.Dataset[String]
 required: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
      .unionAll(stateList)
                ^^^^^^^^^[0m
[0m2021.03.02 15:38:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:17: stale bloop error: type mismatch;
 found   : org.apache.spark.sql.Dataset[String]
 required: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
      .unionAll(stateList)
                ^^^^^^^^^[0m
[0m2021.03.02 15:38:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:17: stale bloop error: type mismatch;
 found   : org.apache.spark.sql.Dataset[String]
 required: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
      .unionAll(stateList)
                ^^^^^^^^^[0m
[0m2021.03.02 15:38:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:17: stale bloop error: type mismatch;
 found   : org.apache.spark.sql.Dataset[String]
 required: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
      .unionAll(stateList)
                ^^^^^^^^^[0m
[0m2021.03.02 15:38:10 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:17: stale bloop error: type mismatch;
 found   : org.apache.spark.sql.Dataset[String]
 required: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
      .unionAll(stateList)
                ^^^^^^^^^[0m
[0m2021.03.02 15:38:10 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:17: stale bloop error: type mismatch;
 found   : org.apache.spark.sql.Dataset[String]
 required: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
      .unionAll(stateList)
                ^^^^^^^^^[0m
[0m2021.03.02 15:38:10 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:17: stale bloop error: type mismatch;
 found   : org.apache.spark.sql.Dataset[String]
 required: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
      .unionAll(stateList)
                ^^^^^^^^^[0m
[0m2021.03.02 15:38:10 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:43:17: stale bloop error: type mismatch;
 found   : org.apache.spark.sql.Dataset[String]
 required: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
      .unionAll(stateList)
                ^^^^^^^^^[0m
[0m2021.03.02 15:38:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:38:12 INFO  time: compiled root in 0.28s[0m
[0m2021.03.02 15:38:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:38:54 WARN  there was one deprecation warning; re-run with -deprecation for details[0m
[0m2021.03.02 15:38:54 INFO  time: compiled root in 1.17s[0m
[0m2021.03.02 15:38:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:39:00 INFO  time: compiled root in 1.3s[0m
[0m2021.03.02 15:40:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:40:11 INFO  time: compiled root in 1.14s[0m
[0m2021.03.02 15:59:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:59:29 INFO  time: compiled root in 0.31s[0m
[0m2021.03.02 15:59:37 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ambiguous reference to overloaded definition,
both method join in class Dataset of type (right: org.apache.spark.sql.Dataset[_], joinExprs: org.apache.spark.sql.Column, joinType: String)org.apache.spark.sql.DataFrame
and  method join in class Dataset of type (right: org.apache.spark.sql.Dataset[_], joinExprs: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame
match expected type ?
> censusData
>       .join[0m
[0m2021.03.02 15:59:37 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ambiguous reference to overloaded definition,
both method join in class Dataset of type (right: org.apache.spark.sql.Dataset[_], joinExprs: org.apache.spark.sql.Column, joinType: String)org.apache.spark.sql.DataFrame
and  method join in class Dataset of type (right: org.apache.spark.sql.Dataset[_], joinExprs: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame
match expected type ?
> censusData
>       .join[0m
[0m2021.03.02 15:59:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:59:40 INFO  time: compiled root in 1.2s[0m
[0m2021.03.02 15:59:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:59:44 INFO  time: compiled root in 1.07s[0m
[0m2021.03.02 15:59:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:59:45 INFO  time: compiled root in 1.5s[0m
[0m2021.03.02 16:03:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:03:27 INFO  time: compiled root in 0.33s[0m
[0m2021.03.02 16:05:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:05:14 INFO  time: compiled root in 1.02s[0m
Mar 02, 2021 4:09:49 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: String index out of range: -1
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
	at java.lang.String.<init>(String.java:196)
	at scala.tools.nsc.interactive.Global.typeCompletions$1(Global.scala:1229)
	at scala.tools.nsc.interactive.Global.completionsAt(Global.scala:1252)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:375)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:373)
	at scala.Option.map(Option.scala:146)
	at scala.meta.internal.pc.SignatureHelpProvider.treeSymbol(SignatureHelpProvider.scala:373)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCall$.unapply(SignatureHelpProvider.scala:180)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.visit(SignatureHelpProvider.scala:309)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.traverse(SignatureHelpProvider.scala:303)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.fromTree(SignatureHelpProvider.scala:272)
	at scala.meta.internal.pc.SignatureHelpProvider.signatureHelp(SignatureHelpProvider.scala:27)

[0m2021.03.02 16:10:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:10:08 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 16:15:10 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:10 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:15 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:15 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:18 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:18 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:22 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:22 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:29 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:29 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:33 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:36 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:36 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:42 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:42 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:45 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:45 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:45 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:45 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:47 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:47 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:51 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:51 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:54 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:54 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:59 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:15:59 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:16:05 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:16:05 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:16:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:16:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:16:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:16:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:21: stale bloop error: value parallelize is not a member of org.apache.spark.sql.SparkSession
    val stateList = spark.parallelize(rawStateList)
                    ^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 16:16:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:16:14 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 16:17:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:17:37 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 16:17:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:17:46 INFO  time: compiled root in 0.13s[0m
[0m2021.03.02 16:17:52 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:17:52 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:17:56 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:17:56 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:01 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:04 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:04 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:07 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:07 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:10 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:10 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:14 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:14 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:18 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:18 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:22 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:22 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:31 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:31 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:34 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:34 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:38 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:38 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:41 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:41 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:44 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:44 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:47 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:47 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:52 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:52 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:55 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:55 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:58 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:18:58 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:02 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:02 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:05 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:05 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:12 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:16 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:16 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:19 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:19 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:28 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:28 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:31 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:31 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:34 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:34 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:37 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:37 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:40 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:43 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:43 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:47 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:47 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:50 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:50 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:46:5: stale bloop error: ')' expected but 'val' found.
    val stateList = 
    ^[0m
[0m2021.03.02 16:19:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:19:51 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 16:20:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:20:22 INFO  time: compiled root in 0.13s[0m
[0m2021.03.02 16:20:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:20:49 INFO  time: compiled root in 0.24s[0m
[0m2021.03.02 16:21:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:21:31 INFO  time: compiled root in 1.45s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import org.apache.spark.annotation.InterfaceStability

/**
 * A container for a [[Dataset]], used for implicit conversions in Scala.
 *
 * To use this, import implicit conversions in SQL:
 * {{{
 *   val spark: SparkSession = ...
 *   import spark.implicits._
 * }}}
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
case class DatasetHolder[T] private[sql](private val ds: Dataset[T]) {

  // This is declared with parentheses to prevent the Scala compiler from treating
  // `rdd.toDS("1")` as invoking this toDS and then apply on the returned Dataset.
  def toDS(): Dataset[T] = ds

  // This is declared with parentheses to prevent the Scala compiler from treating
  // `rdd.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = ds.toDF()

  def toDF(colNames: String*): DataFrame = ds.toDF(colNames : _*)
}

[0m2021.03.02 16:21:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:21:50 INFO  time: compiled root in 1.27s[0m
[0m2021.03.02 16:21:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:22:00 INFO  time: compiled root in 1.3s[0m
[0m2021.03.02 16:28:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:28:34 INFO  time: compiled root in 0.33s[0m
[0m2021.03.02 16:29:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:29:43 INFO  time: compiled root in 0.3s[0m
[0m2021.03.02 16:29:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:29:48 INFO  time: compiled root in 0.32s[0m
[0m2021.03.02 16:31:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:31:54 INFO  time: compiled root in 0.32s[0m
[0m2021.03.02 16:32:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:32:18 INFO  time: compiled root in 0.23s[0m
[0m2021.03.02 16:32:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:32:18 INFO  time: compiled root in 0.23s[0m
[0m2021.03.02 16:32:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:32:21 INFO  time: compiled root in 0.27s[0m
[0m2021.03.02 16:34:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:46: stale bloop error: type mismatch;
 found   : Seq[(String, String)]
 required: org.apache.spark.sql.Dataset[_]
    val combinedCensusData = censusData.join(rawStateList)
                                             ^^^^^^^^^^^^[0m
[0m2021.03.02 16:34:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:46: stale bloop error: type mismatch;
 found   : Seq[(String, String)]
 required: org.apache.spark.sql.Dataset[_]
    val combinedCensusData = censusData.join(rawStateList)
                                             ^^^^^^^^^^^^[0m
[0m2021.03.02 16:34:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:46: stale bloop error: type mismatch;
 found   : Seq[(String, String)]
 required: org.apache.spark.sql.Dataset[_]
    val combinedCensusData = censusData.join(rawStateList)
                                             ^^^^^^^^^^^^[0m
[0m2021.03.02 16:34:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:46: stale bloop error: type mismatch;
 found   : Seq[(String, String)]
 required: org.apache.spark.sql.Dataset[_]
    val combinedCensusData = censusData.join(rawStateList)
                                             ^^^^^^^^^^^^[0m
Mar 02, 2021 4:34:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8547
[0m2021.03.02 16:34:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:46: stale bloop error: type mismatch;
 found   : Seq[(String, String)]
 required: org.apache.spark.sql.Dataset[_]
    val combinedCensusData = censusData.join(rawStateList)
                                             ^^^^^^^^^^^^[0m
[0m2021.03.02 16:34:09 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:49:46: stale bloop error: type mismatch;
 found   : Seq[(String, String)]
 required: org.apache.spark.sql.Dataset[_]
    val combinedCensusData = censusData.join(rawStateList)
                                             ^^^^^^^^^^^^[0m
[0m2021.03.02 16:34:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:34:10 INFO  time: compiled root in 0.32s[0m
[0m2021.03.02 16:35:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:35:35 INFO  time: compiled root in 0.3s[0m
[0m2021.03.02 16:38:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:38:12 INFO  time: compiled root in 1.33s[0m
[0m2021.03.02 16:40:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:40:20 INFO  time: compiled root in 1.6s[0m
[0m2021.03.02 16:41:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:41:21 INFO  time: compiled root in 1.31s[0m
[0m2021.03.02 16:48:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:48:31 INFO  time: compiled root in 1.31s[0m
[0m2021.03.02 16:53:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:53:20 INFO  time: compiled root in 1.21s[0m
[0m2021.03.02 16:54:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:54:22 INFO  time: compiled root in 1.19s[0m
[0m2021.03.02 16:54:23 INFO  shutting down Metals[0m
[0m2021.03.02 16:54:23 INFO  Shut down connection with build server.[0m
[0m2021.03.02 16:54:23 INFO  Shut down connection with build server.[0m
[0m2021.03.02 16:54:23 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.03 10:44:33 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.53.2.[0m
[0m2021.03.03 10:44:33 INFO  time: initialize in 0.47s[0m
[0m2021.03.03 10:44:34 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.03 10:44:33 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4062153829406880831/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.03 10:44:33 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.03 10:44:36 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    val stateList = rawStateList.toDF("State Code", "State Name")
    stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    combinedCensusData
      .select("State Name", "State Code", "Population Estimate Total")
      .show(51, false)
      
    // censusData
    //   .select("Geographic Area Name", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    // val commonCrawl = spark.sparkContext.textFile(
    //       "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    // )
    
    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

      // |-- url_surtkey: string (nullable = true)
      // |-- url: string (nullable = true)
      // |-- url_host_name: string (nullable = true)
      // |-- url_host_tld: string (nullable = true)
      // |-- url_host_2nd_last_part: string (nullable = true)
      // |-- url_host_3rd_last_part: string (nullable = true)
      // |-- url_host_4th_last_part: string (nullable = true)
      // |-- url_host_5th_last_part: string (nullable = true)
      // |-- url_host_registry_suffix: string (nullable = true)
      // |-- url_host_registered_domain: string (nullable = true)
      // |-- url_host_private_suffix: string (nullable = true)
      // |-- url_host_private_domain: string (nullable = true)
      // |-- url_protocol: string (nullable = true)
      // |-- url_port: integer (nullable = true)
      // |-- url_path: string (nullable = true)
      // |-- url_query: string (nullable = true)
      // |-- fetch_time: timestamp (nullable = true)
      // |-- fetch_status: short (nullable = true)
      // |-- content_digest: string (nullable = true)
      // |-- content_mime_type: string (nullable = true)
      // |-- content_mime_detected: string (nullable = true)
      // |-- warc_filename: string (nullable = true)
      // |-- warc_record_offset: integer (nullable = true)
      // |-- warc_record_length: integer (nullable = true)
      // |-- warc_segment: string (nullable = true)
      // |-- crawl: string (nullable = true)
      // |-- subset: string (nullable = true)
    
  }

  case class wet(
    warc: String, 
    warcTpye: String, 
    warcTargetUri: String, 
    warcDate: String, 
    warcRecordID: String,
    warcRefersTo: String,
    warcBlockDigest: String,
    warcIdentifiedContentLang: String,
    contentType: String, 
    contentLength: Int,
    content: String
    ) {}

}
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Loading previous analysis for 'root-test' from '/home/delaneylekien/project3/scalas3read/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/delaneylekien/project3/scalas3read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4062153829406880831/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4062153829406880831/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.03 10:44:38 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.03 10:44:38 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1135811092968473256/bsp.socket'...
[0m2021.03.03 10:44:38 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8086145923423754189/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1135811092968473256/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1135811092968473256/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.03 10:44:39 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8086145923423754189/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8086145923423754189/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.03 10:44:39 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.03 10:44:39 INFO  time: Connected to build server in 5.41s[0m
[0m2021.03.03 10:44:39 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.03 10:44:39 INFO  time: Imported build in 0.36s[0m
[0m2021.03.03 10:44:42 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.03 10:44:42 INFO  time: indexed workspace in 3.74s[0m
[0m2021.03.03 10:47:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:47:13 INFO  time: compiled root in 6.77s[0m
[0m2021.03.03 10:53:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:53:16 INFO  time: compiled root in 2.8s[0m
[0m2021.03.03 10:53:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:53:41 INFO  time: compiled root in 0.65s[0m
[0m2021.03.03 10:53:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:53:46 INFO  time: compiled root in 0.46s[0m
[0m2021.03.03 10:53:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:53:55 INFO  time: compiled root in 1.9s[0m
Mar 03, 2021 10:54:22 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 186
Mar 03, 2021 10:54:22 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 187
[0m2021.03.03 10:54:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 10:54:33 INFO  time: compiled root in 1.69s[0m
Mar 03, 2021 11:02:52 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 269
Mar 03, 2021 11:02:52 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 266
[0m2021.03.03 11:03:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:03:06 INFO  time: compiled root in 1.52s[0m
Mar 03, 2021 11:15:08 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 370
[0m2021.03.03 11:15:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:15:12 INFO  time: compiled root in 1.62s[0m
[0m2021.03.03 11:21:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:21:20 INFO  time: compiled root in 1.55s[0m
[0m2021.03.03 11:22:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:22:53 INFO  time: compiled root in 1.28s[0m
[0m2021.03.03 11:26:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:26:09 INFO  time: compiled root in 1.14s[0m
[0m2021.03.03 11:27:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:27:16 INFO  time: compiled root in 1.81s[0m
[0m2021.03.03 11:28:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:28:37 INFO  time: compiled root in 1.15s[0m
[0m2021.03.03 11:30:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:31:00 INFO  time: compiled root in 1.1s[0m
[0m2021.03.03 11:31:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:31:02 INFO  time: compiled root in 1.27s[0m
[0m2021.03.03 11:31:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:31:51 INFO  time: compiled root in 1.18s[0m
[0m2021.03.03 11:33:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:33:09 INFO  time: compiled root in 1.11s[0m
[0m2021.03.03 11:35:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:35:10 INFO  time: compiled root in 1.11s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.CharArrayWriter

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.commons.lang3.StringUtils

import org.apache.spark.TaskContext
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.api.java.function._
import org.apache.spark.api.python.{PythonRDD, SerDeUtil}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection
import org.apache.spark.sql.catalyst.json.{JacksonGenerator, JSONOptions}
import org.apache.spark.sql.catalyst.optimizer.CombineUnions
import org.apache.spark.sql.catalyst.parser.{ParseException, ParserUtils}
import org.apache.spark.sql.catalyst.plans._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, PartitioningCollection}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.arrow.{ArrowBatchStreamWriter, ArrowConverters}
import org.apache.spark.sql.execution.command._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.execution.python.EvaluatePython
import org.apache.spark.sql.execution.stat.StatFunctions
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.types._
import org.apache.spark.sql.util.SchemaUtils
import org.apache.spark.storage.StorageLevel
import org.apache.spark.unsafe.array.ByteArrayMethods
import org.apache.spark.unsafe.types.CalendarInterval
import org.apache.spark.util.Utils

private[sql] object Dataset {
  def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = {
    val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]])
    // Eagerly bind the encoder so we verify that the encoder matches the underlying
    // schema. The user will get an error if this is not the case.
    // optimization: it is guaranteed that [[InternalRow]] can be converted to [[Row]] so
    // do not do this check in that case. this check can be expensive since it requires running
    // the whole [[Analyzer]] to resolve the deserializer
    if (dataset.exprEnc.clsTag.runtimeClass != classOf[Row]) {
      dataset.deserializer
    }
    dataset
  }

  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {
    val qe = sparkSession.sessionState.executePlan(logicalPlan)
    qe.assertAnalyzed()
    new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))
  }
}

/**
 * A Dataset is a strongly typed collection of domain-specific objects that can be transformed
 * in parallel using functional or relational operations. Each Dataset also has an untyped view
 * called a `DataFrame`, which is a Dataset of [[Row]].
 *
 * Operations available on Datasets are divided into transformations and actions. Transformations
 * are the ones that produce new Datasets, and actions are the ones that trigger computation and
 * return results. Example transformations include map, filter, select, and aggregate (`groupBy`).
 * Example actions count, show, or writing data out to file systems.
 *
 * Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally,
 * a Dataset represents a logical plan that describes the computation required to produce the data.
 * When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
 * physical plan for efficient execution in a parallel and distributed manner. To explore the
 * logical plan as well as optimized physical plan, use the `explain` function.
 *
 * To efficiently support domain-specific objects, an [[Encoder]] is required. The encoder maps
 * the domain specific type `T` to Spark's internal type system. For example, given a class `Person`
 * with two fields, `name` (string) and `age` (int), an encoder is used to tell Spark to generate
 * code at runtime to serialize the `Person` object into a binary structure. This binary structure
 * often has much lower memory footprint as well as are optimized for efficiency in data processing
 * (e.g. in a columnar format). To understand the internal binary representation for data, use the
 * `schema` function.
 *
 * There are typically two ways to create a Dataset. The most common way is by pointing Spark
 * to some files on storage systems, using the `read` function available on a `SparkSession`.
 * {{{
 *   val people = spark.read.parquet("...").as[Person]  // Scala
 *   Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java
 * }}}
 *
 * Datasets can also be created through transformations available on existing Datasets. For example,
 * the following creates a new Dataset by applying a filter on the existing one:
 * {{{
 *   val names = people.map(_.name)  // in Scala; names is a Dataset[String]
 *   Dataset<String> names = people.map((Person p) -> p.name, Encoders.STRING));
 * }}}
 *
 * Dataset operations can also be untyped, through various domain-specific-language (DSL)
 * functions defined in: Dataset (this class), [[Column]], and [[functions]]. These operations
 * are very similar to the operations available in the data frame abstraction in R or Python.
 *
 * To select a column from the Dataset, use `apply` method in Scala and `col` in Java.
 * {{{
 *   val ageCol = people("age")  // in Scala
 *   Column ageCol = people.col("age"); // in Java
 * }}}
 *
 * Note that the [[Column]] type can also be manipulated through its various functions.
 * {{{
 *   // The following creates a new column that increases everybody's age by 10.
 *   people("age") + 10  // in Scala
 *   people.col("age").plus(10);  // in Java
 * }}}
 *
 * A more concrete example in Scala:
 * {{{
 *   // To create Dataset[Row] using SparkSession
 *   val people = spark.read.parquet("...")
 *   val department = spark.read.parquet("...")
 *
 *   people.filter("age > 30")
 *     .join(department, people("deptId") === department("id"))
 *     .groupBy(department("name"), people("gender"))
 *     .agg(avg(people("salary")), max(people("age")))
 * }}}
 *
 * and in Java:
 * {{{
 *   // To create Dataset<Row> using SparkSession
 *   Dataset<Row> people = spark.read().parquet("...");
 *   Dataset<Row> department = spark.read().parquet("...");
 *
 *   people.filter(people.col("age").gt(30))
 *     .join(department, people.col("deptId").equalTo(department.col("id")))
 *     .groupBy(department.col("name"), people.col("gender"))
 *     .agg(avg(people.col("salary")), max(people.col("age")));
 * }}}
 *
 * @groupname basic Basic Dataset functions
 * @groupname action Actions
 * @groupname untypedrel Untyped transformations
 * @groupname typedrel Typed transformations
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class Dataset[T] private[sql](
    @transient val sparkSession: SparkSession,
    @DeveloperApi @InterfaceStability.Unstable @transient val queryExecution: QueryExecution,
    encoder: Encoder[T])
  extends Serializable {

  queryExecution.assertAnalyzed()

  // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure
  // you wrap it with `withNewExecutionId` if this actions doesn't call other action.

  def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sparkSession, sparkSession.sessionState.executePlan(logicalPlan), encoder)
  }

  def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sqlContext.sparkSession, logicalPlan, encoder)
  }

  @transient private[sql] val logicalPlan: LogicalPlan = {
    // For various commands (like DDL) and queries with side effects, we force query execution
    // to happen right away to let these side effects take place eagerly.
    queryExecution.analyzed match {
      case c: Command =>
        LocalRelation(c.output, withAction("command", queryExecution)(_.executeCollect()))
      case u @ Union(children) if children.forall(_.isInstanceOf[Command]) =>
        LocalRelation(u.output, withAction("command", queryExecution)(_.executeCollect()))
      case _ =>
        queryExecution.analyzed
    }
  }

  /**
   * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the
   * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use
   * it when constructing new Dataset objects that have the same object type (that will be
   * possibly resolved to a different schema).
   */
  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)

  // The deserializer expression which can be used to build a projection and turn rows to objects
  // of type T, after collecting rows to the driver side.
  private lazy val deserializer =
    exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer).deserializer

  private implicit def classTag = exprEnc.clsTag

  // sqlContext must be val because a stable identifier is expected when you import implicits
  @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext

  private[sql] def resolve(colName: String): NamedExpression = {
    queryExecution.analyzed.resolveQuoted(colName, sparkSession.sessionState.analyzer.resolver)
      .getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
  }

  private[sql] def numericColumns: Seq[Expression] = {
    schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n =>
      queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get
    }
  }

  /**
   * Get rows represented in Sequence by specific truncate and vertical requirement.
   *
   * @param numRows Number of rows to return
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   */
  private[sql] def getRows(
      numRows: Int,
      truncate: Int): Seq[Seq[String]] = {
    val newDf = toDF()
    val castCols = newDf.logicalPlan.output.map { col =>
      // Since binary types in top-level schema fields have a specific format to print,
      // so we do not cast them to strings here.
      if (col.dataType == BinaryType) {
        Column(col)
      } else {
        Column(col).cast(StringType)
      }
    }
    val data = newDf.select(castCols: _*).take(numRows + 1)

    // For array values, replace Seq and Array with square brackets
    // For cells that are beyond `truncate` characters, replace it with the
    // first `truncate-3` and "..."
    schema.fieldNames.toSeq +: data.map { row =>
      row.toSeq.map { cell =>
        val str = cell match {
          case null => "null"
          case binary: Array[Byte] => binary.map("%02X".format(_)).mkString("[", " ", "]")
          case _ => cell.toString
        }
        if (truncate > 0 && str.length > truncate) {
          // do not show ellipses for strings shorter than 4 characters.
          if (truncate < 4) str.substring(0, truncate)
          else str.substring(0, truncate - 3) + "..."
        } else {
          str
        }
      }: Seq[String]
    }
  }

  /**
   * Compose the string representing rows for output
   *
   * @param _numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   */
  private[sql] def showString(
      _numRows: Int,
      truncate: Int = 20,
      vertical: Boolean = false): String = {
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data.
    val tmpRows = getRows(numRows, truncate)

    val hasMoreData = tmpRows.length - 1 > numRows
    val rows = tmpRows.take(numRows + 1)

    val sb = new StringBuilder
    val numCols = schema.fieldNames.length
    // We set a minimum column width at '3'
    val minimumColWidth = 3

    if (!vertical) {
      // Initialise the width of each column to a minimum value
      val colWidths = Array.fill(numCols)(minimumColWidth)

      // Compute the width of each column
      for (row <- rows) {
        for ((cell, i) <- row.zipWithIndex) {
          colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell))
        }
      }

      val paddedRows = rows.map { row =>
        row.zipWithIndex.map { case (cell, i) =>
          if (truncate > 0) {
            StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          } else {
            StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          }
        }
      }

      // Create SeparateLine
      val sep: String = colWidths.map("-" * _).addString(sb, "+", "+", "+\n").toString()

      // column names
      paddedRows.head.addString(sb, "|", "|", "|\n")
      sb.append(sep)

      // data
      paddedRows.tail.foreach(_.addString(sb, "|", "|", "|\n"))
      sb.append(sep)
    } else {
      // Extended display mode enabled
      val fieldNames = rows.head
      val dataRows = rows.tail

      // Compute the width of field name and data columns
      val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) =>
        math.max(curMax, Utils.stringHalfWidth(fieldName))
      }
      val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) =>
        math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max)
      }

      dataRows.zipWithIndex.foreach { case (row, i) =>
        // "+ 5" in size means a character length except for padded names and data
        val rowHeader = StringUtils.rightPad(
          s"-RECORD $i", fieldNameColWidth + dataColWidth + 5, "-")
        sb.append(rowHeader).append("\n")
        row.zipWithIndex.map { case (cell, j) =>
          val fieldName = StringUtils.rightPad(fieldNames(j),
            fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length)
          val data = StringUtils.rightPad(cell,
            dataColWidth - Utils.stringHalfWidth(cell) + cell.length)
          s" $fieldName | $data "
        }.addString(sb, "", "\n", "\n")
      }
    }

    // Print a footer
    if (vertical && rows.tail.isEmpty) {
      // In a vertical mode, print an empty row set explicitly
      sb.append("(0 rows)\n")
    } else if (hasMoreData) {
      // For Data that has more than "numRows" records
      val rowsString = if (numRows == 1) "row" else "rows"
      sb.append(s"only showing top $numRows $rowsString\n")
    }

    sb.toString()
  }

  override def toString: String = {
    try {
      val builder = new StringBuilder
      val fields = schema.take(2).map {
        case f => s"${f.name}: ${f.dataType.simpleString(2)}"
      }
      builder.append("[")
      builder.append(fields.mkString(", "))
      if (schema.length > 2) {
        if (schema.length - fields.size == 1) {
          builder.append(" ... 1 more field")
        } else {
          builder.append(" ... " + (schema.length - 2) + " more fields")
        }
      }
      builder.append("]").toString()
    } catch {
      case NonFatal(e) =>
        s"Invalid tree; ${e.getMessage}:\n$queryExecution"
    }
  }

  /**
   * Converts this strongly typed collection of data to generic Dataframe. In contrast to the
   * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]
   * objects that allow fields to be accessed by ordinal or name.
   *
   * @group basic
   * @since 1.6.0
   */
  // This is declared with parentheses to prevent the Scala compiler from treating
  // `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))

  /**
   * :: Experimental ::
   * Returns a new Dataset where each record has been mapped on to the specified type. The
   * method used to map columns depend on the type of `U`:
   *  - When `U` is a class, fields for the class will be mapped to columns of the same name
   *    (case sensitivity is determined by `spark.sql.caseSensitive`).
   *  - When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will
   *    be assigned to `_1`).
   *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the
   *    `DataFrame` will be used.
   *
   * If the schema of the Dataset does not match the desired `U` type, you can use `select`
   * along with `alias` or `as` to rearrange or rename as required.
   *
   * Note that `as[]` only changes the view of the data that is passed into typed operations,
   * such as `map()`, and does not eagerly project away any columns that are not present in
   * the specified class.
   *
   * @group basic
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan)

  /**
   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.
   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with
   * meaningful names. For example:
   * {{{
   *   val rdd: RDD[(Int, String)] = ...
   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`
   *   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
   * }}}
   *
   * @group basic
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def toDF(colNames: String*): DataFrame = {
    require(schema.size == colNames.size,
      "The number of columns doesn't match.\n" +
        s"Old column names (${schema.size}): " + schema.fields.map(_.name).mkString(", ") + "\n" +
        s"New column names (${colNames.size}): " + colNames.mkString(", "))

    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>
      Column(oldAttribute).as(newName)
    }
    select(newCols : _*)
  }

  /**
   * Returns the schema of this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def schema: StructType = queryExecution.analyzed.schema

  /**
   * Prints the schema to the console in a nice tree format.
   *
   * @group basic
   * @since 1.6.0
   */
  // scalastyle:off println
  def printSchema(): Unit = println(schema.treeString)
  // scalastyle:on println

  /**
   * Prints the plans (logical and physical) to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(extended: Boolean): Unit = {
    val explain = ExplainCommand(queryExecution.logical, extended = extended)
    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {
      // scalastyle:off println
      r => println(r.getString(0))
      // scalastyle:on println
    }
  }

  /**
   * Prints the physical plan to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(): Unit = explain(extended = false)

  /**
   * Returns all column names and their data types as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def dtypes: Array[(String, String)] = schema.fields.map { field =>
    (field.name, field.dataType.toString)
  }

  /**
   * Returns all column names as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def columns: Array[String] = schema.fields.map(_.name)

  /**
   * Returns true if the `collect` and `take` methods can be run locally
   * (without any Spark executors).
   *
   * @group basic
   * @since 1.6.0
   */
  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]

  /**
   * Returns true if the `Dataset` is empty.
   *
   * @group basic
   * @since 2.4.0
   */
  def isEmpty: Boolean = withAction("isEmpty", limit(1).groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0) == 0
  }

  /**
   * Returns true if this Dataset contains one or more sources that continuously
   * return data as it arrives. A Dataset that reads data from a streaming source
   * must be executed as a `StreamingQuery` using the `start()` method in
   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or
   * `collect()`, will throw an [[AnalysisException]] when there is a streaming
   * source present.
   *
   * @group streaming
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def isStreaming: Boolean = logicalPlan.isStreaming

  /**
   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true)

  /**
   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
   * logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true)

  /**
   * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be
   * used to truncate the logical plan of this Dataset, which is especially useful in iterative
   * algorithms where the plan may grow exponentially. Local checkpoints are written to executor
   * storage and despite potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false)

  /**
   * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. Local checkpoints are written to executor storage and despite
   * potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint(
    eager = eager,
    reliableCheckpoint = false
  )

  /**
   * Returns a checkpointed version of this Dataset.
   *
   * @param eager Whether to checkpoint this dataframe immediately
   * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the
   *                           checkpoint directory. If false creates a local checkpoint using
   *                           the caching subsystem
   */
  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = {
    val internalRdd = queryExecution.toRdd.map(_.copy())
    if (reliableCheckpoint) {
      internalRdd.checkpoint()
    } else {
      internalRdd.localCheckpoint()
    }

    if (eager) {
      internalRdd.count()
    }

    val physicalPlan = queryExecution.executedPlan

    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the
    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner
    // joins.
    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {
      partitioning match {
        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)
        case p => p
      }
    }

    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)

    Dataset.ofRows(
      sparkSession,
      LogicalRDD(
        logicalPlan.output,
        internalRdd,
        outputPartitioning,
        physicalPlan.outputOrdering,
        isStreaming
      )(sparkSession)).as[T]
  }

  /**
   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time
   * before which we assume no more late data is going to arrive.
   *
   * Spark will use this watermark for several purposes:
   *  - To know when a given time window aggregation can be finalized and thus can be emitted when
   *    using output modes that do not allow updates.
   *  - To minimize the amount of state that we need to keep for on-going aggregations,
   *    `mapGroupsWithState` and `dropDuplicates` operators.
   *
   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across
   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost
   *  of coordinating this value across partitions, the actual watermark used is only guaranteed
   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still
   *  process records that arrive more than `delayThreshold` late.
   *
   * @param eventTime the name of the column that contains the event time of the row.
   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest
   *                       record that has been processed in the form of an interval
   *                       (e.g. "1 minute" or "5 hours"). NOTE: This should not be negative.
   *
   * @group streaming
   * @since 2.1.0
   */
  @InterfaceStability.Evolving
  // We only accept an existing column name, not a derived column here as a watermark that is
  // defined on a derived column cannot referenced elsewhere in the plan.
  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {
    val parsedDelay =
      try {
        CalendarInterval.fromCaseInsensitiveString(delayThreshold)
      } catch {
        case e: IllegalArgumentException =>
          throw new AnalysisException(
            s"Unable to parse time delay '$delayThreshold'",
            cause = Some(e))
      }
    require(parsedDelay.milliseconds >= 0 && parsedDelay.months >= 0,
      s"delay threshold ($delayThreshold) should not be negative.")
    EliminateEventTimeWatermark(
      EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan))
  }

  /**
   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
   * and all cells will be aligned right. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   *
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int): Unit = show(numRows, truncate = true)

  /**
   * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters
   * will be truncated, and all cells will be aligned right.
   *
   * @group action
   * @since 1.6.0
   */
  def show(): Unit = show(20)

  /**
   * Displays the top 20 rows of Dataset in a tabular form.
   *
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *                 be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  def show(truncate: Boolean): Unit = show(20, truncate)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   * @param numRows Number of rows to show
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *              be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {
    println(showString(numRows, truncate = 20))
  } else {
    println(showString(numRows, truncate = 0))
  }

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * If `vertical` enabled, this command prints output rows vertically (one line per column value)?
   *
   * {{{
   * -RECORD 0-------------------
   *  year            | 1980
   *  month           | 12
   *  AVG('Adj Close) | 0.503218
   *  AVG('Adj Close) | 0.595103
   * -RECORD 1-------------------
   *  year            | 1981
   *  month           | 01
   *  AVG('Adj Close) | 0.523289
   *  AVG('Adj Close) | 0.570307
   * -RECORD 2-------------------
   *  year            | 1982
   *  month           | 02
   *  AVG('Adj Close) | 0.436504
   *  AVG('Adj Close) | 0.475256
   * -RECORD 3-------------------
   *  year            | 1983
   *  month           | 03
   *  AVG('Adj Close) | 0.410516
   *  AVG('Adj Close) | 0.442194
   * -RECORD 4-------------------
   *  year            | 1984
   *  month           | 04
   *  AVG('Adj Close) | 0.450090
   *  AVG('Adj Close) | 0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   * @group action
   * @since 2.3.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Int, vertical: Boolean): Unit =
    println(showString(numRows, truncate, vertical))
  // scalastyle:on println

  /**
   * Returns a [[DataFrameNaFunctions]] for working with missing data.
   * {{{
   *   // Dropping rows containing any null values.
   *   ds.na.drop()
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())

  /**
   * Returns a [[DataFrameStatFunctions]] for working statistic functions support.
   * {{{
   *   // Finding frequent items in column with name 'a'.
   *   ds.stat.freqItems(Seq("a"))
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())

  /**
   * Join with another `DataFrame`.
   *
   * Behaves as an INNER JOIN and requires a subsequent join predicate.
   *
   * @param right Right side of the join operation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)
  }

  /**
   * Inner equi-join with another `DataFrame` using the given column.
   *
   * Different from other join functions, the join column will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the column "user_id"
   *   df1.join(df2, "user_id")
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumn Name of the column to join on. This column must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumn: String): DataFrame = {
    join(right, Seq(usingColumn))
  }

  /**
   * Inner equi-join with another `DataFrame` using the given columns.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the columns "user_id" and "user_name"
   *   df1.join(df2, Seq("user_id", "user_name"))
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {
    join(right, usingColumns, "inner")
  }

  /**
   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate
   * is specified as an inner join. If you would explicitly like to perform a cross join use the
   * `crossJoin` method.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {
    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right
    // by creating a new instance for one of the branch.
    val joined = sparkSession.sessionState.executePlan(
      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))
      .analyzed.asInstanceOf[Join]

    withPlan {
      Join(
        joined.left,
        joined.right,
        UsingJoin(JoinType(joinType), usingColumns),
        None)
    }
  }

  /**
   * Inner join with another `DataFrame`, using the given join expression.
   *
   * {{{
   *   // The following two are equivalent:
   *   df1.join(df2, $"df1Key" === $"df2Key")
   *   df1.join(df2).where($"df1Key" === $"df2Key")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, "inner")

  /**
   * Join with another `DataFrame`, using the given join expression. The following performs
   * a full outer join between `df1` and `df2`.
   *
   * {{{
   *   // Scala:
   *   import org.apache.spark.sql.functions._
   *   df1.join(df2, $"df1Key" === $"df2Key", "outer")
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
   * }}}
   *
   * @param right Right side of the join.
   * @param joinExprs Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {
    // Note that in this function, we introduce a hack in the case of self-join to automatically
    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].
    // Consider this case: df.join(df, df("key") === df("key"))
    // Since df("key") === df("key") is a trivially true condition, this actually becomes a
    // cartesian join. However, most likely users expect to perform a self join using "key".
    // With that assumption, this hack turns the trivially true condition into equality on join
    // keys that are resolved to both sides.

    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.
    // After the cloning, left and right side will have distinct expression ids.
    val plan = withPlan(
      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))
      .queryExecution.analyzed.asInstanceOf[Join]

    // If auto self join alias is disabled, return the plan.
    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {
      return withPlan(plan)
    }

    // If left/right have no output set intersection, return the plan.
    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed
    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed
    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {
      return withPlan(plan)
    }

    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.
    // By the time we get here, since we have already run analysis, all attributes should've been
    // resolved and become AttributeReference.
    val cond = plan.condition.map { _.transform {
      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)
          if a.sameRef(b) =>
        catalyst.expressions.EqualTo(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
      case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference)
        if a.sameRef(b) =>
        catalyst.expressions.EqualNullSafe(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
    }}

    withPlan {
      plan.copy(condition = cond)
    }
  }

  /**
   * Explicit cartesian join with another `DataFrame`.
   *
   * @param right Right side of the join operation.
   *
   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.
   *
   * @group untypedrel
   * @since 2.1.0
   */
  def crossJoin(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)
  }

  /**
   * :: Experimental ::
   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to
   * true.
   *
   * This is similar to the relation `join` function with one important difference in the
   * result schema. Since `joinWith` preserves objects present on either side of the join, the
   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.
   *
   * This type of join can be useful both for preserving type-safety with the original object
   * types as well as working with relational data where either side of the join has column
   * names in common.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {
    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,
    // etc.
    val joined = sparkSession.sessionState.executePlan(
      Join(
        this.logicalPlan,
        other.logicalPlan,
        JoinType(joinType),
        Some(condition.expr))).analyzed.asInstanceOf[Join]

    if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) {
      throw new AnalysisException("Invalid join type in joinWith: " + joined.joinType.sql)
    }

    // For both join side, combine all outputs into a single column and alias it with "_1" or "_2",
    // to match the schema for the encoder of the join result.
    // Note that we do this before joining them, to enable the join operator to return null for one
    // side, in cases like outer-join.
    val left = {
      val combined = if (this.exprEnc.flat) {
        assert(joined.left.output.length == 1)
        Alias(joined.left.output.head, "_1")()
      } else {
        Alias(CreateStruct(joined.left.output), "_1")()
      }
      Project(combined :: Nil, joined.left)
    }

    val right = {
      val combined = if (other.exprEnc.flat) {
        assert(joined.right.output.length == 1)
        Alias(joined.right.output.head, "_2")()
      } else {
        Alias(CreateStruct(joined.right.output), "_2")()
      }
      Project(combined :: Nil, joined.right)
    }

    // Rewrites the join condition to make the attribute point to correct column/field, after we
    // combine the outputs of each join side.
    val conditionExpr = joined.condition.get transformUp {
      case a: Attribute if joined.left.outputSet.contains(a) =>
        if (this.exprEnc.flat) {
          left.output.head
        } else {
          val index = joined.left.output.indexWhere(_.exprId == a.exprId)
          GetStructField(left.output.head, index)
        }
      case a: Attribute if joined.right.outputSet.contains(a) =>
        if (other.exprEnc.flat) {
          right.output.head
        } else {
          val index = joined.right.output.indexWhere(_.exprId == a.exprId)
          GetStructField(right.output.head, index)
        }
    }

    implicit val tuple2Encoder: Encoder[(T, U)] =
      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)

    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))
  }

  /**
   * :: Experimental ::
   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair
   * where `condition` evaluates to true.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {
    joinWith(other, condition, "inner")
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {
    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = false, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the specified column, all in ascending order.
   * {{{
   *   // The following 3 are equivalent
   *   ds.sort("sortcol")
   *   ds.sort($"sortcol")
   *   ds.sort($"sortcol".asc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortCol: String, sortCols: String*): Dataset[T] = {
    sort((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset sorted by the given expressions. For example:
   * {{{
   *   ds.sort($"col1", $"col2".desc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = true, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def apply(colName: String): Column = col(colName)

  /**
   * Specifies some hint on the current Dataset. As an example, the following code specifies
   * that one of the plan can be broadcasted:
   *
   * {{{
   *   df1.join(df2.hint("broadcast"))
   * }}}
   *
   * @group basic
   * @since 2.2.0
   */
  @scala.annotation.varargs
  def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan {
    UnresolvedHint(name, parameters, logicalPlan)
  }

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def col(colName: String): Column = colName match {
    case "*" =>
      Column(ResolvedStar(queryExecution.analyzed.output))
    case _ =>
      if (sqlContext.conf.supportQuotedRegexColumnName) {
        colRegex(colName)
      } else {
        val expr = resolve(colName)
        Column(expr)
      }
  }

  /**
   * Selects column based on the column name specified as a regex and returns it as [[Column]].
   * @group untypedrel
   * @since 2.3.0
   */
  def colRegex(colName: String): Column = {
    val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis
    colName match {
      case ParserUtils.escapedIdentifier(columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, None, caseSensitive))
      case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive))
      case _ =>
        Column(resolve(colName))
    }
  }

  /**
   * Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def as(alias: String): Dataset[T] = withTypedPlan {
    SubqueryAlias(alias, logicalPlan)
  }

  /**
   * (Scala-specific) Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def as(alias: Symbol): Dataset[T] = as(alias.name)

  /**
   * Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: String): Dataset[T] = as(alias)

  /**
   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: Symbol): Dataset[T] = as(alias)

  /**
   * Selects a set of column based expressions.
   * {{{
   *   ds.select($"colA", $"colB" + 1)
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(cols: Column*): DataFrame = withPlan {
    Project(cols.map(_.named), logicalPlan)
  }

  /**
   * Selects a set of columns. This is a variant of `select` that can only select
   * existing columns using column names (i.e. cannot construct expressions).
   *
   * {{{
   *   // The following two are equivalent:
   *   ds.select("colA", "colB")
   *   ds.select($"colA", $"colB")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)

  /**
   * Selects a set of SQL expressions. This is a variant of `select` that accepts
   * SQL expressions.
   *
   * {{{
   *   // The following are equivalent:
   *   ds.selectExpr("colA", "colB as newName", "abs(colC)")
   *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def selectExpr(exprs: String*): DataFrame = {
    select(exprs.map { expr =>
      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))
    }: _*)
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expression for each element.
   *
   * {{{
   *   val ds = Seq(1, 2, 3).toDS()
   *   val newDS = ds.select(expr("value + 1").as[Int])
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {
    implicit val encoder = c1.encoder
    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan)

    if (encoder.flat) {
      new Dataset[U1](sparkSession, project, encoder)
    } else {
      // Flattens inner fields of U1
      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)
    }
  }

  /**
   * Internal helper function for building typed selects that return tuples. For simplicity and
   * code reuse, we do this without the help of the type system and then use helper functions
   * that cast appropriately for the user facing interface.
   */
  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {
    val encoders = columns.map(_.encoder)
    val namedColumns =
      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)
    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))
    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =
    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =
    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =
    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4, U5](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4],
      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =
    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]

  /**
   * Filters rows using the given condition.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(condition: Column): Dataset[T] = withTypedPlan {
    Filter(condition.expr, logicalPlan)
  }

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.filter("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Filters rows using the given condition. This is an alias for `filter`.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(condition: Column): Dataset[T] = filter(condition)

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.where("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Groups the Dataset using the specified columns, so we can run aggregation on them. See
   * [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy($"department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)
  }

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)
  }

  /**
   * Groups the Dataset using the specified columns, so that we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of groupBy that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy("department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: (T, T) => T): T = withNewRDDExecutionId {
    rdd.reduce(func)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {
    val withGroupingKey = AppendColumns(func, logicalPlan)
    val executed = sparkSession.sessionState.executePlan(withGroupingKey)

    new KeyValueGroupedDataset(
      encoderFor[K],
      encoderFor[T],
      executed,
      logicalPlan.output,
      withGroupingKey.newColumns)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =
    groupByKey(func.call(_))(encoder)

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of rollup that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup("department", "group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of cube that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube("department", "group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg("age" -> "max", "salary" -> "avg")
   *   ds.groupBy().agg("age" -> "max", "salary" -> "avg")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
    groupBy().agg(aggExpr, aggExprs : _*)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * (Java-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(max($"age"), avg($"salary"))
   *   ds.groupBy().agg(max($"age"), avg($"salary"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)

  /**
   * Returns a new Dataset by taking the first `n` rows. The difference between this function
   * and `head` is that `head` is an action and returns an array (by triggering query execution)
   * while `limit` returns a new Dataset.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def limit(n: Int): Dataset[T] = withTypedPlan {
    Limit(Literal(n), logicalPlan)
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @deprecated("use union()", "2.0.0")
  def unionAll(other: Dataset[T]): Dataset[T] = union(other)

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.union(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   4|   5|   6|
   *   // +----+----+----+
   * }}}
   *
   * Notice that the column positions in the schema aren't necessarily matched with the
   * fields in the strongly typed objects in a Dataset. This function resolves columns
   * by their positions in the schema, not the fields in the strongly typed objects. Use
   * [[unionByName]] to resolve columns by field name in the typed objects.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def union(other: Dataset[T]): Dataset[T] = withSetOperator {
    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, other.logicalPlan))
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set
   * union (that does deduplication of elements), use this function followed by a [[distinct]].
   *
   * The difference between this function and [[union]] is that this function
   * resolves columns by name (not by position):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.unionByName(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   6|   4|   5|
   *   // +----+----+----+
   * }}}
   *
   * @group typedrel
   * @since 2.3.0
   */
  def unionByName(other: Dataset[T]): Dataset[T] = withSetOperator {
    // Check column name duplication
    val resolver = sparkSession.sessionState.analyzer.resolver
    val leftOutputAttrs = logicalPlan.output
    val rightOutputAttrs = other.logicalPlan.output

    SchemaUtils.checkColumnNameDuplication(
      leftOutputAttrs.map(_.name),
      "in the left attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)
    SchemaUtils.checkColumnNameDuplication(
      rightOutputAttrs.map(_.name),
      "in the right attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    // Builds a project list for `other` based on `logicalPlan` output names
    val rightProjectList = leftOutputAttrs.map { lattr =>
      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "${lattr.name}" among """ +
            s"""(${rightOutputAttrs.map(_.name).mkString(", ")})""")
      }
    }

    // Delegates failure checks to `CheckAnalysis`
    val notFoundAttrs = rightOutputAttrs.diff(rightProjectList)
    val rightChild = Project(rightProjectList ++ notFoundAttrs, other.logicalPlan)

    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, rightChild))
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.
   * This is equivalent to `INTERSECT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset while
   * preserving the duplicates.
   * This is equivalent to `INTERSECT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard
   * in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = true)
  }


  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.
   * This is equivalent to `EXCEPT DISTINCT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def except(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset while
   * preserving the duplicates.
   * This is equivalent to `EXCEPT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in
   * SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = true)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a user-supplied seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double, seed: Long): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction, seed = seed)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a random seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {
    withTypedPlan {
      Sample(0.0, fraction, withReplacement, seed, logicalPlan)
    }
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the total count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {
    sample(withReplacement, fraction, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * For Java API, use [[randomSplitAsList]].
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its
    // constituent partitions each time a split is materialized which could result in
    // overlapping splits. To prevent this, we explicitly sort each input partition to make the
    // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out
    // from the sort order.
    val sortOrder = logicalPlan.output
      .filter(attr => RowOrdering.isOrderable(attr.dataType))
      .map(SortOrder(_, Ascending))
    val plan = if (sortOrder.nonEmpty) {
      Sort(sortOrder, global = false, logicalPlan)
    } else {
      // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism
      cache()
      logicalPlan
    }
    val sum = weights.sum
    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
    normalizedCumWeights.sliding(2).map { x =>
      new Dataset[T](
        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder)
    }.toArray
  }

  /**
   * Returns a Java list that contains randomly split Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {
    val values = randomSplit(weights, seed)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {
    randomSplit(weights, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   */
  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {
    randomSplit(weights.toArray, seed)
  }

  /**
   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of
   * the input row are implicitly joined with each row that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count
   * the number of books that contain a given word:
   *
   * {{{
   *   case class Book(title: String, words: String)
   *   val ds: Dataset[Book]
   *
   *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))
   *
   *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
   * }}}
   *
   * Using `flatMap()` this can similarly be exploded as:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {
    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]

    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)

    val rowFunction =
      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))
    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero
   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All
   * columns of the input row are implicitly joined with each value that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()`:
   *
   * {{{
   *   ds.select(explode(split('words, " ")).as("word"))
   * }}}
   *
   * or `flatMap()`:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])
    : DataFrame = {
    val dataType = ScalaReflection.schemaFor[B].dataType
    val attributes = AttributeReference(outputColumn, dataType)() :: Nil
    // TODO handle the metadata?
    val elementSchema = attributes.toStructType

    def rowFunction(row: Row): TraversableOnce[InternalRow] = {
      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)
      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))
    }
    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * Returns a new Dataset by adding a column or replacing the existing column that has
   * the same name.
   *
   * `column`'s expression must only refer to attributes supplied by this Dataset. It is an
   * error to add a column that refers to some other Dataset.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col))

  /**
   * Returns a new Dataset by adding columns or replacing the existing columns that has
   * the same names.
   */
  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
    require(colNames.size == cols.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of columns: ${cols.size}")
    SchemaUtils.checkColumnNameDuplication(
      colNames,
      "in given column names",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output

    val columnMap = colNames.zip(cols).toMap

    val replacedAndExistingColumns = output.map { field =>
      columnMap.find { case (colName, _) =>
        resolver(field.name, colName)
      } match {
        case Some((colName: String, col: Column)) => col.as(colName)
        case _ => Column(field)
      }
    }

    val newColumns = columnMap.filter { case (colName, col) =>
      !output.exists(f => resolver(f.name, colName))
    }.map { case (colName, col) => col.as(colName) }

    select(replacedAndExistingColumns ++ newColumns : _*)
  }

  /**
   * Returns a new Dataset by adding columns with metadata.
   */
  private[spark] def withColumns(
      colNames: Seq[String],
      cols: Seq[Column],
      metadata: Seq[Metadata]): DataFrame = {
    require(colNames.size == metadata.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of metadata elements: ${metadata.size}")
    val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) =>
      col.as(colName, metadata)
    }
    withColumns(colNames, newCols)
  }

  /**
   * Returns a new Dataset by adding a column with metadata.
   */
  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame =
    withColumns(Seq(colName), Seq(col), Seq(metadata))

  /**
   * Returns a new Dataset with a column renamed.
   * This is a no-op if schema doesn't contain existingName.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumnRenamed(existingName: String, newName: String): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output
    val shouldRename = output.exists(f => resolver(f.name, existingName))
    if (shouldRename) {
      val columns = output.map { col =>
        if (resolver(col.name, existingName)) {
          Column(col).as(newName)
        } else {
          Column(col)
        }
      }
      select(columns : _*)
    } else {
      toDF()
    }
  }

  /**
   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain
   * column name.
   *
   * This method can only be used to drop top level columns. the colName string is treated
   * literally without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(colName: String): DataFrame = {
    drop(Seq(colName) : _*)
  }

  /**
   * Returns a new Dataset with columns dropped.
   * This is a no-op if schema doesn't contain column name(s).
   *
   * This method can only be used to drop top level columns. the colName string is treated literally
   * without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def drop(colNames: String*): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val remainingCols = allColumns.filter { attribute =>
      colNames.forall(n => !resolver(attribute.name, n))
    }.map(attribute => Column(attribute))
    if (remainingCols.size == allColumns.size) {
      toDF()
    } else {
      this.select(remainingCols: _*)
    }
  }

  /**
   * Returns a new Dataset with a column dropped.
   * This version of drop accepts a [[Column]] rather than a name.
   * This is a no-op if the Dataset doesn't have a column
   * with an equivalent expression.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(col: Column): DataFrame = {
    val expression = col match {
      case Column(u: UnresolvedAttribute) =>
        queryExecution.analyzed.resolveQuoted(
          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)
      case Column(expr: Expression) => expr
    }
    val attrs = this.logicalPlan.output
    val colsAfterDrop = attrs.filter { attr =>
      attr != expression
    }.map(attr => Column(attr))
    select(colsAfterDrop : _*)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `distinct`.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)

  /**
   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>
      // It is possibly there are more than one columns with the same name,
      // so we call filter instead of find.
      val cols = allColumns.filter(col => resolver(col.name, colName))
      if (cols.isEmpty) {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
      cols
    }
    Deduplicate(groupCols, logicalPlan)
  }

  /**
   * Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)

  /**
   * Returns a new [[Dataset]] with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {
    val colNames: Seq[String] = col1 +: cols
    dropDuplicates(colNames)
  }

  /**
   * Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
   * and max. If no columns are given, this function computes statistics for all numerical or
   * string columns.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.describe("age", "height").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // max     92.0  192.0
   * }}}
   *
   * Use [[summary]] for expanded statistics and control over which statistics to compute.
   *
   * @param cols Columns to compute statistics on.
   *
   * @group action
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def describe(cols: String*): DataFrame = {
    val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*)
    selected.summary("count", "mean", "stddev", "min", "max")
  }

  /**
   * Computes specified statistics for numeric and string columns. Available statistics are:
   *
   * - count
   * - mean
   * - stddev
   * - min
   * - max
   * - arbitrary approximate percentiles specified as a percentage (eg, 75%)
   *
   * If no statistics are given, this function computes count, mean, stddev, min,
   * approximate quartiles (percentiles at 25%, 50%, and 75%), and max.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.summary().show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 50%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * {{{
   *   ds.summary("count", "min", "25%", "75%", "max").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * To do a summary for specific columns first select them:
   *
   * {{{
   *   ds.select("age", "height").summary().show()
   * }}}
   *
   * See also [[describe]] for basic statistics.
   *
   * @param statistics Statistics from above list to be computed.
   *
   * @group action
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq)

  /**
   * Returns the first `n` rows.
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @group action
   * @since 1.6.0
   */
  def head(n: Int): Array[T] = withAction("head", limit(n).queryExecution)(collectFromPlan)

  /**
   * Returns the first row.
   * @group action
   * @since 1.6.0
   */
  def head(): T = head(1).head

  /**
   * Returns the first row. Alias for head().
   * @group action
   * @since 1.6.0
   */
  def first(): T = head()

  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: Dataset[T]): Dataset[U] = ...
   *
   *   ds
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: T => Boolean): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: FilterFunction[T]): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {
    MapElements[T, U](func, logicalPlan)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    implicit val uEnc = encoder
    withTypedPlan(MapElements[T, U](func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {
    new Dataset[U](
      sparkSession,
      MapPartitions[T, U](func, logicalPlan),
      implicitly[Encoder[U]])
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `f` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala
    mapPartitions(func)(encoder)
  }

  /**
   * Returns a new `DataFrame` that contains the result of applying a serialized R function
   * `func` to each partition.
   */
  private[sql] def mapPartitionsInR(
      func: Array[Byte],
      packageNames: Array[Byte],
      broadcastVars: Array[Broadcast[Object]],
      schema: StructType): DataFrame = {
    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]
    Dataset.ofRows(
      sparkSession,
      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =
    mapPartitions(_.flatMap(func))

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (T) => Iterator[U] = x => f.call(x).asScala
    flatMap(func)(encoder)
  }

  /**
   * Applies a function `f` to all rows.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(f: T => Unit): Unit = withNewRDDExecutionId {
    rdd.foreach(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each element of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))

  /**
   * Applies a function `f` to each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId {
    rdd.foreachPartition(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = {
    foreachPartition((it: Iterator[T]) => func.call(it.asJava))
  }

  /**
   * Returns the first `n` rows in the Dataset.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def take(n: Int): Array[T] = head(n)

  /**
   * Returns the first `n` rows in the Dataset as a list.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)

  /**
   * Returns an array that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * For Java API, use [[collectAsList]].
   *
   * @group action
   * @since 1.6.0
   */
  def collect(): Array[T] = withAction("collect", queryExecution)(collectFromPlan)

  /**
   * Returns a Java list that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def collectAsList(): java.util.List[T] = withAction("collectAsList", queryExecution) { plan =>
    val values = collectFromPlan(plan)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Returns an iterator that contains all rows in this Dataset.
   *
   * The iterator will consume as much memory as the largest partition in this Dataset.
   *
   * @note this results in multiple Spark jobs, and if the input Dataset is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input Dataset should be cached first.
   *
   * @group action
   * @since 2.0.0
   */
  def toLocalIterator(): java.util.Iterator[T] = {
    withAction("toLocalIterator", queryExecution) { plan =>
      // This projection writes output to a `InternalRow`, which means applying this projection is
      // not thread-safe. Here we create the projection inside this method to make `Dataset`
      // thread-safe.
      val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
      plan.executeToIterator().map { row =>
        // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
        // parameter of its `get` method, so it's safe to use null here.
        objProj(row).get(0, null).asInstanceOf[T]
      }.asJava
    }
  }

  /**
   * Returns the number of rows in the Dataset.
   * @group action
   * @since 1.6.0
   */
  def count(): Long = withAction("count", groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = true, logicalPlan)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments.
    // However, we don't want to complicate the semantics of this API method.
    // Instead, let's give users a friendly error message, pointing them to the new method.
    val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder])
    if (sortOrders.nonEmpty) throw new IllegalArgumentException(
      s"""Invalid partitionExprs specified: $sortOrders
         |For range partitioning use repartitionByRange(...) instead.
       """.stripMargin)
    withTypedPlan {
      RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(partitionExprs: Column*): Dataset[T] = {
    repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    require(partitionExprs.nonEmpty, "At least one partition-by expression must be specified.")
    val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match {
      case expr: SortOrder => expr
      case expr: Expression => SortOrder(expr, Ascending)
    })
    withTypedPlan {
      RepartitionByExpression(sortOrder, logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(partitionExprs: Column*): Dataset[T] = {
    repartitionByRange(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions
   * are requested. If a larger number of partitions is requested, it will stay at the current
   * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in
   * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not
   * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can call repartition. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @group typedrel
   * @since 1.6.0
   */
  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = false, logicalPlan)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `dropDuplicates`.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def distinct(): Dataset[T] = dropDuplicates()

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this)
    this
  }

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def cache(): this.type = persist()

  /**
   * Persist this Dataset with the given storage level.
   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,
   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,
   *                 `MEMORY_AND_DISK_2`, etc.
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(newLevel: StorageLevel): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)
    this
  }

  /**
   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
   *
   * @group basic
   * @since 2.1.0
   */
  def storageLevel: StorageLevel = {
    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>
      cachedData.cachedRepresentation.cacheBuilder.storageLevel
    }.getOrElse(StorageLevel.NONE)
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @param blocking Whether to block until all blocks are deleted.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(blocking: Boolean): this.type = {
    sparkSession.sharedState.cacheManager.uncacheQuery(this, cascade = false, blocking)
    this
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(): this.type = unpersist(blocking = false)

  // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`.
  @transient private lazy val rddQueryExecution: QueryExecution = {
    val deserialized = CatalystSerde.deserialize[T](logicalPlan)
    sparkSession.sessionState.executePlan(deserialized)
  }

  /**
   * Represents the content of the Dataset as an `RDD` of `T`.
   *
   * @group basic
   * @since 1.6.0
   */
  lazy val rdd: RDD[T] = {
    val objectType = exprEnc.deserializer.dataType
    rddQueryExecution.toRdd.mapPartitions { rows =>
      rows.map(_.get(0, objectType).asInstanceOf[T])
    }
  }

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def javaRDD: JavaRDD[T] = toJavaRDD

  /**
   * Registers this Dataset as a temporary table using the given name. The lifetime of this
   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  @deprecated("Use createOrReplaceTempView(viewName) instead.", "2.0.0")
  def registerTempTable(tableName: String): Unit = {
    createOrReplaceTempView(tableName)
  }

  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
   * created it, i.e. it will be automatically dropped when the session terminates. It's not
   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.0.0
   */
  @throws[AnalysisException]
  def createTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = false)
  }



  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 2.0.0
   */
  def createOrReplaceTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = false)
  }

  /**
   * Creates a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.1.0
   */
  @throws[AnalysisException]
  def createGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = true)
  }

  /**
   * Creates or replaces a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @group basic
   * @since 2.2.0
   */
  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = true)
  }

  private def createTempViewCommand(
      viewName: String,
      replace: Boolean,
      global: Boolean): CreateViewCommand = {
    val viewType = if (global) GlobalTempView else LocalTempView

    val tableIdentifier = try {
      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)
    } catch {
      case _: ParseException => throw new AnalysisException(s"Invalid view name: $viewName")
    }
    CreateViewCommand(
      name = tableIdentifier,
      userSpecifiedColumns = Nil,
      comment = None,
      properties = Map.empty,
      originalText = None,
      child = logicalPlan,
      allowExisting = false,
      replace = replace,
      viewType = viewType)
  }

  /**
   * Interface for saving the content of the non-streaming Dataset out into external storage.
   *
   * @group basic
   * @since 1.6.0
   */
  def write: DataFrameWriter[T] = {
    if (isStreaming) {
      logicalPlan.failAnalysis(
        "'write' can not be called on streaming Dataset/DataFrame")
    }
    new DataFrameWriter[T](this)
  }

  /**
   * Interface for saving the content of the streaming Dataset out into external storage.
   *
   * @group basic
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def writeStream: DataStreamWriter[T] = {
    if (!isStreaming) {
      logicalPlan.failAnalysis(
        "'writeStream' can be called only on streaming Dataset/DataFrame")
    }
    new DataStreamWriter[T](this)
  }


  /**
   * Returns the content of the Dataset as a Dataset of JSON strings.
   * @since 2.0.0
   */
  def toJSON: Dataset[String] = {
    val rowSchema = this.schema
    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone
    mapPartitions { iter =>
      val writer = new CharArrayWriter()
      // create the Generator without separator inserted between 2 records
      val gen = new JacksonGenerator(rowSchema, writer,
        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))

      new Iterator[String] {
        override def hasNext: Boolean = iter.hasNext
        override def next(): String = {
          gen.write(exprEnc.toRow(iter.next()))
          gen.flush()

          val json = writer.toString
          if (hasNext) {
            writer.reset()
          } else {
            gen.close()
          }

          json
        }
      }
    } (Encoders.STRING)
  }

  /**
   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply
   * asks each constituent BaseRelation for its respective files and takes the union of all results.
   * Depending on the source relations, this may not find all input files. Duplicates are removed.
   *
   * @group basic
   * @since 2.0.0
   */
  def inputFiles: Array[String] = {
    val files: Seq[String] = queryExecution.optimizedPlan.collect {
      case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) =>
        fsBasedRelation.inputFiles
      case fr: FileRelation =>
        fr.inputFiles
      case r: HiveTableRelation =>
        r.tableMeta.storage.locationUri.map(_.toString).toArray
    }.flatten
    files.toSet.toArray
  }

  ////////////////////////////////////////////////////////////////////////////
  // For Python API
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Converts a JavaRDD to a PythonRDD.
   */
  private[sql] def javaToPython: JavaRDD[Array[Byte]] = {
    val structType = schema  // capture it for closure
    val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType))
    EvaluatePython.javaToPython(rdd)
  }

  private[sql] def collectToPython(): Array[Any] = {
    EvaluatePython.registerPicklers()
    withAction("collectToPython", queryExecution) { plan =>
      val toJava: (Any) => Any = EvaluatePython.toJava(_, schema)
      val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
        plan.executeCollect().iterator.map(toJava))
      PythonRDD.serveIterator(iter, "serve-DataFrame")
    }
  }

  private[sql] def getRowsToPython(
      _numRows: Int,
      truncate: Int): Array[Any] = {
    EvaluatePython.registerPicklers()
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    val rows = getRows(numRows, truncate).map(_.toArray).toArray
    val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType)))
    val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
      rows.iterator.map(toJava))
    PythonRDD.serveIterator(iter, "serve-GetRows")
  }

  /**
   * Collect a Dataset as Arrow batches and serve stream to PySpark.
   */
  private[sql] def collectAsArrowToPython(): Array[Any] = {
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone

    PythonRDD.serveToStreamWithSync("serve-Arrow") { out =>
      withAction("collectAsArrowToPython", queryExecution) { plan =>
        val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId)
        val arrowBatchRdd = toArrowBatchRdd(plan)
        val numPartitions = arrowBatchRdd.partitions.length

        // Store collection results for worst case of 1 to N-1 partitions
        val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1))
        var lastIndex = -1  // index of last partition written

        // Handler to eagerly write partitions to Python in order
        def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = {
          // If result is from next partition in order
          if (index - 1 == lastIndex) {
            batchWriter.writeBatches(arrowBatches.iterator)
            lastIndex += 1
            // Write stored partitions that come next in order
            while (lastIndex < results.length && results(lastIndex) != null) {
              batchWriter.writeBatches(results(lastIndex).iterator)
              results(lastIndex) = null
              lastIndex += 1
            }
            // After last batch, end the stream
            if (lastIndex == results.length) {
              batchWriter.end()
            }
          } else {
            // Store partitions received out of order
            results(index - 1) = arrowBatches
          }
        }

        sparkSession.sparkContext.runJob(
          arrowBatchRdd,
          (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray,
          0 until numPartitions,
          handlePartitionBatches)
      }
    }
  }

  private[sql] def toPythonIterator(): Array[Any] = {
    withNewExecutionId {
      PythonRDD.toLocalIteratorAndServe(javaToPython.rdd)
    }
  }

  ////////////////////////////////////////////////////////////////////////////
  // Private Helpers
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with
   * an execution.
   */
  private def withNewExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, queryExecution)(body)
  }

  /**
   * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect
   * them with an execution. Before performing the action, the metrics of the executed plan will be
   * reset.
   */
  private def withNewRDDExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, rddQueryExecution) {
      rddQueryExecution.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      body
    }
  }

  /**
   * Wrap a Dataset action to track the QueryExecution and time cost, then report to the
   * user-registered callback functions.
   */
  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {
    try {
      qe.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      val start = System.nanoTime()
      val result = SQLExecution.withNewExecutionId(sparkSession, qe) {
        action(qe.executedPlan)
      }
      val end = System.nanoTime()
      sparkSession.listenerManager.onSuccess(name, qe, end - start)
      result
    } catch {
      case e: Throwable =>
        sparkSession.listenerManager.onFailure(name, qe, e)
        throw e
    }
  }

  /**
   * Collect all elements from a spark plan.
   */
  private def collectFromPlan(plan: SparkPlan): Array[T] = {
    // This projection writes output to a `InternalRow`, which means applying this projection is not
    // thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe.
    val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
    plan.executeCollect().map { row =>
      // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
      // parameter of its `get` method, so it's safe to use null here.
      objProj(row).get(0, null).asInstanceOf[T]
    }
  }

  private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = {
    val sortOrder: Seq[SortOrder] = sortExprs.map { col =>
      col.expr match {
        case expr: SortOrder =>
          expr
        case expr: Expression =>
          SortOrder(expr, Ascending)
      }
    }
    withTypedPlan {
      Sort(sortOrder, global = global, logicalPlan)
    }
  }

  /** A convenient function to wrap a logical plan and produce a DataFrame. */
  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = {
    Dataset.ofRows(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a logical plan and produce a Dataset. */
  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    Dataset(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a set based logical plan and produce a Dataset. */
  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) {
      // Set operators widen types (change the schema), so we cannot reuse the row encoder.
      Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]]
    } else {
      Dataset(sparkSession, logicalPlan)
    }
  }

  /** Convert to an RDD of serialized ArrowRecordBatches. */
  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = {
    val schemaCaptured = this.schema
    val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone
    plan.execute().mapPartitionsInternal { iter =>
      val context = TaskContext.get()
      ArrowConverters.toBatchIterator(
        iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context)
    }
  }

  // This is only used in tests, for now.
  private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = {
    toArrowBatchRdd(queryExecution.executedPlan)
  }
}

[0m2021.03.03 11:39:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:39:14 INFO  time: compiled root in 1.16s[0m
[0m2021.03.03 11:39:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:39:30 INFO  time: compiled root in 1.13s[0m
[0m2021.03.03 11:43:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 11:43:11 INFO  time: compiled root in 1.14s[0m
[0m2021.03.03 12:10:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:10:46 INFO  time: compiled root in 1.77s[0m
[0m2021.03.03 12:14:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:15:01 INFO  time: compiled root in 1.56s[0m
[0m2021.03.03 12:20:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:20:39 INFO  time: compiled root in 2.86s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.rdd

import java.util.Random

import scala.collection.{mutable, Map}
import scala.collection.mutable.ArrayBuffer
import scala.io.Codec
import scala.language.implicitConversions
import scala.reflect.{classTag, ClassTag}
import scala.util.hashing

import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus
import org.apache.hadoop.io.{BytesWritable, NullWritable, Text}
import org.apache.hadoop.io.compress.CompressionCodec
import org.apache.hadoop.mapred.TextOutputFormat

import org.apache.spark._
import org.apache.spark.Partitioner._
import org.apache.spark.annotation.{DeveloperApi, Experimental, Since}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.internal.Logging
import org.apache.spark.partial.BoundedDouble
import org.apache.spark.partial.CountEvaluator
import org.apache.spark.partial.GroupedCountEvaluator
import org.apache.spark.partial.PartialResult
import org.apache.spark.storage.{RDDBlockId, StorageLevel}
import org.apache.spark.util.{BoundedPriorityQueue, Utils}
import org.apache.spark.util.collection.{OpenHashMap, Utils => collectionUtils}
import org.apache.spark.util.random.{BernoulliCellSampler, BernoulliSampler, PoissonSampler,
  SamplingUtils}

/**
 * A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable,
 * partitioned collection of elements that can be operated on in parallel. This class contains the
 * basic operations available on all RDDs, such as `map`, `filter`, and `persist`. In addition,
 * [[org.apache.spark.rdd.PairRDDFunctions]] contains operations available only on RDDs of key-value
 * pairs, such as `groupByKey` and `join`;
 * [[org.apache.spark.rdd.DoubleRDDFunctions]] contains operations available only on RDDs of
 * Doubles; and
 * [[org.apache.spark.rdd.SequenceFileRDDFunctions]] contains operations available on RDDs that
 * can be saved as SequenceFiles.
 * All operations are automatically available on any RDD of the right type (e.g. RDD[(Int, Int)])
 * through implicit.
 *
 * Internally, each RDD is characterized by five main properties:
 *
 *  - A list of partitions
 *  - A function for computing each split
 *  - A list of dependencies on other RDDs
 *  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
 *  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for
 *    an HDFS file)
 *
 * All of the scheduling and execution in Spark is done based on these methods, allowing each RDD
 * to implement its own way of computing itself. Indeed, users can implement custom RDDs (e.g. for
 * reading data from a new storage system) by overriding these functions. Please refer to the
 * <a href="http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf">Spark paper</a>
 * for more details on RDD internals.
 */
abstract class RDD[T: ClassTag](
    @transient private var _sc: SparkContext,
    @transient private var deps: Seq[Dependency[_]]
  ) extends Serializable with Logging {

  if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) {
    // This is a warning instead of an exception in order to avoid breaking user programs that
    // might have defined nested RDDs without running jobs with them.
    logWarning("Spark does not support nested RDDs (see SPARK-5063)")
  }

  private def sc: SparkContext = {
    if (_sc == null) {
      throw new SparkException(
        "This RDD lacks a SparkContext. It could happen in the following cases: \n(1) RDD " +
        "transformations and actions are NOT invoked by the driver, but inside of other " +
        "transformations; for example, rdd1.map(x => rdd2.values.count() * x) is invalid " +
        "because the values transformation and count action cannot be performed inside of the " +
        "rdd1.map transformation. For more information, see SPARK-5063.\n(2) When a Spark " +
        "Streaming job recovers from checkpoint, this exception will be hit if a reference to " +
        "an RDD not defined by the streaming job is used in DStream operations. For more " +
        "information, See SPARK-13758.")
    }
    _sc
  }

  /** Construct an RDD with just a one-to-one dependency on one parent */
  def this(@transient oneParent: RDD[_]) =
    this(oneParent.context, List(new OneToOneDependency(oneParent)))

  private[spark] def conf = sc.conf
  // =======================================================================
  // Methods that should be implemented by subclasses of RDD
  // =======================================================================

  /**
   * :: DeveloperApi ::
   * Implemented by subclasses to compute a given partition.
   */
  @DeveloperApi
  def compute(split: Partition, context: TaskContext): Iterator[T]

  /**
   * Implemented by subclasses to return the set of partitions in this RDD. This method will only
   * be called once, so it is safe to implement a time-consuming computation in it.
   *
   * The partitions in this array must satisfy the following property:
   *   `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }`
   */
  protected def getPartitions: Array[Partition]

  /**
   * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only
   * be called once, so it is safe to implement a time-consuming computation in it.
   */
  protected def getDependencies: Seq[Dependency[_]] = deps

  /**
   * Optionally overridden by subclasses to specify placement preferences.
   */
  protected def getPreferredLocations(split: Partition): Seq[String] = Nil

  /** Optionally overridden by subclasses to specify how they are partitioned. */
  @transient val partitioner: Option[Partitioner] = None

  // =======================================================================
  // Methods and fields available on all RDDs
  // =======================================================================

  /** The SparkContext that created this RDD. */
  def sparkContext: SparkContext = sc

  /** A unique ID for this RDD (within its SparkContext). */
  val id: Int = sc.newRddId()

  /** A friendly name for this RDD */
  @transient var name: String = _

  /** Assign a name to this RDD */
  def setName(_name: String): this.type = {
    name = _name
    this
  }

  /**
   * Mark this RDD for persisting using the specified level.
   *
   * @param newLevel the target storage level
   * @param allowOverride whether to override any existing level with the new one
   */
  private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = {
    // TODO: Handle changes of StorageLevel
    if (storageLevel != StorageLevel.NONE && newLevel != storageLevel && !allowOverride) {
      throw new UnsupportedOperationException(
        "Cannot change storage level of an RDD after it was already assigned a level")
    }
    // If this is the first time this RDD is marked for persisting, register it
    // with the SparkContext for cleanups and accounting. Do this only once.
    if (storageLevel == StorageLevel.NONE) {
      sc.cleaner.foreach(_.registerRDDForCleanup(this))
      sc.persistRDD(this)
    }
    storageLevel = newLevel
    this
  }

  /**
   * Set this RDD's storage level to persist its values across operations after the first time
   * it is computed. This can only be used to assign a new storage level if the RDD does not
   * have a storage level set yet. Local checkpointing is an exception.
   */
  def persist(newLevel: StorageLevel): this.type = {
    if (isLocallyCheckpointed) {
      // This means the user previously called localCheckpoint(), which should have already
      // marked this RDD for persisting. Here we should override the old storage level with
      // one that is explicitly requested by the user (after adapting it to use disk).
      persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true)
    } else {
      persist(newLevel, allowOverride = false)
    }
  }

  /**
   * Persist this RDD with the default storage level (`MEMORY_ONLY`).
   */
  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)

  /**
   * Persist this RDD with the default storage level (`MEMORY_ONLY`).
   */
  def cache(): this.type = persist()

  /**
   * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk.
   *
   * @param blocking Whether to block until all blocks are deleted.
   * @return This RDD.
   */
  def unpersist(blocking: Boolean = true): this.type = {
    logInfo("Removing RDD " + id + " from persistence list")
    sc.unpersistRDD(id, blocking)
    storageLevel = StorageLevel.NONE
    this
  }

  /** Get the RDD's current storage level, or StorageLevel.NONE if none is set. */
  def getStorageLevel: StorageLevel = storageLevel

  /**
   * Lock for all mutable state of this RDD (persistence, partitions, dependencies, etc.).  We do
   * not use `this` because RDDs are user-visible, so users might have added their own locking on
   * RDDs; sharing that could lead to a deadlock.
   *
   * One thread might hold the lock on many of these, for a chain of RDD dependencies; but
   * because DAGs are acyclic, and we only ever hold locks for one path in that DAG, there is no
   * chance of deadlock.
   *
   * The use of Integer is simply so this is serializable -- executors may reference the shared
   * fields (though they should never mutate them, that only happens on the driver).
   */
  private val stateLock = new Integer(0)

  // Our dependencies and partitions will be gotten by calling subclass's methods below, and will
  // be overwritten when we're checkpointed
  @volatile private var dependencies_ : Seq[Dependency[_]] = _
  @volatile @transient private var partitions_ : Array[Partition] = _

  /** An Option holding our checkpoint RDD, if we are checkpointed */
  private def checkpointRDD: Option[CheckpointRDD[T]] = checkpointData.flatMap(_.checkpointRDD)

  /**
   * Get the list of dependencies of this RDD, taking into account whether the
   * RDD is checkpointed or not.
   */
  final def dependencies: Seq[Dependency[_]] = {
    checkpointRDD.map(r => List(new OneToOneDependency(r))).getOrElse {
      if (dependencies_ == null) {
        stateLock.synchronized {
          if (dependencies_ == null) {
            dependencies_ = getDependencies
          }
        }
      }
      dependencies_
    }
  }

  /**
   * Get the array of partitions of this RDD, taking into account whether the
   * RDD is checkpointed or not.
   */
  final def partitions: Array[Partition] = {
    checkpointRDD.map(_.partitions).getOrElse {
      if (partitions_ == null) {
        stateLock.synchronized {
          if (partitions_ == null) {
            partitions_ = getPartitions
            partitions_.zipWithIndex.foreach { case (partition, index) =>
              require(partition.index == index,
                s"partitions($index).partition == ${partition.index}, but it should equal $index")
            }
          }
        }
      }
      partitions_
    }
  }

  /**
   * Returns the number of partitions of this RDD.
   */
  @Since("1.6.0")
  final def getNumPartitions: Int = partitions.length

  /**
   * Get the preferred locations of a partition, taking into account whether the
   * RDD is checkpointed.
   */
  final def preferredLocations(split: Partition): Seq[String] = {
    checkpointRDD.map(_.getPreferredLocations(split)).getOrElse {
      getPreferredLocations(split)
    }
  }

  /**
   * Internal method to this RDD; will read from cache if applicable, or otherwise compute it.
   * This should ''not'' be called by users directly, but is available for implementors of custom
   * subclasses of RDD.
   */
  final def iterator(split: Partition, context: TaskContext): Iterator[T] = {
    if (storageLevel != StorageLevel.NONE) {
      getOrCompute(split, context)
    } else {
      computeOrReadCheckpoint(split, context)
    }
  }

  /**
   * Return the ancestors of the given RDD that are related to it only through a sequence of
   * narrow dependencies. This traverses the given RDD's dependency tree using DFS, but maintains
   * no ordering on the RDDs returned.
   */
  private[spark] def getNarrowAncestors: Seq[RDD[_]] = {
    val ancestors = new mutable.HashSet[RDD[_]]

    def visit(rdd: RDD[_]): Unit = {
      val narrowDependencies = rdd.dependencies.filter(_.isInstanceOf[NarrowDependency[_]])
      val narrowParents = narrowDependencies.map(_.rdd)
      val narrowParentsNotVisited = narrowParents.filterNot(ancestors.contains)
      narrowParentsNotVisited.foreach { parent =>
        ancestors.add(parent)
        visit(parent)
      }
    }

    visit(this)

    // In case there is a cycle, do not include the root itself
    ancestors.filterNot(_ == this).toSeq
  }

  /**
   * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing.
   */
  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =
  {
    if (isCheckpointedAndMaterialized) {
      firstParent[T].iterator(split, context)
    } else {
      compute(split, context)
    }
  }

  /**
   * Gets or computes an RDD partition. Used by RDD.iterator() when an RDD is cached.
   */
  private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = {
    val blockId = RDDBlockId(id, partition.index)
    var readCachedBlock = true
    // This method is called on executors, so we need call SparkEnv.get instead of sc.env.
    SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => {
      readCachedBlock = false
      computeOrReadCheckpoint(partition, context)
    }) match {
      case Left(blockResult) =>
        if (readCachedBlock) {
          val existingMetrics = context.taskMetrics().inputMetrics
          existingMetrics.incBytesRead(blockResult.bytes)
          new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) {
            override def next(): T = {
              existingMetrics.incRecordsRead(1)
              delegate.next()
            }
          }
        } else {
          new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]])
        }
      case Right(iter) =>
        new InterruptibleIterator(context, iter.asInstanceOf[Iterator[T]])
    }
  }

  /**
   * Execute a block of code in a scope such that all new RDDs created in this body will
   * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}.
   *
   * Note: Return statements are NOT allowed in the given body.
   */
  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](sc)(body)

  // Transformations (return a new RDD)

  /**
   * Return a new RDD by applying a function to all elements of this RDD.
   */
  def map[U: ClassTag](f: T => U): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (context, pid, iter) => iter.map(cleanF))
  }

  /**
   *  Return a new RDD by first applying a function to all elements of this
   *  RDD, and then flattening the results.
   */
  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (context, pid, iter) => iter.flatMap(cleanF))
  }

  /**
   * Return a new RDD containing only the elements that satisfy a predicate.
   */
  def filter(f: T => Boolean): RDD[T] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[T, T](
      this,
      (context, pid, iter) => iter.filter(cleanF),
      preservesPartitioning = true)
  }

  /**
   * Return a new RDD containing the distinct elements in this RDD.
   */
  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    map(x => (x, null)).reduceByKey((x, y) => x, numPartitions).map(_._1)
  }

  /**
   * Return a new RDD containing the distinct elements in this RDD.
   */
  def distinct(): RDD[T] = withScope {
    distinct(partitions.length)
  }

  /**
   * Return a new RDD that has exactly numPartitions partitions.
   *
   * Can increase or decrease the level of parallelism in this RDD. Internally, this uses
   * a shuffle to redistribute data.
   *
   * If you are decreasing the number of partitions in this RDD, consider using `coalesce`,
   * which can avoid performing a shuffle.
   *
   * TODO Fix the Shuffle+Repartition data loss issue described in SPARK-23207.
   */
  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    coalesce(numPartitions, shuffle = true)
  }

  /**
   * Return a new RDD that is reduced into `numPartitions` partitions.
   *
   * This results in a narrow dependency, e.g. if you go from 1000 partitions
   * to 100 partitions, there will not be a shuffle, instead each of the 100
   * new partitions will claim 10 of the current partitions. If a larger number
   * of partitions is requested, it will stay at the current number of partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can pass shuffle = true. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @note With shuffle = true, you can actually coalesce to a larger number
   * of partitions. This is useful if you have a small number of partitions,
   * say 100, potentially with a few partitions being abnormally large. Calling
   * coalesce(1000, shuffle = true) will result in 1000 partitions with the
   * data distributed using a hash partitioner. The optional partition coalescer
   * passed in must be serializable.
   */
  def coalesce(numPartitions: Int, shuffle: Boolean = false,
               partitionCoalescer: Option[PartitionCoalescer] = Option.empty)
              (implicit ord: Ordering[T] = null)
      : RDD[T] = withScope {
    require(numPartitions > 0, s"Number of partitions ($numPartitions) must be positive.")
    if (shuffle) {
      /** Distributes elements evenly across output partitions, starting from a random partition. */
      val distributePartition = (index: Int, items: Iterator[T]) => {
        var position = new Random(hashing.byteswap32(index)).nextInt(numPartitions)
        items.map { t =>
          // Note that the hash code of the key will just be the key itself. The HashPartitioner
          // will mod it with the number of total partitions.
          position = position + 1
          (position, t)
        }
      } : Iterator[(Int, T)]

      // include a shuffle step so that our upstream tasks are still distributed
      new CoalescedRDD(
        new ShuffledRDD[Int, T, T](
          mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true),
          new HashPartitioner(numPartitions)),
        numPartitions,
        partitionCoalescer).values
    } else {
      new CoalescedRDD(this, numPartitions, partitionCoalescer)
    }
  }

  /**
   * Return a sampled subset of this RDD.
   *
   * @param withReplacement can elements be sampled multiple times (replaced when sampled out)
   * @param fraction expected size of the sample as a fraction of this RDD's size
   *  without replacement: probability that each element is chosen; fraction must be [0, 1]
   *  with replacement: expected number of times each element is chosen; fraction must be greater
   *  than or equal to 0
   * @param seed seed for the random number generator
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[RDD]].
   */
  def sample(
      withReplacement: Boolean,
      fraction: Double,
      seed: Long = Utils.random.nextLong): RDD[T] = {
    require(fraction >= 0,
      s"Fraction must be nonnegative, but got ${fraction}")

    withScope {
      require(fraction >= 0.0, "Negative fraction value: " + fraction)
      if (withReplacement) {
        new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed)
      } else {
        new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed)
      }
    }
  }

  /**
   * Randomly splits this RDD with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1
   * @param seed random seed
   *
   * @return split RDDs in an array
   */
  def randomSplit(
      weights: Array[Double],
      seed: Long = Utils.random.nextLong): Array[RDD[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    withScope {
      val sum = weights.sum
      val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
      normalizedCumWeights.sliding(2).map { x =>
        randomSampleWithRange(x(0), x(1), seed)
      }.toArray
    }
  }


  /**
   * Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability
   * range.
   * @param lb lower bound to use for the Bernoulli sampler
   * @param ub upper bound to use for the Bernoulli sampler
   * @param seed the seed for the Random number generator
   * @return A random sub-sample of the RDD without replacement.
   */
  private[spark] def randomSampleWithRange(lb: Double, ub: Double, seed: Long): RDD[T] = {
    this.mapPartitionsWithIndex( { (index, partition) =>
      val sampler = new BernoulliCellSampler[T](lb, ub)
      sampler.setSeed(seed + index)
      sampler.sample(partition)
    }, isOrderSensitive = true, preservesPartitioning = true)
  }

  /**
   * Return a fixed-size sampled subset of this RDD in an array
   *
   * @param withReplacement whether sampling is done with replacement
   * @param num size of the returned sample
   * @param seed seed for the random number generator
   * @return sample of specified size in an array
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   */
  def takeSample(
      withReplacement: Boolean,
      num: Int,
      seed: Long = Utils.random.nextLong): Array[T] = withScope {
    val numStDev = 10.0

    require(num >= 0, "Negative number of elements requested")
    require(num <= (Int.MaxValue - (numStDev * math.sqrt(Int.MaxValue)).toInt),
      "Cannot support a sample size > Int.MaxValue - " +
      s"$numStDev * math.sqrt(Int.MaxValue)")

    if (num == 0) {
      new Array[T](0)
    } else {
      val initialCount = this.count()
      if (initialCount == 0) {
        new Array[T](0)
      } else {
        val rand = new Random(seed)
        if (!withReplacement && num >= initialCount) {
          Utils.randomizeInPlace(this.collect(), rand)
        } else {
          val fraction = SamplingUtils.computeFractionForSampleSize(num, initialCount,
            withReplacement)
          var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect()

          // If the first sample didn't turn out large enough, keep trying to take samples;
          // this shouldn't happen often because we use a big multiplier for the initial size
          var numIters = 0
          while (samples.length < num) {
            logWarning(s"Needed to re-sample due to insufficient sample size. Repeat #$numIters")
            samples = this.sample(withReplacement, fraction, rand.nextInt()).collect()
            numIters += 1
          }
          Utils.randomizeInPlace(samples, rand).take(num)
        }
      }
    }
  }

  /**
   * Return the union of this RDD and another one. Any identical elements will appear multiple
   * times (use `.distinct()` to eliminate them).
   */
  def union(other: RDD[T]): RDD[T] = withScope {
    sc.union(this, other)
  }

  /**
   * Return the union of this RDD and another one. Any identical elements will appear multiple
   * times (use `.distinct()` to eliminate them).
   */
  def ++(other: RDD[T]): RDD[T] = withScope {
    this.union(other)
  }

  /**
   * Return this RDD sorted by the given key function.
   */
  def sortBy[K](
      f: (T) => K,
      ascending: Boolean = true,
      numPartitions: Int = this.partitions.length)
      (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope {
    this.keyBy[K](f)
        .sortByKey(ascending, numPartitions)
        .values
  }

  /**
   * Return the intersection of this RDD and another one. The output will not contain any duplicate
   * elements, even if the input RDDs did.
   *
   * @note This method performs a shuffle internally.
   */
  def intersection(other: RDD[T]): RDD[T] = withScope {
    this.map(v => (v, null)).cogroup(other.map(v => (v, null)))
        .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty }
        .keys
  }

  /**
   * Return the intersection of this RDD and another one. The output will not contain any duplicate
   * elements, even if the input RDDs did.
   *
   * @note This method performs a shuffle internally.
   *
   * @param partitioner Partitioner to use for the resulting RDD
   */
  def intersection(
      other: RDD[T],
      partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    this.map(v => (v, null)).cogroup(other.map(v => (v, null)), partitioner)
        .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty }
        .keys
  }

  /**
   * Return the intersection of this RDD and another one. The output will not contain any duplicate
   * elements, even if the input RDDs did.  Performs a hash partition across the cluster
   *
   * @note This method performs a shuffle internally.
   *
   * @param numPartitions How many partitions to use in the resulting RDD
   */
  def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope {
    intersection(other, new HashPartitioner(numPartitions))
  }

  /**
   * Return an RDD created by coalescing all elements within each partition into an array.
   */
  def glom(): RDD[Array[T]] = withScope {
    new MapPartitionsRDD[Array[T], T](this, (context, pid, iter) => Iterator(iter.toArray))
  }

  /**
   * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of
   * elements (a, b) where a is in `this` and b is in `other`.
   */
  def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope {
    new CartesianRDD(sc, this, other)
  }

  /**
   * Return an RDD of grouped items. Each group consists of a key and a sequence of elements
   * mapping to that key. The ordering of elements within each group is not guaranteed, and
   * may even differ each time the resulting RDD is evaluated.
   *
   * @note This operation may be very expensive. If you are grouping in order to perform an
   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`
   * or `PairRDDFunctions.reduceByKey` will provide much better performance.
   */
  def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope {
    groupBy[K](f, defaultPartitioner(this))
  }

  /**
   * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements
   * mapping to that key. The ordering of elements within each group is not guaranteed, and
   * may even differ each time the resulting RDD is evaluated.
   *
   * @note This operation may be very expensive. If you are grouping in order to perform an
   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`
   * or `PairRDDFunctions.reduceByKey` will provide much better performance.
   */
  def groupBy[K](
      f: T => K,
      numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope {
    groupBy(f, new HashPartitioner(numPartitions))
  }

  /**
   * Return an RDD of grouped items. Each group consists of a key and a sequence of elements
   * mapping to that key. The ordering of elements within each group is not guaranteed, and
   * may even differ each time the resulting RDD is evaluated.
   *
   * @note This operation may be very expensive. If you are grouping in order to perform an
   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`
   * or `PairRDDFunctions.reduceByKey` will provide much better performance.
   */
  def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null)
      : RDD[(K, Iterable[T])] = withScope {
    val cleanF = sc.clean(f)
    this.map(t => (cleanF(t), t)).groupByKey(p)
  }

  /**
   * Return an RDD created by piping elements to a forked external process.
   */
  def pipe(command: String): RDD[String] = withScope {
    // Similar to Runtime.exec(), if we are given a single string, split it into words
    // using a standard StringTokenizer (i.e. by spaces)
    pipe(PipedRDD.tokenize(command))
  }

  /**
   * Return an RDD created by piping elements to a forked external process.
   */
  def pipe(command: String, env: Map[String, String]): RDD[String] = withScope {
    // Similar to Runtime.exec(), if we are given a single string, split it into words
    // using a standard StringTokenizer (i.e. by spaces)
    pipe(PipedRDD.tokenize(command), env)
  }

  /**
   * Return an RDD created by piping elements to a forked external process. The resulting RDD
   * is computed by executing the given process once per partition. All elements
   * of each input partition are written to a process's stdin as lines of input separated
   * by a newline. The resulting partition consists of the process's stdout output, with
   * each line of stdout resulting in one element of the output partition. A process is invoked
   * even for empty partitions.
   *
   * The print behavior can be customized by providing two functions.
   *
   * @param command command to run in forked process.
   * @param env environment variables to set.
   * @param printPipeContext Before piping elements, this function is called as an opportunity
   *                         to pipe context data. Print line function (like out.println) will be
   *                         passed as printPipeContext's parameter.
   * @param printRDDElement Use this function to customize how to pipe elements. This function
   *                        will be called with each RDD element as the 1st parameter, and the
   *                        print line function (like out.println()) as the 2nd parameter.
   *                        An example of pipe the RDD data of groupBy() in a streaming way,
   *                        instead of constructing a huge String to concat all the elements:
   *                        {{{
   *                        def printRDDElement(record:(String, Seq[String]), f:String=>Unit) =
   *                          for (e <- record._2) {f(e)}
   *                        }}}
   * @param separateWorkingDir Use separate working directories for each task.
   * @param bufferSize Buffer size for the stdin writer for the piped process.
   * @param encoding Char encoding used for interacting (via stdin, stdout and stderr) with
   *                 the piped process
   * @return the result RDD
   */
  def pipe(
      command: Seq[String],
      env: Map[String, String] = Map(),
      printPipeContext: (String => Unit) => Unit = null,
      printRDDElement: (T, String => Unit) => Unit = null,
      separateWorkingDir: Boolean = false,
      bufferSize: Int = 8192,
      encoding: String = Codec.defaultCharsetCodec.name): RDD[String] = withScope {
    new PipedRDD(this, command, env,
      if (printPipeContext ne null) sc.clean(printPipeContext) else null,
      if (printRDDElement ne null) sc.clean(printRDDElement) else null,
      separateWorkingDir,
      bufferSize,
      encoding)
  }

  /**
   * Return a new RDD by applying a function to each partition of this RDD.
   *
   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which
   * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.
   */
  def mapPartitions[U: ClassTag](
      f: Iterator[T] => Iterator[U],
      preservesPartitioning: Boolean = false): RDD[U] = withScope {
    val cleanedF = sc.clean(f)
    new MapPartitionsRDD(
      this,
      (context: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(iter),
      preservesPartitioning)
  }

  /**
   * [performance] Spark's internal mapPartitionsWithIndex method that skips closure cleaning.
   * It is a performance API to be used carefully only if we are sure that the RDD elements are
   * serializable and don't require closure cleaning.
   *
   * @param preservesPartitioning indicates whether the input function preserves the partitioner,
   *                              which should be `false` unless this is a pair RDD and the input
   *                              function doesn't modify the keys.
   * @param isOrderSensitive whether or not the function is order-sensitive. If it's order
   *                         sensitive, it may return totally different result when the input order
   *                         is changed. Mostly stateful functions are order-sensitive.
   */
  private[spark] def mapPartitionsWithIndexInternal[U: ClassTag](
      f: (Int, Iterator[T]) => Iterator[U],
      preservesPartitioning: Boolean = false,
      isOrderSensitive: Boolean = false): RDD[U] = withScope {
    new MapPartitionsRDD(
      this,
      (context: TaskContext, index: Int, iter: Iterator[T]) => f(index, iter),
      preservesPartitioning = preservesPartitioning,
      isOrderSensitive = isOrderSensitive)
  }

  /**
   * [performance] Spark's internal mapPartitions method that skips closure cleaning.
   */
  private[spark] def mapPartitionsInternal[U: ClassTag](
      f: Iterator[T] => Iterator[U],
      preservesPartitioning: Boolean = false): RDD[U] = withScope {
    new MapPartitionsRDD(
      this,
      (context: TaskContext, index: Int, iter: Iterator[T]) => f(iter),
      preservesPartitioning)
  }

  /**
   * Return a new RDD by applying a function to each partition of this RDD, while tracking the index
   * of the original partition.
   *
   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which
   * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.
   */
  def mapPartitionsWithIndex[U: ClassTag](
      f: (Int, Iterator[T]) => Iterator[U],
      preservesPartitioning: Boolean = false): RDD[U] = withScope {
    val cleanedF = sc.clean(f)
    new MapPartitionsRDD(
      this,
      (context: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter),
      preservesPartitioning)
  }

  /**
   * Return a new RDD by applying a function to each partition of this RDD, while tracking the index
   * of the original partition.
   *
   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which
   * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.
   *
   * `isOrderSensitive` indicates whether the function is order-sensitive. If it is order
   * sensitive, it may return totally different result when the input order
   * is changed. Mostly stateful functions are order-sensitive.
   */
  private[spark] def mapPartitionsWithIndex[U: ClassTag](
      f: (Int, Iterator[T]) => Iterator[U],
      preservesPartitioning: Boolean,
      isOrderSensitive: Boolean): RDD[U] = withScope {
    val cleanedF = sc.clean(f)
    new MapPartitionsRDD(
      this,
      (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter),
      preservesPartitioning,
      isOrderSensitive = isOrderSensitive)
  }

  /**
   * Zips this RDD with another one, returning key-value pairs with the first element in each RDD,
   * second element in each RDD, etc. Assumes that the two RDDs have the *same number of
   * partitions* and the *same number of elements in each partition* (e.g. one was made through
   * a map on the other).
   */
  def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope {
    zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) =>
      new Iterator[(T, U)] {
        def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match {
          case (true, true) => true
          case (false, false) => false
          case _ => throw new SparkException("Can only zip RDDs with " +
            "same number of elements in each partition")
        }
        def next(): (T, U) = (thisIter.next(), otherIter.next())
      }
    }
  }

  /**
   * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by
   * applying a function to the zipped partitions. Assumes that all the RDDs have the
   * *same number of partitions*, but does *not* require them to have the same number
   * of elements in each partition.
   */
  def zipPartitions[B: ClassTag, V: ClassTag]
      (rdd2: RDD[B], preservesPartitioning: Boolean)
      (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope {
    new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning)
  }

  def zipPartitions[B: ClassTag, V: ClassTag]
      (rdd2: RDD[B])
      (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope {
    zipPartitions(rdd2, preservesPartitioning = false)(f)
  }

  def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag]
      (rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean)
      (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope {
    new ZippedPartitionsRDD3(sc, sc.clean(f), this, rdd2, rdd3, preservesPartitioning)
  }

  def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag]
      (rdd2: RDD[B], rdd3: RDD[C])
      (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope {
    zipPartitions(rdd2, rdd3, preservesPartitioning = false)(f)
  }

  def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag]
      (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean)
      (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope {
    new ZippedPartitionsRDD4(sc, sc.clean(f), this, rdd2, rdd3, rdd4, preservesPartitioning)
  }

  def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag]
      (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D])
      (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope {
    zipPartitions(rdd2, rdd3, rdd4, preservesPartitioning = false)(f)
  }


  // Actions (launch a job to return a value to the user program)

  /**
   * Applies a function f to all elements of this RDD.
   */
  def foreach(f: T => Unit): Unit = withScope {
    val cleanF = sc.clean(f)
    sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF))
  }

  /**
   * Applies a function f to each partition of this RDD.
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withScope {
    val cleanF = sc.clean(f)
    sc.runJob(this, (iter: Iterator[T]) => cleanF(iter))
  }

  /**
   * Return an array that contains all of the elements in this RDD.
   *
   * @note This method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   */
  def collect(): Array[T] = withScope {
    val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
    Array.concat(results: _*)
  }

  /**
   * Return an iterator that contains all of the elements in this RDD.
   *
   * The iterator will consume as much memory as the largest partition in this RDD.
   *
   * @note This results in multiple Spark jobs, and if the input RDD is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input RDD should be cached first.
   */
  def toLocalIterator: Iterator[T] = withScope {
    def collectPartition(p: Int): Array[T] = {
      sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head
    }
    partitions.indices.iterator.flatMap(i => collectPartition(i))
  }

  /**
   * Return an RDD that contains all matching values by applying `f`.
   */
  def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    filter(cleanF.isDefinedAt).map(cleanF)
  }

  /**
   * Return an RDD with the elements from `this` that are not in `other`.
   *
   * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting
   * RDD will be &lt;= us.
   */
  def subtract(other: RDD[T]): RDD[T] = withScope {
    subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length)))
  }

  /**
   * Return an RDD with the elements from `this` that are not in `other`.
   */
  def subtract(other: RDD[T], numPartitions: Int): RDD[T] = withScope {
    subtract(other, new HashPartitioner(numPartitions))
  }

  /**
   * Return an RDD with the elements from `this` that are not in `other`.
   */
  def subtract(
      other: RDD[T],
      p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    if (partitioner == Some(p)) {
      // Our partitioner knows how to handle T (which, since we have a partitioner, is
      // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples
      val p2 = new Partitioner() {
        override def numPartitions: Int = p.numPartitions
        override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1)
      }
      // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies
      // anyway, and when calling .keys, will not have a partitioner set, even though
      // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be
      // partitioned by the right/real keys (e.g. p).
      this.map(x => (x, null)).subtractByKey(other.map((_, null)), p2).keys
    } else {
      this.map(x => (x, null)).subtractByKey(other.map((_, null)), p).keys
    }
  }

  /**
   * Reduces the elements of this RDD using the specified commutative and
   * associative binary operator.
   */
  def reduce(f: (T, T) => T): T = withScope {
    val cleanF = sc.clean(f)
    val reducePartition: Iterator[T] => Option[T] = iter => {
      if (iter.hasNext) {
        Some(iter.reduceLeft(cleanF))
      } else {
        None
      }
    }
    var jobResult: Option[T] = None
    val mergeResult = (index: Int, taskResult: Option[T]) => {
      if (taskResult.isDefined) {
        jobResult = jobResult match {
          case Some(value) => Some(f(value, taskResult.get))
          case None => taskResult
        }
      }
    }
    sc.runJob(this, reducePartition, mergeResult)
    // Get the final result out of our Option, or throw an exception if the RDD was empty
    jobResult.getOrElse(throw new UnsupportedOperationException("empty collection"))
  }

  /**
   * Reduces the elements of this RDD in a multi-level tree pattern.
   *
   * @param depth suggested depth of the tree (default: 2)
   * @see [[org.apache.spark.rdd.RDD#reduce]]
   */
  def treeReduce(f: (T, T) => T, depth: Int = 2): T = withScope {
    require(depth >= 1, s"Depth must be greater than or equal to 1 but got $depth.")
    val cleanF = context.clean(f)
    val reducePartition: Iterator[T] => Option[T] = iter => {
      if (iter.hasNext) {
        Some(iter.reduceLeft(cleanF))
      } else {
        None
      }
    }
    val partiallyReduced = mapPartitions(it => Iterator(reducePartition(it)))
    val op: (Option[T], Option[T]) => Option[T] = (c, x) => {
      if (c.isDefined && x.isDefined) {
        Some(cleanF(c.get, x.get))
      } else if (c.isDefined) {
        c
      } else if (x.isDefined) {
        x
      } else {
        None
      }
    }
    partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth)
      .getOrElse(throw new UnsupportedOperationException("empty collection"))
  }

  /**
   * Aggregate the elements of each partition, and then the results for all the partitions, using a
   * given associative function and a neutral "zero value". The function
   * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object
   * allocation; however, it should not modify t2.
   *
   * This behaves somewhat differently from fold operations implemented for non-distributed
   * collections in functional languages like Scala. This fold operation may be applied to
   * partitions individually, and then fold those results into the final result, rather than
   * apply the fold to each element sequentially in some defined ordering. For functions
   * that are not commutative, the result may differ from that of a fold applied to a
   * non-distributed collection.
   *
   * @param zeroValue the initial value for the accumulated result of each partition for the `op`
   *                  operator, and also the initial value for the combine results from different
   *                  partitions for the `op` operator - this will typically be the neutral
   *                  element (e.g. `Nil` for list concatenation or `0` for summation)
   * @param op an operator used to both accumulate results within a partition and combine results
   *                  from different partitions
   */
  def fold(zeroValue: T)(op: (T, T) => T): T = withScope {
    // Clone the zero value since we will also be serializing it as part of tasks
    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())
    val cleanOp = sc.clean(op)
    val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp)
    val mergeResult = (index: Int, taskResult: T) => jobResult = op(jobResult, taskResult)
    sc.runJob(this, foldPartition, mergeResult)
    jobResult
  }

  /**
   * Aggregate the elements of each partition, and then the results for all the partitions, using
   * given combine functions and a neutral "zero value". This function can return a different result
   * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U
   * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are
   * allowed to modify and return their first argument instead of creating a new U to avoid memory
   * allocation.
   *
   * @param zeroValue the initial value for the accumulated result of each partition for the
   *                  `seqOp` operator, and also the initial value for the combine results from
   *                  different partitions for the `combOp` operator - this will typically be the
   *                  neutral element (e.g. `Nil` for list concatenation or `0` for summation)
   * @param seqOp an operator used to accumulate results within a partition
   * @param combOp an associative operator used to combine results from different partitions
   */
  def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = withScope {
    // Clone the zero value since we will also be serializing it as part of tasks
    var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance())
    val cleanSeqOp = sc.clean(seqOp)
    val cleanCombOp = sc.clean(combOp)
    val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp)
    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)
    sc.runJob(this, aggregatePartition, mergeResult)
    jobResult
  }

  /**
   * Aggregates the elements of this RDD in a multi-level tree pattern.
   * This method is semantically identical to [[org.apache.spark.rdd.RDD#aggregate]].
   *
   * @param depth suggested depth of the tree (default: 2)
   */
  def treeAggregate[U: ClassTag](zeroValue: U)(
      seqOp: (U, T) => U,
      combOp: (U, U) => U,
      depth: Int = 2): U = withScope {
    require(depth >= 1, s"Depth must be greater than or equal to 1 but got $depth.")
    if (partitions.length == 0) {
      Utils.clone(zeroValue, context.env.closureSerializer.newInstance())
    } else {
      val cleanSeqOp = context.clean(seqOp)
      val cleanCombOp = context.clean(combOp)
      val aggregatePartition =
        (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp)
      var partiallyAggregated: RDD[U] = mapPartitions(it => Iterator(aggregatePartition(it)))
      var numPartitions = partiallyAggregated.partitions.length
      val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2)
      // If creating an extra level doesn't help reduce
      // the wall-clock time, we stop tree aggregation.

      // Don't trigger TreeAggregation when it doesn't save wall-clock time
      while (numPartitions > scale + math.ceil(numPartitions.toDouble / scale)) {
        numPartitions /= scale
        val curNumPartitions = numPartitions
        partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex {
          (i, iter) => iter.map((i % curNumPartitions, _))
        }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values
      }
      val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())
      partiallyAggregated.fold(copiedZeroValue)(cleanCombOp)
    }
  }

  /**
   * Return the number of elements in the RDD.
   */
  def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum

  /**
   * Approximate version of count() that returns a potentially incomplete result
   * within a timeout, even if not all tasks have finished.
   *
   * The confidence is the probability that the error bounds of the result will
   * contain the true value. That is, if countApprox were called repeatedly
   * with confidence 0.9, we would expect 90% of the results to contain the
   * true count. The confidence must be in the range [0,1] or an exception will
   * be thrown.
   *
   * @param timeout maximum time to wait for the job, in milliseconds
   * @param confidence the desired statistical confidence in the result
   * @return a potentially incomplete result, with error bounds
   */
  def countApprox(
      timeout: Long,
      confidence: Double = 0.95): PartialResult[BoundedDouble] = withScope {
    require(0.0 <= confidence && confidence <= 1.0, s"confidence ($confidence) must be in [0,1]")
    val countElements: (TaskContext, Iterator[T]) => Long = { (ctx, iter) =>
      var result = 0L
      while (iter.hasNext) {
        result += 1L
        iter.next()
      }
      result
    }
    val evaluator = new CountEvaluator(partitions.length, confidence)
    sc.runApproximateJob(this, countElements, evaluator, timeout)
  }

  /**
   * Return the count of each unique value in this RDD as a local map of (value, count) pairs.
   *
   * @note This method should only be used if the resulting map is expected to be small, as
   * the whole thing is loaded into the driver's memory.
   * To handle very large results, consider using
   *
   * {{{
   * rdd.map(x => (x, 1L)).reduceByKey(_ + _)
   * }}}
   *
   * , which returns an RDD[T, Long] instead of a map.
   */
  def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope {
    map(value => (value, null)).countByKey()
  }

  /**
   * Approximate version of countByValue().
   *
   * @param timeout maximum time to wait for the job, in milliseconds
   * @param confidence the desired statistical confidence in the result
   * @return a potentially incomplete result, with error bounds
   */
  def countByValueApprox(timeout: Long, confidence: Double = 0.95)
      (implicit ord: Ordering[T] = null)
      : PartialResult[Map[T, BoundedDouble]] = withScope {
    require(0.0 <= confidence && confidence <= 1.0, s"confidence ($confidence) must be in [0,1]")
    if (elementClassTag.runtimeClass.isArray) {
      throw new SparkException("countByValueApprox() does not support arrays")
    }
    val countPartition: (TaskContext, Iterator[T]) => OpenHashMap[T, Long] = { (ctx, iter) =>
      val map = new OpenHashMap[T, Long]
      iter.foreach {
        t => map.changeValue(t, 1L, _ + 1L)
      }
      map
    }
    val evaluator = new GroupedCountEvaluator[T](partitions.length, confidence)
    sc.runApproximateJob(this, countPartition, evaluator, timeout)
  }

  /**
   * Return approximate number of distinct elements in the RDD.
   *
   * The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice:
   * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available
   * <a href="http://dx.doi.org/10.1145/2452376.2452456">here</a>.
   *
   * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater
   * than `p`) would trigger sparse representation of registers, which may reduce the memory
   * consumption and increase accuracy when the cardinality is small.
   *
   * @param p The precision value for the normal set.
   *          `p` must be a value between 4 and `sp` if `sp` is not zero (32 max).
   * @param sp The precision value for the sparse set, between 0 and 32.
   *           If `sp` equals 0, the sparse representation is skipped.
   */
  def countApproxDistinct(p: Int, sp: Int): Long = withScope {
    require(p >= 4, s"p ($p) must be >= 4")
    require(sp <= 32, s"sp ($sp) must be <= 32")
    require(sp == 0 || p <= sp, s"p ($p) cannot be greater than sp ($sp)")
    val zeroCounter = new HyperLogLogPlus(p, sp)
    aggregate(zeroCounter)(
      (hll: HyperLogLogPlus, v: T) => {
        hll.offer(v)
        hll
      },
      (h1: HyperLogLogPlus, h2: HyperLogLogPlus) => {
        h1.addAll(h2)
        h1
      }).cardinality()
  }

  /**
   * Return approximate number of distinct elements in the RDD.
   *
   * The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice:
   * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available
   * <a href="http://dx.doi.org/10.1145/2452376.2452456">here</a>.
   *
   * @param relativeSD Relative accuracy. Smaller values create counters that require more space.
   *                   It must be greater than 0.000017.
   */
  def countApproxDistinct(relativeSD: Double = 0.05): Long = withScope {
    require(relativeSD > 0.000017, s"accuracy ($relativeSD) must be greater than 0.000017")
    val p = math.ceil(2.0 * math.log(1.054 / relativeSD) / math.log(2)).toInt
    countApproxDistinct(if (p < 4) 4 else p, 0)
  }

  /**
   * Zips this RDD with its element indices. The ordering is first based on the partition index
   * and then the ordering of items within each partition. So the first item in the first
   * partition gets index 0, and the last item in the last partition receives the largest index.
   *
   * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type.
   * This method needs to trigger a spark job when this RDD contains more than one partitions.
   *
   * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of
   * elements in a partition. The index assigned to each element is therefore not guaranteed,
   * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee
   * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.
   */
  def zipWithIndex(): RDD[(T, Long)] = withScope {
    new ZippedWithIndexRDD(this)
  }

  /**
   * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k,
   * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method
   * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]].
   *
   * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of
   * elements in a partition. The unique ID assigned to each element is therefore not guaranteed,
   * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee
   * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.
   */
  def zipWithUniqueId(): RDD[(T, Long)] = withScope {
    val n = this.partitions.length.toLong
    this.mapPartitionsWithIndex { case (k, iter) =>
      Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) =>
        (item, i * n + k)
      }
    }
  }

  /**
   * Take the first num elements of the RDD. It works by first scanning one partition, and use the
   * results from that partition to estimate the number of additional partitions needed to satisfy
   * the limit.
   *
   * @note This method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @note Due to complications in the internal implementation, this method will raise
   * an exception if called on an RDD of `Nothing` or `Null`.
   */
  def take(num: Int): Array[T] = withScope {
    val scaleUpFactor = Math.max(conf.getInt("spark.rdd.limit.scaleUpFactor", 4), 2)
    if (num == 0) {
      new Array[T](0)
    } else {
      val buf = new ArrayBuffer[T]
      val totalParts = this.partitions.length
      var partsScanned = 0
      while (buf.size < num && partsScanned < totalParts) {
        // The number of partitions to try in this iteration. It is ok for this number to be
        // greater than totalParts because we actually cap it at totalParts in runJob.
        var numPartsToTry = 1L
        val left = num - buf.size
        if (partsScanned > 0) {
          // If we didn't find any rows after the previous iteration, quadruple and retry.
          // Otherwise, interpolate the number of partitions we need to try, but overestimate
          // it by 50%. We also cap the estimation in the end.
          if (buf.isEmpty) {
            numPartsToTry = partsScanned * scaleUpFactor
          } else {
            // As left > 0, numPartsToTry is always >= 1
            numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt
            numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor)
          }
        }

        val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt)
        val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p)

        res.foreach(buf ++= _.take(num - buf.size))
        partsScanned += p.size
      }

      buf.toArray
    }
  }

  /**
   * Return the first element in this RDD.
   */
  def first(): T = withScope {
    take(1) match {
      case Array(t) => t
      case _ => throw new UnsupportedOperationException("empty collection")
    }
  }

  /**
   * Returns the top k (largest) elements from this RDD as defined by the specified
   * implicit Ordering[T] and maintains the ordering. This does the opposite of
   * [[takeOrdered]]. For example:
   * {{{
   *   sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1)
   *   // returns Array(12)
   *
   *   sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2)
   *   // returns Array(6, 5)
   * }}}
   *
   * @note This method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @param num k, the number of top elements to return
   * @param ord the implicit ordering for T
   * @return an array of top elements
   */
  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope {
    takeOrdered(num)(ord.reverse)
  }

  /**
   * Returns the first k (smallest) elements from this RDD as defined by the specified
   * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]].
   * For example:
   * {{{
   *   sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1)
   *   // returns Array(2)
   *
   *   sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2)
   *   // returns Array(2, 3)
   * }}}
   *
   * @note This method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @param num k, the number of elements to return
   * @param ord the implicit ordering for T
   * @return an array of top elements
   */
  def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope {
    if (num == 0) {
      Array.empty
    } else {
      val mapRDDs = mapPartitions { items =>
        // Priority keeps the largest elements, so let's reverse the ordering.
        val queue = new BoundedPriorityQueue[T](num)(ord.reverse)
        queue ++= collectionUtils.takeOrdered(items, num)(ord)
        Iterator.single(queue)
      }
      if (mapRDDs.partitions.length == 0) {
        Array.empty
      } else {
        mapRDDs.reduce { (queue1, queue2) =>
          queue1 ++= queue2
          queue1
        }.toArray.sorted(ord)
      }
    }
  }

  /**
   * Returns the max of this RDD as defined by the implicit Ordering[T].
   * @return the maximum element of the RDD
   * */
  def max()(implicit ord: Ordering[T]): T = withScope {
    this.reduce(ord.max)
  }

  /**
   * Returns the min of this RDD as defined by the implicit Ordering[T].
   * @return the minimum element of the RDD
   * */
  def min()(implicit ord: Ordering[T]): T = withScope {
    this.reduce(ord.min)
  }

  /**
   * @note Due to complications in the internal implementation, this method will raise an
   * exception if called on an RDD of `Nothing` or `Null`. This may be come up in practice
   * because, for example, the type of `parallelize(Seq())` is `RDD[Nothing]`.
   * (`parallelize(Seq())` should be avoided anyway in favor of `parallelize(Seq[T]())`.)
   * @return true if and only if the RDD contains no elements at all. Note that an RDD
   *         may be empty even when it has at least 1 partition.
   */
  def isEmpty(): Boolean = withScope {
    partitions.length == 0 || take(1).length == 0
  }

  /**
   * Save this RDD as a text file, using string representations of elements.
   */
  def saveAsTextFile(path: String): Unit = withScope {
    // https://issues.apache.org/jira/browse/SPARK-2075
    //
    // NullWritable is a `Comparable` in Hadoop 1.+, so the compiler cannot find an implicit
    // Ordering for it and will use the default `null`. However, it's a `Comparable[NullWritable]`
    // in Hadoop 2.+, so the compiler will call the implicit `Ordering.ordered` method to create an
    // Ordering for `NullWritable`. That's why the compiler will generate different anonymous
    // classes for `saveAsTextFile` in Hadoop 1.+ and Hadoop 2.+.
    //
    // Therefore, here we provide an explicit Ordering `null` to make sure the compiler generate
    // same bytecodes for `saveAsTextFile`.
    val nullWritableClassTag = implicitly[ClassTag[NullWritable]]
    val textClassTag = implicitly[ClassTag[Text]]
    val r = this.mapPartitions { iter =>
      val text = new Text()
      iter.map { x =>
        text.set(x.toString)
        (NullWritable.get(), text)
      }
    }
    RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null)
      .saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)
  }

  /**
   * Save this RDD as a compressed text file, using string representations of elements.
   */
  def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit = withScope {
    // https://issues.apache.org/jira/browse/SPARK-2075
    val nullWritableClassTag = implicitly[ClassTag[NullWritable]]
    val textClassTag = implicitly[ClassTag[Text]]
    val r = this.mapPartitions { iter =>
      val text = new Text()
      iter.map { x =>
        text.set(x.toString)
        (NullWritable.get(), text)
      }
    }
    RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null)
      .saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec)
  }

  /**
   * Save this RDD as a SequenceFile of serialized objects.
   */
  def saveAsObjectFile(path: String): Unit = withScope {
    this.mapPartitions(iter => iter.grouped(10).map(_.toArray))
      .map(x => (NullWritable.get(), new BytesWritable(Utils.serialize(x))))
      .saveAsSequenceFile(path)
  }

  /**
   * Creates tuples of the elements in this RDD by applying `f`.
   */
  def keyBy[K](f: T => K): RDD[(K, T)] = withScope {
    val cleanedF = sc.clean(f)
    map(x => (cleanedF(x), x))
  }

  /** A private method for tests, to look at the contents of each partition */
  private[spark] def collectPartitions(): Array[Array[T]] = withScope {
    sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
  }

  /**
   * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir` and all references to its parent
   * RDDs will be removed. This function must be called before any job has been
   * executed on this RDD. It is strongly recommended that this RDD is persisted in
   * memory, otherwise saving it on a file will require recomputation.
   */
  def checkpoint(): Unit = RDDCheckpointData.synchronized {
    // NOTE: we use a global lock here due to complexities downstream with ensuring
    // children RDD partitions point to the correct parent partitions. In the future
    // we should revisit this consideration.
    if (context.checkpointDir.isEmpty) {
      throw new SparkException("Checkpoint directory has not been set in the SparkContext")
    } else if (checkpointData.isEmpty) {
      checkpointData = Some(new ReliableRDDCheckpointData(this))
    }
  }

  /**
   * Mark this RDD for local checkpointing using Spark's existing caching layer.
   *
   * This method is for users who wish to truncate RDD lineages while skipping the expensive
   * step of replicating the materialized data in a reliable distributed file system. This is
   * useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).
   *
   * Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed
   * data is written to ephemeral local storage in the executors instead of to a reliable,
   * fault-tolerant storage. The effect is that if an executor fails during the computation,
   * the checkpointed data may no longer be accessible, causing an irrecoverable job failure.
   *
   * This is NOT safe to use with dynamic allocation, which removes executors along
   * with their cached blocks. If you must use both features, you are advised to set
   * `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.
   *
   * The checkpoint directory set through `SparkContext#setCheckpointDir` is not used.
   */
  def localCheckpoint(): this.type = RDDCheckpointData.synchronized {
    if (conf.getBoolean("spark.dynamicAllocation.enabled", false) &&
        conf.contains("spark.dynamicAllocation.cachedExecutorIdleTimeout")) {
      logWarning("Local checkpointing is NOT safe to use with dynamic allocation, " +
        "which removes executors along with their cached blocks. If you must use both " +
        "features, you are advised to set `spark.dynamicAllocation.cachedExecutorIdleTimeout` " +
        "to a high value. E.g. If you plan to use the RDD for 1 hour, set the timeout to " +
        "at least 1 hour.")
    }

    // Note: At this point we do not actually know whether the user will call persist() on
    // this RDD later, so we must explicitly call it here ourselves to ensure the cached
    // blocks are registered for cleanup later in the SparkContext.
    //
    // If, however, the user has already called persist() on this RDD, then we must adapt
    // the storage level he/she specified to one that is appropriate for local checkpointing
    // (i.e. uses disk) to guarantee correctness.

    if (storageLevel == StorageLevel.NONE) {
      persist(LocalRDDCheckpointData.DEFAULT_STORAGE_LEVEL)
    } else {
      persist(LocalRDDCheckpointData.transformStorageLevel(storageLevel), allowOverride = true)
    }

    // If this RDD is already checkpointed and materialized, its lineage is already truncated.
    // We must not override our `checkpointData` in this case because it is needed to recover
    // the checkpointed data. If it is overridden, next time materializing on this RDD will
    // cause error.
    if (isCheckpointedAndMaterialized) {
      logWarning("Not marking RDD for local checkpoint because it was already " +
        "checkpointed and materialized")
    } else {
      // Lineage is not truncated yet, so just override any existing checkpoint data with ours
      checkpointData match {
        case Some(_: ReliableRDDCheckpointData[_]) => logWarning(
          "RDD was already marked for reliable checkpointing: overriding with local checkpoint.")
        case _ =>
      }
      checkpointData = Some(new LocalRDDCheckpointData(this))
    }
    this
  }

  /**
   * Return whether this RDD is checkpointed and materialized, either reliably or locally.
   */
  def isCheckpointed: Boolean = isCheckpointedAndMaterialized

  /**
   * Return whether this RDD is checkpointed and materialized, either reliably or locally.
   * This is introduced as an alias for `isCheckpointed` to clarify the semantics of the
   * return value. Exposed for testing.
   */
  private[spark] def isCheckpointedAndMaterialized: Boolean =
    checkpointData.exists(_.isCheckpointed)

  /**
   * Return whether this RDD is marked for local checkpointing.
   * Exposed for testing.
   */
  private[rdd] def isLocallyCheckpointed: Boolean = {
    checkpointData match {
      case Some(_: LocalRDDCheckpointData[T]) => true
      case _ => false
    }
  }

  /**
   * Return whether this RDD is reliably checkpointed and materialized.
   */
  private[rdd] def isReliablyCheckpointed: Boolean = {
    checkpointData match {
      case Some(reliable: ReliableRDDCheckpointData[_]) if reliable.isCheckpointed => true
      case _ => false
    }
  }

  /**
   * Gets the name of the directory to which this RDD was checkpointed.
   * This is not defined if the RDD is checkpointed locally.
   */
  def getCheckpointFile: Option[String] = {
    checkpointData match {
      case Some(reliable: ReliableRDDCheckpointData[T]) => reliable.getCheckpointDir
      case _ => None
    }
  }

  /**
   * :: Experimental ::
   * Marks the current stage as a barrier stage, where Spark must launch all tasks together.
   * In case of a task failure, instead of only restarting the failed task, Spark will abort the
   * entire stage and re-launch all tasks for this stage.
   * The barrier execution mode feature is experimental and it only handles limited scenarios.
   * Please read the linked SPIP and design docs to understand the limitations and future plans.
   * @return an [[RDDBarrier]] instance that provides actions within a barrier stage
   * @see [[org.apache.spark.BarrierTaskContext]]
   * @see <a href="https://jira.apache.org/jira/browse/SPARK-24374">SPIP: Barrier Execution Mode</a>
   * @see <a href="https://jira.apache.org/jira/browse/SPARK-24582">Design Doc</a>
   */
  @Experimental
  @Since("2.4.0")
  def barrier(): RDDBarrier[T] = withScope(new RDDBarrier[T](this))

  // =======================================================================
  // Other internal methods and fields
  // =======================================================================

  private var storageLevel: StorageLevel = StorageLevel.NONE

  /** User code that created this RDD (e.g. `textFile`, `parallelize`). */
  @transient private[spark] val creationSite = sc.getCallSite()

  /**
   * The scope associated with the operation that created this RDD.
   *
   * This is more flexible than the call site and can be defined hierarchically. For more
   * detail, see the documentation of {{RDDOperationScope}}. This scope is not defined if the
   * user instantiates this RDD himself without using any Spark operations.
   */
  @transient private[spark] val scope: Option[RDDOperationScope] = {
    Option(sc.getLocalProperty(SparkContext.RDD_SCOPE_KEY)).map(RDDOperationScope.fromJson)
  }

  private[spark] def getCreationSite: String = Option(creationSite).map(_.shortForm).getOrElse("")

  private[spark] def elementClassTag: ClassTag[T] = classTag[T]

  private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None

  // Whether to checkpoint all ancestor RDDs that are marked for checkpointing. By default,
  // we stop as soon as we find the first such RDD, an optimization that allows us to write
  // less data but is not safe for all workloads. E.g. in streaming we may checkpoint both
  // an RDD and its parent in every batch, in which case the parent may never be checkpointed
  // and its lineage never truncated, leading to OOMs in the long run (SPARK-6847).
  private val checkpointAllMarkedAncestors =
    Option(sc.getLocalProperty(RDD.CHECKPOINT_ALL_MARKED_ANCESTORS)).exists(_.toBoolean)

  /** Returns the first parent RDD */
  protected[spark] def firstParent[U: ClassTag]: RDD[U] = {
    dependencies.head.rdd.asInstanceOf[RDD[U]]
  }

  /** Returns the jth parent RDD: e.g. rdd.parent[T](0) is equivalent to rdd.firstParent[T] */
  protected[spark] def parent[U: ClassTag](j: Int): RDD[U] = {
    dependencies(j).rdd.asInstanceOf[RDD[U]]
  }

  /** The [[org.apache.spark.SparkContext]] that this RDD was created on. */
  def context: SparkContext = sc

  /**
   * Private API for changing an RDD's ClassTag.
   * Used for internal Java-Scala API compatibility.
   */
  private[spark] def retag(cls: Class[T]): RDD[T] = {
    val classTag: ClassTag[T] = ClassTag.apply(cls)
    this.retag(classTag)
  }

  /**
   * Private API for changing an RDD's ClassTag.
   * Used for internal Java-Scala API compatibility.
   */
  private[spark] def retag(implicit classTag: ClassTag[T]): RDD[T] = {
    this.mapPartitions(identity, preservesPartitioning = true)(classTag)
  }

  // Avoid handling doCheckpoint multiple times to prevent excessive recursion
  @transient private var doCheckpointCalled = false

  /**
   * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD
   * has completed (therefore the RDD has been materialized and potentially stored in memory).
   * doCheckpoint() is called recursively on the parent RDDs.
   */
  private[spark] def doCheckpoint(): Unit = {
    RDDOperationScope.withScope(sc, "checkpoint", allowNesting = false, ignoreParent = true) {
      if (!doCheckpointCalled) {
        doCheckpointCalled = true
        if (checkpointData.isDefined) {
          if (checkpointAllMarkedAncestors) {
            // TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint
            // them in parallel.
            // Checkpoint parents first because our lineage will be truncated after we
            // checkpoint ourselves
            dependencies.foreach(_.rdd.doCheckpoint())
          }
          checkpointData.get.checkpoint()
        } else {
          dependencies.foreach(_.rdd.doCheckpoint())
        }
      }
    }
  }

  /**
   * Changes the dependencies of this RDD from its original parents to a new RDD (`newRDD`)
   * created from the checkpoint file, and forget its old dependencies and partitions.
   */
  private[spark] def markCheckpointed(): Unit = stateLock.synchronized {
    clearDependencies()
    partitions_ = null
    deps = null    // Forget the constructor argument for dependencies too
  }

  /**
   * Clears the dependencies of this RDD. This method must ensure that all references
   * to the original parent RDDs are removed to enable the parent RDDs to be garbage
   * collected. Subclasses of RDD may override this method for implementing their own cleaning
   * logic. See [[org.apache.spark.rdd.UnionRDD]] for an example.
   */
  protected def clearDependencies(): Unit = stateLock.synchronized {
    dependencies_ = null
  }

  /** A description of this RDD and its recursive dependencies for debugging. */
  def toDebugString: String = {
    // Get a debug description of an rdd without its children
    def debugSelf(rdd: RDD[_]): Seq[String] = {
      import Utils.bytesToString

      val persistence = if (storageLevel != StorageLevel.NONE) storageLevel.description else ""
      val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info =>
        "    CachedPartitions: %d; MemorySize: %s; ExternalBlockStoreSize: %s; DiskSize: %s".format(
          info.numCachedPartitions, bytesToString(info.memSize),
          bytesToString(info.externalBlockStoreSize), bytesToString(info.diskSize)))

      s"$rdd [$persistence]" +: storageInfo
    }

    // Apply a different rule to the last child
    def debugChildren(rdd: RDD[_], prefix: String): Seq[String] = {
      val len = rdd.dependencies.length
      len match {
        case 0 => Seq.empty
        case 1 =>
          val d = rdd.dependencies.head
          debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]], true)
        case _ =>
          val frontDeps = rdd.dependencies.take(len - 1)
          val frontDepStrings = frontDeps.flatMap(
            d => debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]]))

          val lastDep = rdd.dependencies.last
          val lastDepStrings =
            debugString(lastDep.rdd, prefix, lastDep.isInstanceOf[ShuffleDependency[_, _, _]], true)

          frontDepStrings ++ lastDepStrings
      }
    }
    // The first RDD in the dependency stack has no parents, so no need for a +-
    def firstDebugString(rdd: RDD[_]): Seq[String] = {
      val partitionStr = "(" + rdd.partitions.length + ")"
      val leftOffset = (partitionStr.length - 1) / 2
      val nextPrefix = (" " * leftOffset) + "|" + (" " * (partitionStr.length - leftOffset))

      debugSelf(rdd).zipWithIndex.map{
        case (desc: String, 0) => s"$partitionStr $desc"
        case (desc: String, _) => s"$nextPrefix $desc"
      } ++ debugChildren(rdd, nextPrefix)
    }
    def shuffleDebugString(rdd: RDD[_], prefix: String = "", isLastChild: Boolean): Seq[String] = {
      val partitionStr = "(" + rdd.partitions.length + ")"
      val leftOffset = (partitionStr.length - 1) / 2
      val thisPrefix = prefix.replaceAll("\\|\\s+$", "")
      val nextPrefix = (
        thisPrefix
        + (if (isLastChild) "  " else "| ")
        + (" " * leftOffset) + "|" + (" " * (partitionStr.length - leftOffset)))

      debugSelf(rdd).zipWithIndex.map{
        case (desc: String, 0) => s"$thisPrefix+-$partitionStr $desc"
        case (desc: String, _) => s"$nextPrefix$desc"
      } ++ debugChildren(rdd, nextPrefix)
    }
    def debugString(
        rdd: RDD[_],
        prefix: String = "",
        isShuffle: Boolean = true,
        isLastChild: Boolean = false): Seq[String] = {
      if (isShuffle) {
        shuffleDebugString(rdd, prefix, isLastChild)
      } else {
        debugSelf(rdd).map(prefix + _) ++ debugChildren(rdd, prefix)
      }
    }
    firstDebugString(this).mkString("\n")
  }

  override def toString: String = "%s%s[%d] at %s".format(
    Option(name).map(_ + " ").getOrElse(""), getClass.getSimpleName, id, getCreationSite)

  def toJavaRDD() : JavaRDD[T] = {
    new JavaRDD(this)(elementClassTag)
  }

  /**
   * Whether the RDD is in a barrier stage. Spark must launch all the tasks at the same time for a
   * barrier stage.
   *
   * An RDD is in a barrier stage, if at least one of its parent RDD(s), or itself, are mapped from
   * an [[RDDBarrier]]. This function always returns false for a [[ShuffledRDD]], since a
   * [[ShuffledRDD]] indicates start of a new stage.
   *
   * A [[MapPartitionsRDD]] can be transformed from an [[RDDBarrier]], under that case the
   * [[MapPartitionsRDD]] shall be marked as barrier.
   */
  private[spark] def isBarrier(): Boolean = isBarrier_

  // From performance concern, cache the value to avoid repeatedly compute `isBarrier()` on a long
  // RDD chain.
  @transient protected lazy val isBarrier_ : Boolean =
    dependencies.filter(!_.isInstanceOf[ShuffleDependency[_, _, _]]).exists(_.rdd.isBarrier())

  /**
   * Returns the deterministic level of this RDD's output. Please refer to [[DeterministicLevel]]
   * for the definition.
   *
   * By default, an reliably checkpointed RDD, or RDD without parents(root RDD) is DETERMINATE. For
   * RDDs with parents, we will generate a deterministic level candidate per parent according to
   * the dependency. The deterministic level of the current RDD is the deterministic level
   * candidate that is deterministic least. Please override [[getOutputDeterministicLevel]] to
   * provide custom logic of calculating output deterministic level.
   */
  // TODO: make it public so users can set deterministic level to their custom RDDs.
  // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for
  // different partitions.
  private[spark] final lazy val outputDeterministicLevel: DeterministicLevel.Value = {
    if (isReliablyCheckpointed) {
      DeterministicLevel.DETERMINATE
    } else {
      getOutputDeterministicLevel
    }
  }

  @DeveloperApi
  protected def getOutputDeterministicLevel: DeterministicLevel.Value = {
    val deterministicLevelCandidates = dependencies.map {
      // The shuffle is not really happening, treat it like narrow dependency and assume the output
      // deterministic level of current RDD is same as parent.
      case dep: ShuffleDependency[_, _, _] if dep.rdd.partitioner.exists(_ == dep.partitioner) =>
        dep.rdd.outputDeterministicLevel

      case dep: ShuffleDependency[_, _, _] =>
        if (dep.rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE) {
          // If map output was indeterminate, shuffle output will be indeterminate as well
          DeterministicLevel.INDETERMINATE
        } else if (dep.keyOrdering.isDefined && dep.aggregator.isDefined) {
          // if aggregator specified (and so unique keys) and key ordering specified - then
          // consistent ordering.
          DeterministicLevel.DETERMINATE
        } else {
          // In Spark, the reducer fetches multiple remote shuffle blocks at the same time, and
          // the arrival order of these shuffle blocks are totally random. Even if the parent map
          // RDD is DETERMINATE, the reduce RDD is always UNORDERED.
          DeterministicLevel.UNORDERED
        }

      // For narrow dependency, assume the output deterministic level of current RDD is same as
      // parent.
      case dep => dep.rdd.outputDeterministicLevel
    }

    if (deterministicLevelCandidates.isEmpty) {
      // By default we assume the root RDD is determinate.
      DeterministicLevel.DETERMINATE
    } else {
      deterministicLevelCandidates.maxBy(_.id)
    }
  }

}


/**
 * Defines implicit functions that provide extra functionalities on RDDs of specific types.
 *
 * For example, [[RDD.rddToPairRDDFunctions]] converts an RDD into a [[PairRDDFunctions]] for
 * key-value-pair RDDs, and enabling extra functionalities such as `PairRDDFunctions.reduceByKey`.
 */
object RDD {

  private[spark] val CHECKPOINT_ALL_MARKED_ANCESTORS =
    "spark.checkpoint.checkpointAllMarkedAncestors"

  // The following implicit functions were in SparkContext before 1.3 and users had to
  // `import SparkContext._` to enable them. Now we move them here to make the compiler find
  // them automatically. However, we still keep the old functions in SparkContext for backward
  // compatibility and forward to the following functions directly.

  implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)])
    (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = {
    new PairRDDFunctions(rdd)
  }

  implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]): AsyncRDDActions[T] = {
    new AsyncRDDActions(rdd)
  }

  implicit def rddToSequenceFileRDDFunctions[K, V](rdd: RDD[(K, V)])
      (implicit kt: ClassTag[K], vt: ClassTag[V],
                keyWritableFactory: WritableFactory[K],
                valueWritableFactory: WritableFactory[V])
    : SequenceFileRDDFunctions[K, V] = {
    implicit val keyConverter = keyWritableFactory.convert
    implicit val valueConverter = valueWritableFactory.convert
    new SequenceFileRDDFunctions(rdd,
      keyWritableFactory.writableClass(kt), valueWritableFactory.writableClass(vt))
  }

  implicit def rddToOrderedRDDFunctions[K : Ordering : ClassTag, V: ClassTag](rdd: RDD[(K, V)])
    : OrderedRDDFunctions[K, V, (K, V)] = {
    new OrderedRDDFunctions[K, V, (K, V)](rdd)
  }

  implicit def doubleRDDToDoubleRDDFunctions(rdd: RDD[Double]): DoubleRDDFunctions = {
    new DoubleRDDFunctions(rdd)
  }

  implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T])
    : DoubleRDDFunctions = {
    new DoubleRDDFunctions(rdd.map(x => num.toDouble(x)))
  }
}

/**
 * The deterministic level of RDD's output (i.e. what `RDD#compute` returns). This explains how
 * the output will diff when Spark reruns the tasks for the RDD. There are 3 deterministic levels:
 * 1. DETERMINATE: The RDD output is always the same data set in the same order after a rerun.
 * 2. UNORDERED: The RDD output is always the same data set but the order can be different
 *               after a rerun.
 * 3. INDETERMINATE. The RDD output can be different after a rerun.
 *
 * Note that, the output of an RDD usually relies on the parent RDDs. When the parent RDD's output
 * is INDETERMINATE, it's very likely the RDD's output is also INDETERMINATE.
 */
private[spark] object DeterministicLevel extends Enumeration {
  val DETERMINATE, UNORDERED, INDETERMINATE = Value
}

[0m2021.03.03 12:21:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:21:27 INFO  time: compiled root in 0.46s[0m
[0m2021.03.03 12:21:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:21:58 INFO  time: compiled root in 0.27s[0m
Mar 03, 2021 12:22:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1588
[0m2021.03.03 12:22:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:22:55 INFO  time: compiled root in 1.47s[0m
[0m2021.03.03 12:23:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:23:11 INFO  time: compiled root in 2.3s[0m
[0m2021.03.03 12:23:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:23:20 INFO  time: compiled root in 1.18s[0m
[0m2021.03.03 12:23:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:23:29 INFO  time: compiled root in 1.32s[0m
[0m2021.03.03 12:23:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:23:42 INFO  time: compiled root in 1.62s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.Closeable
import java.util.concurrent.atomic.AtomicReference

import scala.collection.JavaConverters._
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.spark.{SPARK_VERSION, SparkConf, SparkContext, TaskContext}
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.internal.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd}
import org.apache.spark.sql.catalog.Catalog
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions.AttributeReference
import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, Range}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.internal._
import org.apache.spark.sql.internal.StaticSQLConf.CATALOG_IMPLEMENTATION
import org.apache.spark.sql.sources.BaseRelation
import org.apache.spark.sql.streaming._
import org.apache.spark.sql.types.{DataType, StructType}
import org.apache.spark.sql.util.ExecutionListenerManager
import org.apache.spark.util.{CallSite, Utils}


/**
 * The entry point to programming Spark with the Dataset and DataFrame API.
 *
 * In environments that this has been created upfront (e.g. REPL, notebooks), use the builder
 * to get an existing session:
 *
 * {{{
 *   SparkSession.builder().getOrCreate()
 * }}}
 *
 * The builder can also be used to create a new session:
 *
 * {{{
 *   SparkSession.builder
 *     .master("local")
 *     .appName("Word Count")
 *     .config("spark.some.config.option", "some-value")
 *     .getOrCreate()
 * }}}
 *
 * @param sparkContext The Spark context associated with this Spark session.
 * @param existingSharedState If supplied, use the existing shared state
 *                            instead of creating a new one.
 * @param parentSessionState If supplied, inherit all session state (i.e. temporary
 *                            views, SQL config, UDFs etc) from parent.
 */
@InterfaceStability.Stable
class SparkSession private(
    @transient val sparkContext: SparkContext,
    @transient private val existingSharedState: Option[SharedState],
    @transient private val parentSessionState: Option[SessionState],
    @transient private[sql] val extensions: SparkSessionExtensions)
  extends Serializable with Closeable with Logging { self =>

  // The call site where this SparkSession was constructed.
  private val creationSite: CallSite = Utils.getCallSite()

  private[sql] def this(sc: SparkContext) {
    this(sc, None, None, new SparkSessionExtensions)
  }

  sparkContext.assertNotStopped()

  // If there is no active SparkSession, uses the default SQL conf. Otherwise, use the session's.
  SQLConf.setSQLConfGetter(() => {
    SparkSession.getActiveSession.filterNot(_.sparkContext.isStopped).map(_.sessionState.conf)
      .getOrElse(SQLConf.getFallbackConf)
  })

  /**
   * The version of Spark on which this application is running.
   *
   * @since 2.0.0
   */
  def version: String = SPARK_VERSION

  /* ----------------------- *
   |  Session-related state  |
   * ----------------------- */

  /**
   * State shared across sessions, including the `SparkContext`, cached data, listener,
   * and a catalog that interacts with external systems.
   *
   * This is internal to Spark and there is no guarantee on interface stability.
   *
   * @since 2.2.0
   */
  @InterfaceStability.Unstable
  @transient
  lazy val sharedState: SharedState = {
    existingSharedState.getOrElse(new SharedState(sparkContext))
  }

  /**
   * Initial options for session. This options are applied once when sessionState is created.
   */
  @transient
  private[sql] val initialSessionOptions = new scala.collection.mutable.HashMap[String, String]

  /**
   * State isolated across sessions, including SQL configurations, temporary tables, registered
   * functions, and everything else that accepts a [[org.apache.spark.sql.internal.SQLConf]].
   * If `parentSessionState` is not null, the `SessionState` will be a copy of the parent.
   *
   * This is internal to Spark and there is no guarantee on interface stability.
   *
   * @since 2.2.0
   */
  @InterfaceStability.Unstable
  @transient
  lazy val sessionState: SessionState = {
    parentSessionState
      .map(_.clone(this))
      .getOrElse {
        val state = SparkSession.instantiateSessionState(
          SparkSession.sessionStateClassName(sparkContext.conf),
          self)
        initialSessionOptions.foreach { case (k, v) => state.conf.setConfString(k, v) }
        state
      }
  }

  /**
   * A wrapped version of this session in the form of a [[SQLContext]], for backward compatibility.
   *
   * @since 2.0.0
   */
  @transient
  val sqlContext: SQLContext = new SQLContext(this)

  /**
   * Runtime configuration interface for Spark.
   *
   * This is the interface through which the user can get and set all Spark and Hadoop
   * configurations that are relevant to Spark SQL. When getting the value of a config,
   * this defaults to the value set in the underlying `SparkContext`, if any.
   *
   * @since 2.0.0
   */
  @transient lazy val conf: RuntimeConfig = new RuntimeConfig(sessionState.conf)

  /**
   * :: Experimental ::
   * An interface to register custom [[org.apache.spark.sql.util.QueryExecutionListener]]s
   * that listen for execution metrics.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def listenerManager: ExecutionListenerManager = sessionState.listenerManager

  /**
   * :: Experimental ::
   * A collection of methods that are considered experimental, but can be used to hook into
   * the query planner for advanced functionality.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Unstable
  def experimental: ExperimentalMethods = sessionState.experimentalMethods

  /**
   * A collection of methods for registering user-defined functions (UDF).
   *
   * The following example registers a Scala closure as UDF:
   * {{{
   *   sparkSession.udf.register("myUDF", (arg1: Int, arg2: String) => arg2 + arg1)
   * }}}
   *
   * The following example registers a UDF in Java:
   * {{{
   *   sparkSession.udf().register("myUDF",
   *       (Integer arg1, String arg2) -> arg2 + arg1,
   *       DataTypes.StringType);
   * }}}
   *
   * @note The user-defined functions must be deterministic. Due to optimization,
   * duplicate invocations may be eliminated or the function may even be invoked more times than
   * it is present in the query.
   *
   * @since 2.0.0
   */
  def udf: UDFRegistration = sessionState.udfRegistration

  /**
   * :: Experimental ::
   * Returns a `StreamingQueryManager` that allows managing all the
   * `StreamingQuery`s active on `this`.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Unstable
  def streams: StreamingQueryManager = sessionState.streamingQueryManager

  /**
   * Start a new session with isolated SQL configurations, temporary tables, registered
   * functions are isolated, but sharing the underlying `SparkContext` and cached data.
   *
   * @note Other than the `SparkContext`, all shared state is initialized lazily.
   * This method will force the initialization of the shared state to ensure that parent
   * and child sessions are set up with the same shared state. If the underlying catalog
   * implementation is Hive, this will initialize the metastore, which may take some time.
   *
   * @since 2.0.0
   */
  def newSession(): SparkSession = {
    new SparkSession(sparkContext, Some(sharedState), parentSessionState = None, extensions)
  }

  /**
   * Create an identical copy of this `SparkSession`, sharing the underlying `SparkContext`
   * and shared state. All the state of this session (i.e. SQL configurations, temporary tables,
   * registered functions) is copied over, and the cloned session is set up with the same shared
   * state as this session. The cloned session is independent of this session, that is, any
   * non-global change in either session is not reflected in the other.
   *
   * @note Other than the `SparkContext`, all shared state is initialized lazily.
   * This method will force the initialization of the shared state to ensure that parent
   * and child sessions are set up with the same shared state. If the underlying catalog
   * implementation is Hive, this will initialize the metastore, which may take some time.
   */
  private[sql] def cloneSession(): SparkSession = {
    val result = new SparkSession(sparkContext, Some(sharedState), Some(sessionState), extensions)
    result.sessionState // force copy of SessionState
    result
  }


  /* --------------------------------- *
   |  Methods for creating DataFrames  |
   * --------------------------------- */

  /**
   * Returns a `DataFrame` with no rows or columns.
   *
   * @since 2.0.0
   */
  @transient
  lazy val emptyDataFrame: DataFrame = {
    createDataFrame(sparkContext.emptyRDD[Row].setName("empty"), StructType(Nil))
  }

  /**
   * :: Experimental ::
   * Creates a new [[Dataset]] of type T containing zero elements.
   *
   * @return 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def emptyDataset[T: Encoder]: Dataset[T] = {
    val encoder = implicitly[Encoder[T]]
    new Dataset(self, LocalRelation(encoder.schema.toAttributes), encoder)
  }

  /**
   * :: Experimental ::
   * Creates a `DataFrame` from an RDD of Product (e.g. case classes, tuples).
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataFrame[A <: Product : TypeTag](rdd: RDD[A]): DataFrame = {
    SparkSession.setActiveSession(this)
    val encoder = Encoders.product[A]
    Dataset.ofRows(self, ExternalRDD(rdd, self)(encoder))
  }

  /**
   * :: Experimental ::
   * Creates a `DataFrame` from a local Seq of Product.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataFrame[A <: Product : TypeTag](data: Seq[A]): DataFrame = {
    SparkSession.setActiveSession(this)
    val schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]
    val attributeSeq = schema.toAttributes
    Dataset.ofRows(self, LocalRelation.fromProduct(attributeSeq, data))
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from an `RDD` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided RDD matches
   * the provided schema. Otherwise, there will be runtime exception.
   * Example:
   * {{{
   *  import org.apache.spark.sql._
   *  import org.apache.spark.sql.types._
   *  val sparkSession = new org.apache.spark.sql.SparkSession(sc)
   *
   *  val schema =
   *    StructType(
   *      StructField("name", StringType, false) ::
   *      StructField("age", IntegerType, true) :: Nil)
   *
   *  val people =
   *    sc.textFile("examples/src/main/resources/people.txt").map(
   *      _.split(",")).map(p => Row(p(0), p(1).trim.toInt))
   *  val dataFrame = sparkSession.createDataFrame(people, schema)
   *  dataFrame.printSchema
   *  // root
   *  // |-- name: string (nullable = false)
   *  // |-- age: integer (nullable = true)
   *
   *  dataFrame.createOrReplaceTempView("people")
   *  sparkSession.sql("select name from people").collect.foreach(println)
   * }}}
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame = {
    createDataFrame(rowRDD, schema, needsConversion = true)
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from a `JavaRDD` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided RDD matches
   * the provided schema. Otherwise, there will be runtime exception.
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rowRDD: JavaRDD[Row], schema: StructType): DataFrame = {
    createDataFrame(rowRDD.rdd, schema)
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from a `java.util.List` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided List matches
   * the provided schema. Otherwise, there will be runtime exception.
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rows: java.util.List[Row], schema: StructType): DataFrame = {
    Dataset.ofRows(self, LocalRelation.fromExternalRows(schema.toAttributes, rows.asScala))
  }

  /**
   * Applies a schema to an RDD of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   * SELECT * queries will return the columns in an undefined order.
   *
   * @since 2.0.0
   */
  def createDataFrame(rdd: RDD[_], beanClass: Class[_]): DataFrame = {
    val attributeSeq: Seq[AttributeReference] = getSchema(beanClass)
    val className = beanClass.getName
    val rowRdd = rdd.mapPartitions { iter =>
    // BeanInfo is not serializable so we must rediscover it remotely for each partition.
      SQLContext.beansToRows(iter, Utils.classForName(className), attributeSeq)
    }
    Dataset.ofRows(self, LogicalRDD(attributeSeq, rowRdd.setName(rdd.name))(self))
  }

  /**
   * Applies a schema to an RDD of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   * SELECT * queries will return the columns in an undefined order.
   *
   * @since 2.0.0
   */
  def createDataFrame(rdd: JavaRDD[_], beanClass: Class[_]): DataFrame = {
    createDataFrame(rdd.rdd, beanClass)
  }

  /**
   * Applies a schema to a List of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   *          SELECT * queries will return the columns in an undefined order.
   * @since 1.6.0
   */
  def createDataFrame(data: java.util.List[_], beanClass: Class[_]): DataFrame = {
    val attrSeq = getSchema(beanClass)
    val rows = SQLContext.beansToRows(data.asScala.iterator, beanClass, attrSeq)
    Dataset.ofRows(self, LocalRelation(attrSeq, rows.toSeq))
  }

  /**
   * Convert a `BaseRelation` created for external data sources into a `DataFrame`.
   *
   * @since 2.0.0
   */
  def baseRelationToDataFrame(baseRelation: BaseRelation): DataFrame = {
    Dataset.ofRows(self, LogicalRelation(baseRelation))
  }

  /* ------------------------------- *
   |  Methods for creating DataSets  |
   * ------------------------------- */

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from a local Seq of data of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * == Example ==
   *
   * {{{
   *
   *   import spark.implicits._
   *   case class Person(name: String, age: Long)
   *   val data = Seq(Person("Michael", 29), Person("Andy", 30), Person("Justin", 19))
   *   val ds = spark.createDataset(data)
   *
   *   ds.show()
   *   // +-------+---+
   *   // |   name|age|
   *   // +-------+---+
   *   // |Michael| 29|
   *   // |   Andy| 30|
   *   // | Justin| 19|
   *   // +-------+---+
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: Seq[T]): Dataset[T] = {
    // `ExpressionEncoder` is not thread-safe, here we create a new encoder.
    val enc = encoderFor[T].copy()
    val attributes = enc.schema.toAttributes
    val encoded = data.map(d => enc.toRow(d).copy())
    val plan = new LocalRelation(attributes, encoded)
    Dataset[T](self, plan)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from an RDD of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: RDD[T]): Dataset[T] = {
    Dataset[T](self, ExternalRDD(data, self))
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from a `java.util.List` of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * == Java Example ==
   *
   * {{{
   *     List<String> data = Arrays.asList("hello", "world");
   *     Dataset<String> ds = spark.createDataset(data, Encoders.STRING());
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: java.util.List[T]): Dataset[T] = {
    createDataset(data.asScala)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from 0 to `end` (exclusive) with step value 1.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(end: Long): Dataset[java.lang.Long] = range(0, end)

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with step value 1.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long): Dataset[java.lang.Long] = {
    range(start, end, step = 1, numPartitions = sparkContext.defaultParallelism)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with a step value.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long, step: Long): Dataset[java.lang.Long] = {
    range(start, end, step, numPartitions = sparkContext.defaultParallelism)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with a step value, with partition number
   * specified.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long] = {
    new Dataset(self, Range(start, end, step, numPartitions), Encoders.LONG)
  }

  /**
   * Creates a `DataFrame` from an `RDD[InternalRow]`.
   */
  private[sql] def internalCreateDataFrame(
      catalystRows: RDD[InternalRow],
      schema: StructType,
      isStreaming: Boolean = false): DataFrame = {
    // TODO: use MutableProjection when rowRDD is another DataFrame and the applied
    // schema differs from the existing schema on any field data type.
    val logicalPlan = LogicalRDD(
      schema.toAttributes,
      catalystRows,
      isStreaming = isStreaming)(self)
    Dataset.ofRows(self, logicalPlan)
  }

  /**
   * Creates a `DataFrame` from an `RDD[Row]`.
   * User can specify whether the input rows should be converted to Catalyst rows.
   */
  private[sql] def createDataFrame(
      rowRDD: RDD[Row],
      schema: StructType,
      needsConversion: Boolean) = {
    // TODO: use MutableProjection when rowRDD is another DataFrame and the applied
    // schema differs from the existing schema on any field data type.
    val catalystRows = if (needsConversion) {
      val encoder = RowEncoder(schema)
      rowRDD.map(encoder.toRow)
    } else {
      rowRDD.map { r: Row => InternalRow.fromSeq(r.toSeq) }
    }
    internalCreateDataFrame(catalystRows.setName(rowRDD.name), schema)
  }


  /* ------------------------- *
   |  Catalog-related methods  |
   * ------------------------- */

  /**
   * Interface through which the user may create, drop, alter or query underlying
   * databases, tables, functions etc.
   *
   * @since 2.0.0
   */
  @transient lazy val catalog: Catalog = new CatalogImpl(self)

  /**
   * Returns the specified table/view as a `DataFrame`.
   *
   * @param tableName is either a qualified or unqualified name that designates a table or view.
   *                  If a database is specified, it identifies the table/view from the database.
   *                  Otherwise, it first attempts to find a temporary view with the given name
   *                  and then match the table/view from the current database.
   *                  Note that, the global temporary view database is also valid here.
   * @since 2.0.0
   */
  def table(tableName: String): DataFrame = {
    table(sessionState.sqlParser.parseTableIdentifier(tableName))
  }

  private[sql] def table(tableIdent: TableIdentifier): DataFrame = {
    Dataset.ofRows(self, UnresolvedRelation(tableIdent))
  }

  /* ----------------- *
   |  Everything else  |
   * ----------------- */

  /**
   * Executes a SQL query using Spark, returning the result as a `DataFrame`.
   * The dialect that is used for SQL parsing can be configured with 'spark.sql.dialect'.
   *
   * @since 2.0.0
   */
  def sql(sqlText: String): DataFrame = {
    Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))
  }

  /**
   * Returns a [[DataFrameReader]] that can be used to read non-streaming data in as a
   * `DataFrame`.
   * {{{
   *   sparkSession.read.parquet("/path/to/file.parquet")
   *   sparkSession.read.schema(schema).json("/path/to/file.json")
   * }}}
   *
   * @since 2.0.0
   */
  def read: DataFrameReader = new DataFrameReader(self)

  /**
   * Returns a `DataStreamReader` that can be used to read streaming data in as a `DataFrame`.
   * {{{
   *   sparkSession.readStream.parquet("/path/to/directory/of/parquet/files")
   *   sparkSession.readStream.schema(schema).json("/path/to/directory/of/json/files")
   * }}}
   *
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def readStream: DataStreamReader = new DataStreamReader(self)

  /**
   * Executes some code block and prints to stdout the time taken to execute the block. This is
   * available in Scala only and is used primarily for interactive testing and debugging.
   *
   * @since 2.1.0
   */
  def time[T](f: => T): T = {
    val start = System.nanoTime()
    val ret = f
    val end = System.nanoTime()
    // scalastyle:off println
    println(s"Time taken: ${(end - start) / 1000 / 1000} ms")
    // scalastyle:on println
    ret
  }

  // scalastyle:off
  // Disable style checker so "implicits" object can start with lowercase i
  /**
   * :: Experimental ::
   * (Scala-specific) Implicit methods available in Scala for converting
   * common Scala objects into `DataFrame`s.
   *
   * {{{
   *   val sparkSession = SparkSession.builder.getOrCreate()
   *   import sparkSession.implicits._
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  object implicits extends SQLImplicits with Serializable {
    protected override def _sqlContext: SQLContext = SparkSession.this.sqlContext
  }
  // scalastyle:on

  /**
   * Stop the underlying `SparkContext`.
   *
   * @since 2.0.0
   */
  def stop(): Unit = {
    sparkContext.stop()
  }

  /**
   * Synonym for `stop()`.
   *
   * @since 2.1.0
   */
  override def close(): Unit = stop()

  /**
   * Parses the data type in our internal string representation. The data type string should
   * have the same format as the one generated by `toString` in scala.
   * It is only used by PySpark.
   */
  protected[sql] def parseDataType(dataTypeString: String): DataType = {
    DataType.fromJson(dataTypeString)
  }

  /**
   * Apply a schema defined by the schemaString to an RDD. It is only used by PySpark.
   */
  private[sql] def applySchemaToPythonRDD(
      rdd: RDD[Array[Any]],
      schemaString: String): DataFrame = {
    val schema = DataType.fromJson(schemaString).asInstanceOf[StructType]
    applySchemaToPythonRDD(rdd, schema)
  }

  /**
   * Apply `schema` to an RDD.
   *
   * @note Used by PySpark only
   */
  private[sql] def applySchemaToPythonRDD(
      rdd: RDD[Array[Any]],
      schema: StructType): DataFrame = {
    val rowRdd = rdd.mapPartitions { iter =>
      val fromJava = python.EvaluatePython.makeFromJava(schema)
      iter.map(r => fromJava(r).asInstanceOf[InternalRow])
    }
    internalCreateDataFrame(rowRdd, schema)
  }

  /**
   * Returns a Catalyst Schema for the given java bean class.
   */
  private def getSchema(beanClass: Class[_]): Seq[AttributeReference] = {
    val (dataType, _) = JavaTypeInference.inferDataType(beanClass)
    dataType.asInstanceOf[StructType].fields.map { f =>
      AttributeReference(f.name, f.dataType, f.nullable)()
    }
  }

}


@InterfaceStability.Stable
object SparkSession extends Logging {

  /**
   * Builder for [[SparkSession]].
   */
  @InterfaceStability.Stable
  class Builder extends Logging {

    private[this] val options = new scala.collection.mutable.HashMap[String, String]

    private[this] val extensions = new SparkSessionExtensions

    private[this] var userSuppliedContext: Option[SparkContext] = None

    private[spark] def sparkContext(sparkContext: SparkContext): Builder = synchronized {
      userSuppliedContext = Option(sparkContext)
      this
    }

    /**
     * Sets a name for the application, which will be shown in the Spark web UI.
     * If no application name is set, a randomly generated name will be used.
     *
     * @since 2.0.0
     */
    def appName(name: String): Builder = config("spark.app.name", name)

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: String): Builder = synchronized {
      options += key -> value
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Long): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Double): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Boolean): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a list of config options based on the given `SparkConf`.
     *
     * @since 2.0.0
     */
    def config(conf: SparkConf): Builder = synchronized {
      conf.getAll.foreach { case (k, v) => options += k -> v }
      this
    }

    /**
     * Sets the Spark master URL to connect to, such as "local" to run locally, "local[4]" to
     * run locally with 4 cores, or "spark://master:7077" to run on a Spark standalone cluster.
     *
     * @since 2.0.0
     */
    def master(master: String): Builder = config("spark.master", master)

    /**
     * Enables Hive support, including connectivity to a persistent Hive metastore, support for
     * Hive serdes, and Hive user-defined functions.
     *
     * @since 2.0.0
     */
    def enableHiveSupport(): Builder = synchronized {
      if (hiveClassesArePresent) {
        config(CATALOG_IMPLEMENTATION.key, "hive")
      } else {
        throw new IllegalArgumentException(
          "Unable to instantiate SparkSession with Hive support because " +
            "Hive classes are not found.")
      }
    }

    /**
     * Inject extensions into the [[SparkSession]]. This allows a user to add Analyzer rules,
     * Optimizer rules, Planning Strategies or a customized parser.
     *
     * @since 2.2.0
     */
    def withExtensions(f: SparkSessionExtensions => Unit): Builder = synchronized {
      f(extensions)
      this
    }

    /**
     * Gets an existing [[SparkSession]] or, if there is no existing one, creates a new
     * one based on the options set in this builder.
     *
     * This method first checks whether there is a valid thread-local SparkSession,
     * and if yes, return that one. It then checks whether there is a valid global
     * default SparkSession, and if yes, return that one. If no valid global default
     * SparkSession exists, the method creates a new SparkSession and assigns the
     * newly created SparkSession as the global default.
     *
     * In case an existing SparkSession is returned, the non-static config options specified in
     * this builder will be applied to the existing SparkSession.
     *
     * @since 2.0.0
     */
    def getOrCreate(): SparkSession = synchronized {
      assertOnDriver()
      // Get the session from current thread's active session.
      var session = activeThreadSession.get()
      if ((session ne null) && !session.sparkContext.isStopped) {
        applyModifiableSettings(session)
        return session
      }

      // Global synchronization so we will only set the default session once.
      SparkSession.synchronized {
        // If the current thread does not have an active session, get it from the global session.
        session = defaultSession.get()
        if ((session ne null) && !session.sparkContext.isStopped) {
          applyModifiableSettings(session)
          return session
        }

        // No active nor global default session. Create a new one.
        val sparkContext = userSuppliedContext.getOrElse {
          val sparkConf = new SparkConf()
          options.foreach { case (k, v) => sparkConf.set(k, v) }

          // set a random app name if not given.
          if (!sparkConf.contains("spark.app.name")) {
            sparkConf.setAppName(java.util.UUID.randomUUID().toString)
          }

          SparkContext.getOrCreate(sparkConf)
          // Do not update `SparkConf` for existing `SparkContext`, as it's shared by all sessions.
        }

        // Initialize extensions if the user has defined a configurator class.
        val extensionConfOption = sparkContext.conf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS)
        if (extensionConfOption.isDefined) {
          val extensionConfClassName = extensionConfOption.get
          try {
            val extensionConfClass = Utils.classForName(extensionConfClassName)
            val extensionConf = extensionConfClass.newInstance()
              .asInstanceOf[SparkSessionExtensions => Unit]
            extensionConf(extensions)
          } catch {
            // Ignore the error if we cannot find the class or when the class has the wrong type.
            case e @ (_: ClassCastException |
                      _: ClassNotFoundException |
                      _: NoClassDefFoundError) =>
              logWarning(s"Cannot use $extensionConfClassName to configure session extensions.", e)
          }
        }

        session = new SparkSession(sparkContext, None, None, extensions)
        options.foreach { case (k, v) => session.initialSessionOptions.put(k, v) }
        setDefaultSession(session)
        setActiveSession(session)

        // Register a successfully instantiated context to the singleton. This should be at the
        // end of the class definition so that the singleton is updated only if there is no
        // exception in the construction of the instance.
        sparkContext.addSparkListener(new SparkListener {
          override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {
            defaultSession.set(null)
          }
        })
      }

      return session
    }

    private def applyModifiableSettings(session: SparkSession): Unit = {
      val (staticConfs, otherConfs) =
        options.partition(kv => SQLConf.staticConfKeys.contains(kv._1))

      otherConfs.foreach { case (k, v) => session.sessionState.conf.setConfString(k, v) }

      if (staticConfs.nonEmpty) {
        logWarning("Using an existing SparkSession; the static sql configurations will not take" +
          " effect.")
      }
      if (otherConfs.nonEmpty) {
        logWarning("Using an existing SparkSession; some spark core configurations may not take" +
          " effect.")
      }
    }
  }

  /**
   * Creates a [[SparkSession.Builder]] for constructing a [[SparkSession]].
   *
   * @since 2.0.0
   */
  def builder(): Builder = new Builder

  /**
   * Changes the SparkSession that will be returned in this thread and its children when
   * SparkSession.getOrCreate() is called. This can be used to ensure that a given thread receives
   * a SparkSession with an isolated session, instead of the global (first created) context.
   *
   * @since 2.0.0
   */
  def setActiveSession(session: SparkSession): Unit = {
    activeThreadSession.set(session)
  }

  /**
   * Clears the active SparkSession for current thread. Subsequent calls to getOrCreate will
   * return the first created context instead of a thread-local override.
   *
   * @since 2.0.0
   */
  def clearActiveSession(): Unit = {
    activeThreadSession.remove()
  }

  /**
   * Sets the default SparkSession that is returned by the builder.
   *
   * @since 2.0.0
   */
  def setDefaultSession(session: SparkSession): Unit = {
    defaultSession.set(session)
  }

  /**
   * Clears the default SparkSession that is returned by the builder.
   *
   * @since 2.0.0
   */
  def clearDefaultSession(): Unit = {
    defaultSession.set(null)
  }

  /**
   * Returns the active SparkSession for the current thread, returned by the builder.
   *
   * @note Return None, when calling this function on executors
   *
   * @since 2.2.0
   */
  def getActiveSession: Option[SparkSession] = {
    if (TaskContext.get != null) {
      // Return None when running on executors.
      None
    } else {
      Option(activeThreadSession.get)
    }
  }

  /**
   * Returns the default SparkSession that is returned by the builder.
   *
   * @note Return None, when calling this function on executors
   *
   * @since 2.2.0
   */
  def getDefaultSession: Option[SparkSession] = {
    if (TaskContext.get != null) {
      // Return None when running on executors.
      None
    } else {
      Option(defaultSession.get)
    }
  }

  /**
   * Returns the currently active SparkSession, otherwise the default one. If there is no default
   * SparkSession, throws an exception.
   *
   * @since 2.4.0
   */
  def active: SparkSession = {
    getActiveSession.getOrElse(getDefaultSession.getOrElse(
      throw new IllegalStateException("No active or default Spark session found")))
  }

  ////////////////////////////////////////////////////////////////////////////////////////
  // Private methods from now on
  ////////////////////////////////////////////////////////////////////////////////////////

  /** The active SparkSession for the current thread. */
  private val activeThreadSession = new InheritableThreadLocal[SparkSession]

  /** Reference to the root SparkSession. */
  private val defaultSession = new AtomicReference[SparkSession]

  private val HIVE_SESSION_STATE_BUILDER_CLASS_NAME =
    "org.apache.spark.sql.hive.HiveSessionStateBuilder"

  private def sessionStateClassName(conf: SparkConf): String = {
    conf.get(CATALOG_IMPLEMENTATION) match {
      case "hive" => HIVE_SESSION_STATE_BUILDER_CLASS_NAME
      case "in-memory" => classOf[SessionStateBuilder].getCanonicalName
    }
  }

  private def assertOnDriver(): Unit = {
    if (Utils.isTesting && TaskContext.get != null) {
      // we're accessing it during task execution, fail.
      throw new IllegalStateException(
        "SparkSession should only be created and accessed on the driver.")
    }
  }

  /**
   * Helper method to create an instance of `SessionState` based on `className` from conf.
   * The result is either `SessionState` or a Hive based `SessionState`.
   */
  private def instantiateSessionState(
      className: String,
      sparkSession: SparkSession): SessionState = {
    try {
      // invoke `new [Hive]SessionStateBuilder(SparkSession, Option[SessionState])`
      val clazz = Utils.classForName(className)
      val ctor = clazz.getConstructors.head
      ctor.newInstance(sparkSession, None).asInstanceOf[BaseSessionStateBuilder].build()
    } catch {
      case NonFatal(e) =>
        throw new IllegalArgumentException(s"Error while instantiating '$className':", e)
    }
  }

  /**
   * @return true if Hive classes can be loaded, otherwise false.
   */
  private[spark] def hiveClassesArePresent: Boolean = {
    try {
      Utils.classForName(HIVE_SESSION_STATE_BUILDER_CLASS_NAME)
      Utils.classForName("org.apache.hadoop.hive.conf.HiveConf")
      true
    } catch {
      case _: ClassNotFoundException | _: NoClassDefFoundError => false
    }
  }

  private[spark] def cleanupAnyExistingSession(): Unit = {
    val session = getActiveSession.orElse(getDefaultSession)
    if (session.isDefined) {
      logWarning(
        s"""An existing Spark session exists as the active or default session.
           |This probably means another suite leaked it. Attempting to stop it before continuing.
           |This existing Spark session was created at:
           |
           |${session.get.creationSite.longForm}
           |
         """.stripMargin)
      session.get.stop()
      SparkSession.clearActiveSession()
      SparkSession.clearDefaultSession()
    }
  }
}

[0m2021.03.03 12:25:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:25:47 INFO  time: compiled root in 1.26s[0m
[0m2021.03.03 12:44:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 12:44:48 INFO  time: compiled root in 1.21s[0m
[0m2021.03.03 14:03:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:04:03 INFO  time: compiled root in 4.68s[0m
[0m2021.03.03 14:04:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:04:33 INFO  time: compiled root in 1.28s[0m
[0m2021.03.03 14:05:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:05:17 INFO  time: compiled root in 1.13s[0m
/*                     __                                               *\
**     ________ ___   / /  ___     Scala API                            **
**    / __/ __// _ | / /  / _ |    (c) 2003-2013, LAMP/EPFL             **
**  __\ \/ /__/ __ |/ /__/ __ |    http://scala-lang.org/               **
** /____/\___/_/ |_/____/_/ | |                                         **
**                          |/                                          **
\*                                                                      */

package scala.io

import java.util.Arrays
import java.io.{ InputStream, BufferedReader, InputStreamReader, PushbackReader }
import Source.DefaultBufSize
import scala.collection.{ Iterator, AbstractIterator }
import scala.collection.mutable.ArrayBuffer

/** This object provides convenience methods to create an iterable
 *  representation of a source file.
 *
 *  @author  Burak Emir, Paul Phillips
 */
class BufferedSource(inputStream: InputStream, bufferSize: Int)(implicit val codec: Codec) extends Source {
  def this(inputStream: InputStream)(implicit codec: Codec) = this(inputStream, DefaultBufSize)(codec)
  def reader() = new InputStreamReader(inputStream, codec.decoder)
  def bufferedReader() = new BufferedReader(reader(), bufferSize)

  // The same reader has to be shared between the iterators produced
  // by iter and getLines. This is because calling hasNext can cause a
  // block of data to be read from the stream, which will then be lost
  // to getLines if it creates a new reader, even though next() was
  // never called on the original.
  private var charReaderCreated = false
  private lazy val charReader = {
    charReaderCreated = true
    bufferedReader()
  }

  override lazy val iter = (
    Iterator
    continually (codec wrap charReader.read())
    takeWhile (_ != -1)
    map (_.toChar)
  )

  private def decachedReader: BufferedReader = {
    // Don't want to lose a buffered char sitting in iter either. Yes,
    // this is ridiculous, but if I can't get rid of Source, and all the
    // Iterator bits are designed into Source, and people create Sources
    // in the repl, and the repl calls toString for the result line, and
    // that calls hasNext to find out if they're empty, and that leads
    // to chars being buffered, and no, I don't work here, they left a
    // door unlocked.
    // To avoid inflicting this silliness indiscriminately, we can
    // skip it if the char reader was never created: and almost always
    // it will not have been created, since getLines will be called
    // immediately on the source.
    if (charReaderCreated && iter.hasNext) {
      val pb = new PushbackReader(charReader)
      pb unread iter.next().toInt
      new BufferedReader(pb, bufferSize)
    }
    else charReader
  }


  class BufferedLineIterator extends AbstractIterator[String] with Iterator[String] {
    private val lineReader = decachedReader
    var nextLine: String = null

    override def hasNext = {
      if (nextLine == null)
        nextLine = lineReader.readLine

      nextLine != null
    }
    override def next(): String = {
      val result = {
        if (nextLine == null) lineReader.readLine
        else try nextLine finally nextLine = null
      }
      if (result == null) Iterator.empty.next()
      else result
    }
  }

  override def getLines(): Iterator[String] = new BufferedLineIterator

  /** Efficiently converts the entire remaining input into a string. */
  override def mkString = {
    // Speed up slurping of whole data set in the simplest cases.
    val allReader = decachedReader
    val sb = new StringBuilder
    val buf = new Array[Char](bufferSize)
    var n = 0
    while (n != -1) {
      n = allReader.read(buf)
      if (n>0) sb.appendAll(buf, 0, n)
    }
    sb.result
  }
}

[0m2021.03.03 14:05:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:05:45 INFO  time: compiled root in 1.23s[0m
[0m2021.03.03 14:05:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:06:01 INFO  time: compiled root in 2.15s[0m
[0m2021.03.03 14:12:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:12:01 INFO  time: compiled root in 0.27s[0m
[0m2021.03.03 14:23:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 14:23:36 INFO  time: compiled root in 1.36s[0m
Mar 03, 2021 3:02:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3150
Mar 03, 2021 3:02:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3151
[0m2021.03.03 15:02:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:02:35 INFO  time: compiled root in 4.18s[0m
[0m2021.03.03 15:03:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:03:05 INFO  time: compiled root in 3.93s[0m
[0m2021.03.03 15:03:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:03:44 INFO  time: compiled root in 0.88s[0m
[0m2021.03.03 15:03:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:03:48 INFO  time: compiled root in 2.51s[0m
[0m2021.03.03 15:05:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:05:38 INFO  time: compiled root in 1.34s[0m
[0m2021.03.03 15:07:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:07:20 INFO  time: compiled root in 0.58s[0m
[0m2021.03.03 15:07:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:07:28 INFO  time: compiled root in 1.35s[0m
[0m2021.03.03 15:09:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:09:05 INFO  time: compiled root in 1.16s[0m
[0m2021.03.03 15:21:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:21:32 INFO  time: compiled root in 1.34s[0m
[0m2021.03.03 15:21:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:21:43 INFO  time: compiled root in 1.27s[0m
[0m2021.03.03 15:43:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:43:26 INFO  time: compiled root in 2.44s[0m
Mar 03, 2021 3:43:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4021
[0m2021.03.03 15:44:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:44:15 INFO  time: compiled root in 1.52s[0m
[0m2021.03.03 15:46:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:46:18 INFO  time: compiled root in 4s[0m
[0m2021.03.03 15:46:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:46:34 INFO  time: compiled root in 1.2s[0m
Mar 03, 2021 3:47:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4329
Mar 03, 2021 3:48:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4379
[0m2021.03.03 15:48:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:48:25 INFO  time: compiled root in 1.25s[0m
[0m2021.03.03 16:05:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:05:52 INFO  time: compiled root in 1.19s[0m
[0m2021.03.03 16:08:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:08:19 INFO  time: compiled root in 1.14s[0m
[0m2021.03.03 16:08:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:08:49 INFO  time: compiled root in 1.38s[0m
[0m2021.03.03 16:10:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:10:05 INFO  time: compiled root in 1.2s[0m
[0m2021.03.03 16:10:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:10:13 INFO  time: compiled root in 1.23s[0m
[0m2021.03.03 16:10:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:10:39 INFO  time: compiled root in 1.2s[0m
[0m2021.03.03 16:10:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:10:47 INFO  time: compiled root in 1.24s[0m
[0m2021.03.03 16:12:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:12:29 INFO  time: compiled root in 1.3s[0m
[0m2021.03.03 16:13:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:13:40 INFO  time: compiled root in 1.24s[0m
[0m2021.03.03 16:15:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:15:26 INFO  time: compiled root in 1.3s[0m
[0m2021.03.03 16:16:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:16:42 INFO  time: compiled root in 1.15s[0m
[0m2021.03.03 16:19:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:19:10 INFO  time: compiled root in 1.22s[0m
[0m2021.03.03 16:23:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:23:26 INFO  time: compiled root in 1.25s[0m
[0m2021.03.03 16:27:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:27:03 INFO  time: compiled root in 1.29s[0m
[0m2021.03.03 16:27:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:27:32 INFO  time: compiled root in 1.25s[0m
[0m2021.03.03 16:41:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:41:03 INFO  time: compiled root in 1.32s[0m
[0m2021.03.03 16:41:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:41:07 INFO  time: compiled root in 1.3s[0m
[0m2021.03.03 16:41:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:41:09 INFO  time: compiled root in 1.29s[0m
[0m2021.03.03 16:41:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:41:12 INFO  time: compiled root in 1.42s[0m
[0m2021.03.03 16:43:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:43:34 INFO  time: compiled root in 1.37s[0m
[0m2021.03.03 16:46:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:46:57 INFO  time: compiled root in 1.32s[0m
[0m2021.03.03 16:46:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:47:02 INFO  time: compiled root in 2.56s[0m
[0m2021.03.03 16:48:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:48:59 INFO  time: compiled root in 1.82s[0m
[0m2021.03.03 16:50:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:50:04 INFO  time: compiled root in 1.1s[0m
[0m2021.03.03 16:51:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:51:07 INFO  time: compiled root in 0.31s[0m
[0m2021.03.03 16:51:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:51:16 INFO  time: compiled root in 1.37s[0m
[0m2021.03.03 16:52:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:52:57 INFO  time: compiled root in 1.4s[0m
[0m2021.03.03 16:53:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:53:56 INFO  time: compiled root in 1.45s[0m
[0m2021.03.03 16:55:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:55:57 INFO  time: compiled root in 1.2s[0m
Mar 03, 2021 4:56:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5756
[0m2021.03.03 16:57:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:57:36 INFO  time: compiled root in 1.23s[0m
Mar 03, 2021 4:58:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5858
[0m2021.03.03 16:58:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:58:46 INFO  time: compiled root in 1.27s[0m
Mar 03, 2021 4:59:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5873
[0m2021.03.03 16:59:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:59:37 INFO  time: compiled root in 1.22s[0m
[0m2021.03.03 16:59:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:59:48 INFO  time: compiled root in 1.15s[0m
Mar 03, 2021 5:00:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5910
[0m2021.03.03 17:00:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:01:01 INFO  time: compiled root in 1.33s[0m
Mar 03, 2021 5:02:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5932
Mar 03, 2021 5:03:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5938
Mar 03, 2021 5:04:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5945
Mar 03, 2021 5:04:20 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5951
[0m2021.03.03 17:04:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:04:36 INFO  time: compiled root in 1.12s[0m
[0m2021.03.03 17:05:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:05:49 INFO  time: compiled root in 1.19s[0m
Mar 03, 2021 5:06:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5990
Mar 03, 2021 5:07:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5996
Mar 03, 2021 5:07:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6002
Mar 03, 2021 5:08:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6008
[0m2021.03.03 17:08:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:08:58 INFO  time: compiled root in 1.19s[0m
Mar 03, 2021 5:09:48 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6089
[0m2021.03.03 17:09:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:09:56 INFO  time: compiled root in 1.15s[0m
Mar 03, 2021 5:09:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6108
[0m2021.03.03 17:11:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:11:14 INFO  time: compiled root in 1.16s[0m
[0m2021.03.03 17:11:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:11:59 INFO  time: compiled root in 1.22s[0m
[0m2021.03.03 17:12:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:12:51 INFO  time: compiled root in 1.25s[0m
Mar 03, 2021 5:14:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6340
[0m2021.03.03 17:14:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:14:23 INFO  time: compiled root in 1.25s[0m
Mar 03, 2021 5:15:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6366
[0m2021.03.03 17:15:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:15:43 INFO  time: compiled root in 1.26s[0m
[0m2021.03.03 17:15:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:15:48 INFO  time: compiled root in 1.21s[0m
[0m2021.03.03 17:15:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:15:54 INFO  time: compiled root in 1.25s[0m
[0m2021.03.03 17:16:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:00 INFO  time: compiled root in 1.25s[0m
[0m2021.03.03 17:18:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:18:13 INFO  time: compiled root in 1.23s[0m
[0m2021.03.03 17:18:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:18:21 INFO  time: compiled root in 1.22s[0m
[0m2021.03.03 17:19:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:19:11 INFO  time: compiled root in 0.28s[0m
[0m2021.03.03 17:19:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:19:16 INFO  time: compiled root in 1.23s[0m
[0m2021.03.03 17:20:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:20:35 INFO  time: compiled root in 1.25s[0m
Mar 03, 2021 5:20:39 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6531
Mar 03, 2021 5:20:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6537
[0m2021.03.03 17:22:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:22:45 INFO  time: compiled root in 1.27s[0m
[0m2021.03.03 17:23:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:23:40 INFO  time: compiled root in 2.12s[0m
[0m2021.03.03 17:24:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:24:30 INFO  time: compiled root in 2.28s[0m
[0m2021.03.03 17:25:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:25:45 INFO  time: compiled root in 1.21s[0m
[0m2021.03.03 17:26:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:26:44 INFO  time: compiled root in 1.23s[0m
[0m2021.03.03 17:28:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:28:11 INFO  time: compiled root in 1.13s[0m
[0m2021.03.03 17:28:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:28:42 INFO  time: compiled root in 1.15s[0m
[0m2021.03.03 17:29:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:29:49 INFO  time: compiled root in 1.25s[0m
[0m2021.03.03 17:30:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:30:35 INFO  time: compiled root in 1.12s[0m
[0m2021.03.03 17:32:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:32:12 INFO  time: compiled root in 1.13s[0m
[0m2021.03.03 17:33:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:33:08 INFO  time: compiled root in 1.22s[0m
[0m2021.03.03 17:33:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:33:22 INFO  time: compiled root in 1.16s[0m
[0m2021.03.03 17:37:38 INFO  shutting down Metals[0m
[0m2021.03.03 17:37:38 INFO  Shut down connection with build server.[0m
[0m2021.03.03 17:37:38 INFO  Shut down connection with build server.[0m
[0m2021.03.03 17:37:38 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.04 09:48:12 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.53.2.[0m
[0m2021.03.04 09:48:13 INFO  time: initialize in 0.48s[0m
[0m2021.03.04 09:48:13 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.04 09:48:13 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4986172226761941597/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.04 09:48:13 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.03.04 09:48:16 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .show()

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warc: String, 
    warcTpye: String, 
    warcTargetUri: String, 
    warcDate: String, 
    warcRecordID: String,
    warcRefersTo: String,
    warcBlockDigest: String,
    warcIdentifiedContentLang: String,
    contentType: String, 
    contentLength: Int,
    content: String
    ) {}

}
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root-test' from '/home/delaneylekien/project3/scalas3read/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/delaneylekien/project3/scalas3read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4986172226761941597/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4986172226761941597/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 09:48:19 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .show()

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warc: String, 
    warcTpye: String, 
    warcTargetUri: String, 
    warcDate: String, 
    warcRecordID: String,
    warcRefersTo: String,
    warcBlockDigest: String,
    warcIdentifiedContentLang: String,
    contentType: String, 
    contentLength: Int,
    content: String
    ) {}

}
[0m2021.03.04 09:48:19 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher934900876309253592/bsp.socket'...
[0mWaiting for the bsp connection to come up...2021.03.04 09:48:19
 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6093073443015322195/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher934900876309253592/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher934900876309253592/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6093073443015322195/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6093073443015322195/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 09:48:19 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 09:48:19 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 09:48:19 INFO  time: Connected to build server in 5.92s[0m
[0m2021.03.04 09:48:19 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.04 09:48:19 INFO  time: code lens generation in 5.59s[0m
[0m2021.03.04 09:48:20 INFO  time: Imported build in 0.34s[0m
[0m2021.03.04 09:48:22 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.04 09:48:22 INFO  time: indexed workspace in 3.41s[0m
Mar 04, 2021 9:53:20 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 51
[0m2021.03.04 09:53:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 09:53:47 INFO  time: compiled root in 2.87s[0m
[0m2021.03.04 09:53:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 09:53:47 INFO  time: compiled root in 0.72s[0m
[0m2021.03.04 09:53:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 09:53:58 INFO  time: compiled root in 4.85s[0m
[0m2021.03.04 09:54:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 09:54:03 INFO  time: compiled root in 2.61s[0m
[0m2021.03.04 09:54:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 09:54:14 INFO  time: compiled root in 1.75s[0m
[0m2021.03.04 10:01:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:01:17 INFO  time: compiled root in 0.41s[0m
[0m2021.03.04 10:01:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:01:36 INFO  time: compiled root in 0.17s[0m
[0m2021.03.04 10:01:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:01:38 INFO  time: compiled root in 0.24s[0m
[0m2021.03.04 10:01:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:01:45 INFO  time: compiled root in 0.35s[0m
[0m2021.03.04 10:05:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:05:29 INFO  time: compiled root in 1.63s[0m
[0m2021.03.04 10:05:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:05:50 INFO  time: compiled root in 1.5s[0m
[0m2021.03.04 10:06:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:06:52 INFO  time: compiled root in 1.54s[0m
[0m2021.03.04 10:08:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:08:32 INFO  time: compiled root in 0.37s[0m
[0m2021.03.04 10:08:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:08:36 INFO  time: compiled root in 0.33s[0m
[0m2021.03.04 10:09:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:09:24 INFO  time: compiled root in 0.34s[0m
[0m2021.03.04 10:09:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:09:29 INFO  time: compiled root in 1.55s[0m
[0m2021.03.04 10:09:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:09:39 INFO  time: compiled root in 1.36s[0m
[0m2021.03.04 10:09:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:09:47 INFO  time: compiled root in 1.26s[0m
[0m2021.03.04 10:10:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:10:21 INFO  time: compiled root in 1.91s[0m
[0m2021.03.04 10:16:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:16:51 INFO  time: compiled root in 0.3s[0m
Mar 04, 2021 10:18:27 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1300
[0m2021.03.04 10:18:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:18:50 INFO  time: compiled root in 1.31s[0m
[0m2021.03.04 10:19:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:19:53 INFO  time: compiled root in 1.31s[0m
[0m2021.03.04 10:32:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:32:19 INFO  time: compiled root in 1.25s[0m
[0m2021.03.04 10:32:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:32:51 INFO  time: compiled root in 0.27s[0m
[0m2021.03.04 10:32:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:33:00 INFO  time: compiled root in 1.31s[0m
[0m2021.03.04 10:33:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:33:05 INFO  time: compiled root in 1.38s[0m
[0m2021.03.04 10:33:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:33:56 INFO  time: compiled root in 1.23s[0m
[0m2021.03.04 10:34:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:34:00 INFO  time: compiled root in 0.25s[0m
[0m2021.03.04 10:34:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:34:24 INFO  time: compiled root in 0.25s[0m
[0m2021.03.04 10:36:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:36:23 INFO  time: compiled root in 1.19s[0m
[0m2021.03.04 10:37:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:37:38 INFO  time: compiled root in 1.15s[0m
[0m2021.03.04 10:59:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:59:26 INFO  time: compiled root in 0.29s[0m
[0m2021.03.04 10:59:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 10:59:32 INFO  time: compiled root in 0.28s[0m
[0m2021.03.04 11:02:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:02:15 INFO  time: compiled root in 0.29s[0m
[0m2021.03.04 11:02:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:02:17 INFO  time: compiled root in 0.34s[0m
[0m2021.03.04 11:02:42 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:75:24: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameters colName, col.
> commonCrawl
>       .withColumn()[0m
[0m2021.03.04 11:02:42 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:75:24: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameters colName, col.
> commonCrawl
>       .withColumn()[0m
[0m2021.03.04 11:03:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:03:12 INFO  time: compiled root in 0.27s[0m
[0m2021.03.04 11:03:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:03:21 INFO  time: compiled root in 1.17s[0m
[0m2021.03.04 11:04:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:04:29 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 11:04:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:04:48 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 11:05:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:05:17 INFO  time: compiled root in 0.13s[0m
[0m2021.03.04 11:05:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:05:35 INFO  time: compiled root in 0.16s[0m
[0m2021.03.04 11:05:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:05:45 INFO  time: compiled root in 1.17s[0m
[0m2021.03.04 11:06:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:06:12 INFO  time: compiled root in 1.21s[0m
Mar 04, 2021 11:06:18 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2928
[0m2021.03.04 11:06:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:06:22 INFO  time: compiled root in 1.27s[0m
[0m2021.03.04 11:07:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:07:27 INFO  time: compiled root in 1.73s[0m
[0m2021.03.04 11:10:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:10:54 INFO  time: compiled root in 1.18s[0m
[0m2021.03.04 11:12:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:12:23 INFO  time: compiled root in 1.11s[0m
[0m2021.03.04 11:14:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:14:42 INFO  time: compiled root in 1.23s[0m
[0m2021.03.04 11:15:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:15:04 INFO  time: compiled root in 1.4s[0m
[0m2021.03.04 11:17:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:17:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:76:45: stale bloop error: unclosed string literal
      .withColumn("_tmp", split($"cut WET", "\n\n))
                                            ^[0m
[0m2021.03.04 11:17:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:76:45: stale bloop error: unclosed string literal
      .withColumn("_tmp", split($"cut WET", "\n\n))
                                            ^[0m
[0m2021.03.04 11:17:23 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:114:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 11:17:23 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 11:17:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:76:45: stale bloop error: unclosed string literal
      .withColumn("_tmp", split($"cut WET", "\n\n))
                                            ^[0m
[0m2021.03.04 11:17:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:114:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 11:17:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:76:45: stale bloop error: unclosed string literal
      .withColumn("_tmp", split($"cut WET", "\n\n))
                                            ^[0m
[0m2021.03.04 11:17:26 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:114:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 11:17:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:17:28 INFO  time: compiled root in 1.17s[0m
[0m2021.03.04 11:18:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:18:32 INFO  time: compiled root in 1.3s[0m
[0m2021.03.04 11:20:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:20:27 INFO  time: compiled root in 1.36s[0m
[0m2021.03.04 11:21:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:21:50 INFO  time: compiled root in 1.13s[0m
[0m2021.03.04 11:23:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:23:47 INFO  time: compiled root in 1.24s[0m
[0m2021.03.04 11:23:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:23:49 INFO  time: compiled root in 1.11s[0m
[0m2021.03.04 11:26:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:26:11 INFO  time: compiled root in 1.14s[0m
[0m2021.03.04 11:26:12 INFO  shutting down Metals[0m
[0m2021.03.04 11:26:12 INFO  Shut down connection with build server.[0m
[0m2021.03.04 11:26:12 INFO  Shut down connection with build server.[0m
[0m2021.03.04 11:26:12 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.04 11:26:24 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.53.2.[0m
[0m2021.03.04 11:26:25 INFO  time: initialize in 0.51s[0m
[0m2021.03.04 11:26:26 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2391245163863187943/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.04 11:26:25 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
[0m2021.03.04 11:26:26 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.03.04 11:26:28 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
      .show(10, false)

    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root-test' from '/home/delaneylekien/project3/scalas3read/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/delaneylekien/project3/scalas3read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2391245163863187943/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2391245163863187943/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 11:26:31 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 11:26:31 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1134147624752695986/bsp.socket'...[0m
2021.03.04 11:26:31 INFO  Attempting to connect to the build server...Waiting for the bsp connection to come up...
[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2843367764616908782/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2843367764616908782/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2843367764616908782/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1134147624752695986/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1134147624752695986/bsp.socket...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 11:26:31 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 11:26:31 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 11:26:31 INFO  time: Connected to build server in 5.63s[0m
[0m2021.03.04 11:26:31 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.04 11:26:31 INFO  time: Imported build in 0.35s[0m
[0m2021.03.04 11:26:32 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
      .show(10, false)

    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
      .show(10, false)

    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
[0m2021.03.04 11:26:32 INFO  time: code lens generation in 1.77s[0m
[0m2021.03.04 11:26:33 INFO  time: code lens generation in 2.94s[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
      .show(10, false)

    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
[0m2021.03.04 11:26:33 INFO  time: code lens generation in 3.25s[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
      .show(10, false)

    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
[0m2021.03.04 11:26:33 INFO  time: code lens generation in 4.19s[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
      .show(10, false)

    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
[0m2021.03.04 11:26:33 INFO  time: code lens generation in 4.56s[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
      .show(10, false)

    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
[0m2021.03.04 11:26:33 INFO  time: code lens generation in 4.67s[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
      .show(10, false)

    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
[0m2021.03.04 11:26:33 INFO  time: code lens generation in 6.27s[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
      .show(10, false)

    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
[0m2021.03.04 11:26:33 INFO  time: code lens generation in 7s[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
      .show(10, false)

    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
      .show(10, false)

    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
      .show(10, false)

    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
[0m2021.03.04 11:26:33 INFO  time: code lens generation in 1.39s[0m
[0m2021.03.04 11:26:36 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.04 11:26:36 INFO  time: indexed workspace in 5.09s[0m
Mar 04, 2021 11:26:41 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 50
Mar 04, 2021 11:26:44 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 64
[0m2021.03.04 11:26:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:26:59 INFO  time: compiled root in 8.34s[0m
Mar 04, 2021 11:27:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 119
Mar 04, 2021 11:27:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 121
Mar 04, 2021 11:27:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 123
Mar 04, 2021 11:27:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 125
Mar 04, 2021 11:27:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 122
Mar 04, 2021 11:27:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 124
Mar 04, 2021 11:27:00 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 126
[0m2021.03.04 11:27:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:27:56 INFO  time: compiled root in 1.04s[0m
[0m2021.03.04 11:28:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:28:05 INFO  time: compiled root in 2.46s[0m
[0m2021.03.04 11:28:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:28:22 INFO  time: compiled root in 1.54s[0m
[0m2021.03.04 11:28:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:28:25 INFO  time: compiled root in 1.53s[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.04 11:29:54 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.53.2.[0m
[0m2021.03.04 11:29:54 INFO  time: initialize in 0.63s[0m
[0m2021.03.04 11:29:58 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7722908738797070551/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.04 11:29:58 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
[0m2021.03.04 11:29:59 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.03.04 11:30:01 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
[0m2021.03.04 11:30:02 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.53.2.[0m
[0m2021.03.04 11:30:03 INFO  time: initialize in 0.61s[0m
Waiting for the bsp connection to come up...
[0m2021.03.04 11:30:03 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8951987607049527752/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.04 11:30:03 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
[0m2021.03.04 11:30:04 INFO  skipping build import with status 'Installed'[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
  
    cuttingCrawl
      .filter($"WARC Header".contains("job"))
      .show(10, false)
    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Loading previous analysis for 'root-test' from '/home/delaneylekien/project3/scalas3read/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/delaneylekien/project3/scalas3read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7722908738797070551/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7722908738797070551/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root-test' from '/home/delaneylekien/project3/scalas3read/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/delaneylekien/project3/scalas3read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8951987607049527752/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8951987607049527752/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 11:30:04 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 11:30:04 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 11:30:04 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.04 11:30:04 INFO  Attempting to connect to the build server...[0mOpening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7681761507503486900/bsp.socket'...

Waiting for the bsp connection to come up...
[0m2021.03.04 11:30:04 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5380652262927728287/bsp.socket'...
[0m2021.03.04 11:30:04 Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5803006746139865266/bsp.socket'...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3986358340414877732/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5803006746139865266/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5803006746139865266/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5380652262927728287/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5380652262927728287/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 11:30:05 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 11:30:05 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7681761507503486900/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7681761507503486900/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 11:30:05 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3986358340414877732/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3986358340414877732/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 11:30:05 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 11:30:05 INFO  time: Connected to build server in 1.83s[0m
[0m2021.03.04 11:30:05 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.04 11:30:05 INFO  time: Connected to build server in 6.86s[0m
[0m2021.03.04 11:30:05 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.04 11:30:05 INFO  time: Imported build in 0.21s[0m
[0m2021.03.04 11:30:06 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
[0m2021.03.04 11:30:06 INFO  time: Imported build in 0.11s[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
  
    cuttingCrawl
      .filter($"WARC Header".contains("job"))
      .show(10, false)
    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
[0m2021.03.04 11:30:09 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.04 11:30:10 INFO  time: indexed workspace in 5.13s[0m
[0m2021.03.04 11:30:11 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
  
    cuttingCrawl
      .filter($"WARC Header".contains("job"))
      .show(10, false)
    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
[0m2021.03.04 11:30:12 INFO  time: code lens generation in 8.62s[0m
[0m2021.03.04 11:30:13 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.04 11:30:14 INFO  time: indexed workspace in 8.39s[0m
[0m2021.03.04 11:30:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:30:35 INFO  time: compiled root in 7.56s[0m
[0m2021.03.04 11:31:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:31:54 INFO  time: compiled root in 2.43s[0m
[0m2021.03.04 11:33:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:33:39 INFO  time: compiled root in 2.6s[0m
[0m2021.03.04 11:34:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:34:04 INFO  time: compiled root in 2.52s[0m
[0m2021.03.04 11:34:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:34:15 INFO  time: compiled root in 1.79s[0m
[0m2021.03.04 11:34:55 INFO  shutting down Metals[0m
[0m2021.03.04 11:34:55 INFO  Shut down connection with build server.[0m
[0m2021.03.04 11:34:55 INFO  Shut down connection with build server.[0m
[0m2021.03.04 11:34:55 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.04 11:35:44 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.53.2.[0m
[0m2021.03.04 11:35:45 INFO  time: initialize in 0.73s[0m
[0m2021.03.04 11:35:46 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8847173524377034423/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.04 11:35:45 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
[0m2021.03.04 11:35:46 INFO  skipping build import with status 'Installed'[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8847173524377034423/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8847173524377034423/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 11:35:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 11:35:46 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.04 11:35:46 INFO  Attempting to connect to the build server...[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1461384036412096383/bsp.socket'...
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4403358220051999336/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1461384036412096383/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1461384036412096383/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4403358220051999336/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4403358220051999336/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
[0m2021.03.04 11:35:46Starting thread that pumps server stdout and redirects it to the client stdout... 
INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 11:35:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 11:35:46 INFO  time: Connected to build server in 0.8s[0m
[0m2021.03.04 11:35:46 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.04 11:35:48 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
[0m2021.03.04 11:35:48 INFO  time: Imported build in 0.12s[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
  
    cuttingCrawl
      .filter($"WARC Header".contains("job"))
      .select($"Plain Text")
      .show(5, false)
    
    
    

    

    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class wet(
    warcContent: String
    ) {}

}
[0m2021.03.04 11:35:54 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.04 11:35:54 INFO  time: indexed workspace in 6.94s[0m
[0m2021.03.04 11:36:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:36:48 INFO  time: compiled root in 0.51s[0m
[0m2021.03.04 11:36:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:36:58 INFO  time: compiled root in 1.82s[0m
[0m2021.03.04 11:37:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:37:06 INFO  time: compiled root in 1.68s[0m
[0m2021.03.04 11:37:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:37:10 INFO  time: compiled root in 1.69s[0m
[0m2021.03.04 11:38:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:38:45 INFO  time: compiled root in 1.51s[0m
[0m2021.03.04 11:38:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:38:51 INFO  time: compiled root in 2.01s[0m
[0m2021.03.04 11:39:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:39:05 INFO  time: compiled root in 1.38s[0m
Mar 04, 2021 11:40:23 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 148
Mar 04, 2021 11:40:23 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 150
[0m2021.03.04 11:40:38 INFO  shutting down Metals[0m
[0m2021.03.04 11:40:38 INFO  Shut down connection with build server.[0m
[0m2021.03.04 11:40:38 INFO  Shut down connection with build server.[0m
[0m2021.03.04 11:40:38 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.04 11:40:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:40:49 INFO  time: compiled root in 1.42s[0m
[0m2021.03.04 11:51:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:51:41 INFO  time: compiled root in 1.3s[0m
[0m2021.03.04 11:54:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:54:19 INFO  time: compiled root in 1.45s[0m
[0m2021.03.04 11:59:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:59:41 INFO  time: compiled root in 0.29s[0m
[0m2021.03.04 11:59:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 11:59:56 INFO  time: compiled root in 1.36s[0m
[0m2021.03.04 12:48:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:48:10 INFO  time: compiled root in 3.79s[0m
[0m2021.03.04 12:48:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:48:13 INFO  time: compiled root in 3.05s[0m
[0m2021.03.04 14:02:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:02:26 INFO  time: compiled root in 3.08s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.JavaConverters._
import scala.language.implicitConversions

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.encoders.{encoderFor, ExpressionEncoder}
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression
import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
import org.apache.spark.sql.catalyst.util.toPrettySQL
import org.apache.spark.sql.execution.aggregate.TypedAggregateExpression
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.lit
import org.apache.spark.sql.types._

private[sql] object Column {

  def apply(colName: String): Column = new Column(colName)

  def apply(expr: Expression): Column = new Column(expr)

  def unapply(col: Column): Option[Expression] = Some(col.expr)

  private[sql] def generateAlias(e: Expression): String = {
    e match {
      case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
        a.aggregateFunction.toString
      case expr => toPrettySQL(expr)
    }
  }
}

/**
 * A [[Column]] where an [[Encoder]] has been given for the expected input and return type.
 * To create a [[TypedColumn]], use the `as` function on a [[Column]].
 *
 * @tparam T The input type expected for this expression.  Can be `Any` if the expression is type
 *           checked by the analyzer instead of the compiler (i.e. `expr("sum(...)")`).
 * @tparam U The output type of this column.
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class TypedColumn[-T, U](
    expr: Expression,
    private[sql] val encoder: ExpressionEncoder[U])
  extends Column(expr) {

  /**
   * Inserts the specific input type and schema into any expressions that are expected to operate
   * on a decoded object.
   */
  private[sql] def withInputType(
      inputEncoder: ExpressionEncoder[_],
      inputAttributes: Seq[Attribute]): TypedColumn[T, U] = {
    val unresolvedDeserializer = UnresolvedDeserializer(inputEncoder.deserializer, inputAttributes)
    val newExpr = expr transform {
      case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty =>
        ta.withInputInfo(
          deser = unresolvedDeserializer,
          cls = inputEncoder.clsTag.runtimeClass,
          schema = inputEncoder.schema)
    }
    new TypedColumn[T, U](newExpr, encoder)
  }

  /**
   * Gives the [[TypedColumn]] a name (alias).
   * If the current `TypedColumn` has metadata associated with it, this metadata will be propagated
   * to the new column.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  override def name(alias: String): TypedColumn[T, U] =
    new TypedColumn[T, U](super.name(alias).expr, encoder)

}

/**
 * A column that will be computed based on the data in a `DataFrame`.
 *
 * A new column can be constructed based on the input columns present in a DataFrame:
 *
 * {{{
 *   df("columnName")            // On a specific `df` DataFrame.
 *   col("columnName")           // A generic column not yet associated with a DataFrame.
 *   col("columnName.field")     // Extracting a struct field
 *   col("`a.column.with.dots`") // Escape `.` in column names.
 *   $"columnName"               // Scala short hand for a named column.
 * }}}
 *
 * [[Column]] objects can be composed to form complex expressions:
 *
 * {{{
 *   $"a" + 1
 *   $"a" === $"b"
 * }}}
 *
 * @note The internal Catalyst expression can be accessed via [[expr]], but this method is for
 * debugging purposes only and can change in any future Spark releases.
 *
 * @groupname java_expr_ops Java-specific expression operators
 * @groupname expr_ops Expression operators
 * @groupname df_ops DataFrame functions
 * @groupname Ungrouped Support functions for DataFrames
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class Column(val expr: Expression) extends Logging {

  def this(name: String) = this(name match {
    case "*" => UnresolvedStar(None)
    case _ if name.endsWith(".*") =>
      val parts = UnresolvedAttribute.parseAttributeName(name.substring(0, name.length - 2))
      UnresolvedStar(Some(parts))
    case _ => UnresolvedAttribute.quotedString(name)
  })

  override def toString: String = toPrettySQL(expr)

  override def equals(that: Any): Boolean = that match {
    case that: Column => that.expr.equals(this.expr)
    case _ => false
  }

  override def hashCode: Int = this.expr.hashCode()

  /** Creates a column based on the given expression. */
  private def withExpr(newExpr: Expression): Column = new Column(newExpr)

  /**
   * Returns the expression for this column either with an existing or auto assigned name.
   */
  private[sql] def named: NamedExpression = expr match {
    // Wrap UnresolvedAttribute with UnresolvedAlias, as when we resolve UnresolvedAttribute, we
    // will remove intermediate Alias for ExtractValue chain, and we need to alias it again to
    // make it a NamedExpression.
    case u: UnresolvedAttribute => UnresolvedAlias(u)

    case u: UnresolvedExtractValue => UnresolvedAlias(u)

    case expr: NamedExpression => expr

    // Leave an unaliased generator with an empty list of names since the analyzer will generate
    // the correct defaults after the nested expression's type has been resolved.
    case g: Generator => MultiAlias(g, Nil)

    case func: UnresolvedFunction => UnresolvedAlias(func, Some(Column.generateAlias))

    // If we have a top level Cast, there is a chance to give it a better alias, if there is a
    // NamedExpression under this Cast.
    case c: Cast =>
      c.transformUp {
        case c @ Cast(_: NamedExpression, _, _) => UnresolvedAlias(c)
      } match {
        case ne: NamedExpression => ne
        case _ => Alias(expr, toPrettySQL(expr))()
      }

    case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
      UnresolvedAlias(a, Some(Column.generateAlias))

    // Wait until the struct is resolved. This will generate a nicer looking alias.
    case struct: CreateNamedStructLike => UnresolvedAlias(struct)

    case expr: Expression => Alias(expr, toPrettySQL(expr))()
  }

  /**
   * Provides a type hint about the expected return value of this column.  This information can
   * be used by operations such as `select` on a [[Dataset]] to automatically convert the
   * results into the correct JVM types.
   * @since 1.6.0
   */
  def as[U : Encoder]: TypedColumn[Any, U] = new TypedColumn[Any, U](expr, encoderFor[U])

  /**
   * Extracts a value or values from a complex type.
   * The following types of extraction are supported:
   * <ul>
   * <li>Given an Array, an integer ordinal can be used to retrieve a single value.</li>
   * <li>Given a Map, a key of the correct type can be used to retrieve an individual value.</li>
   * <li>Given a Struct, a string fieldName can be used to extract that field.</li>
   * <li>Given an Array of Structs, a string fieldName can be used to extract filed
   *    of every struct in that array, and return an Array of fields.</li>
   * </ul>
   * @group expr_ops
   * @since 1.4.0
   */
  def apply(extraction: Any): Column = withExpr {
    UnresolvedExtractValue(expr, lit(extraction).expr)
  }

  /**
   * Unary minus, i.e. negate the expression.
   * {{{
   *   // Scala: select the amount column and negates all values.
   *   df.select( -df("amount") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.select( negate(col("amount") );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_- : Column = withExpr { UnaryMinus(expr) }

  /**
   * Inversion of boolean expression, i.e. NOT.
   * {{{
   *   // Scala: select rows that are not active (isActive === false)
   *   df.filter( !df("isActive") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( not(df.col("isActive")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_! : Column = withExpr { Not(expr) }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def === (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} = $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualTo(expr, right)
  }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def equalTo(other: Any): Column = this === other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") =!= df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
    */
  def =!= (other: Any): Column = withExpr{ Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
    */
  @deprecated("!== does not have the same precedence as ===, use =!= instead", "2.0.0")
  def !== (other: Any): Column = this =!= other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def notEqual(other: Any): Column = withExpr { Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > 21 )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def > (other: Any): Column = withExpr { GreaterThan(expr, lit(other).expr) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > lit(21) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def gt(other: Any): Column = this > other

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def < (other: Any): Column = withExpr { LessThan(expr, lit(other).expr) }

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def lt(other: Any): Column = this < other

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <= (other: Any): Column = withExpr { LessThanOrEqual(expr, lit(other).expr) }

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def leq(other: Any): Column = this <= other

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def >= (other: Any): Column = withExpr { GreaterThanOrEqual(expr, lit(other).expr) }

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def geq(other: Any): Column = this >= other

  /**
   * Equality test that is safe for null values.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <=> (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} <=> $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualNullSafe(expr, right)
  }

  /**
   * Equality test that is safe for null values.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def eqNullSafe(other: Any): Column = this <=> other

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def when(condition: Column, value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches :+ ((condition.expr, lit(value).expr))) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "when() cannot be applied once otherwise() is applied")
    case _ =>
      throw new IllegalArgumentException(
        "when() can only be applied on a Column previously generated by when() function")
  }

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def otherwise(value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches, Option(lit(value).expr)) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied once on a Column previously generated by when()")
    case _ =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied on a Column previously generated by when()")
  }

  /**
   * True if the current column is between the lower bound and upper bound, inclusive.
   *
   * @group java_expr_ops
   * @since 1.4.0
   */
  def between(lowerBound: Any, upperBound: Any): Column = {
    (this >= lowerBound) && (this <= upperBound)
  }

  /**
   * True if the current expression is NaN.
   *
   * @group expr_ops
   * @since 1.5.0
   */
  def isNaN: Column = withExpr { IsNaN(expr) }

  /**
   * True if the current expression is null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNull: Column = withExpr { IsNull(expr) }

  /**
   * True if the current expression is NOT null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNotNull: Column = withExpr { IsNotNull(expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def || (other: Any): Column = withExpr { Or(expr, lit(other).expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def or(other: Column): Column = this || other

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def && (other: Any): Column = withExpr { And(expr, lit(other).expr) }

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def and(other: Column): Column = this && other

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def + (other: Any): Column = withExpr { Add(expr, lit(other).expr) }

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def plus(other: Any): Column = this + other

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def - (other: Any): Column = withExpr { Subtract(expr, lit(other).expr) }

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def minus(other: Any): Column = this - other

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def * (other: Any): Column = withExpr { Multiply(expr, lit(other).expr) }

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def multiply(other: Any): Column = this * other

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def / (other: Any): Column = withExpr { Divide(expr, lit(other).expr) }

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def divide(other: Any): Column = this / other

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def % (other: Any): Column = withExpr { Remainder(expr, lit(other).expr) }

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def mod(other: Any): Column = this % other

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the evaluated values of the arguments.
   *
   * Note: Since the type of the elements in the list are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def isin(list: Any*): Column = withExpr { In(expr, list.map(lit(_).expr)) }

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: scala.collection.Iterable[_]): Column = isin(values.toSeq: _*)

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group java_expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: java.lang.Iterable[_]): Column = isInCollection(values.asScala)

  /**
   * SQL like expression. Returns a boolean column based on a SQL LIKE match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def like(literal: String): Column = withExpr { Like(expr, lit(literal).expr) }

  /**
   * SQL RLIKE expression (LIKE with Regex). Returns a boolean column based on a regex
   * match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def rlike(literal: String): Column = withExpr { RLike(expr, lit(literal).expr) }

  /**
   * An expression that gets an item at position `ordinal` out of an array,
   * or gets a value by key `key` in a `MapType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getItem(key: Any): Column = withExpr { UnresolvedExtractValue(expr, Literal(key)) }

  /**
   * An expression that gets a field by name in a `StructType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getField(fieldName: String): Column = withExpr {
    UnresolvedExtractValue(expr, Literal(fieldName))
  }

  /**
   * An expression that returns a substring.
   * @param startPos expression for the starting position.
   * @param len expression for the length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Column, len: Column): Column = withExpr {
    Substring(expr, startPos.expr, len.expr)
  }

  /**
   * An expression that returns a substring.
   * @param startPos starting position.
   * @param len length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Int, len: Int): Column = withExpr {
    Substring(expr, lit(startPos).expr, lit(len).expr)
  }

  /**
   * Contains the other element. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def contains(other: Any): Column = withExpr { Contains(expr, lit(other).expr) }

  /**
   * String starts with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(other: Column): Column = withExpr { StartsWith(expr, lit(other).expr) }

  /**
   * String starts with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(literal: String): Column = this.startsWith(lit(literal))

  /**
   * String ends with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(other: Column): Column = withExpr { EndsWith(expr, lit(other).expr) }

  /**
   * String ends with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(literal: String): Column = this.endsWith(lit(literal))

  /**
   * Gives the column an alias. Same as `as`.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".alias("colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def alias(alias: String): Column = name(alias)

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String): Column = name(alias)

  /**
   * (Scala-specific) Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Seq[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Array[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as('colB))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: Symbol): Column = name(alias.name)

  /**
   * Gives the column an alias with metadata.
   * {{{
   *   val metadata: Metadata = ...
   *   df.select($"colA".as("colB", metadata))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String, metadata: Metadata): Column = withExpr {
    Alias(expr, alias)(explicitMetadata = Some(metadata))
  }

  /**
   * Gives the column a name (alias).
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".name("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def name(alias: String): Column = withExpr {
    expr match {
      case ne: NamedExpression => Alias(expr, alias)(explicitMetadata = Some(ne.metadata))
      case other => Alias(other, alias)()
    }
  }

  /**
   * Casts the column to a different data type.
   * {{{
   *   // Casts colA to IntegerType.
   *   import org.apache.spark.sql.types.IntegerType
   *   df.select(df("colA").cast(IntegerType))
   *
   *   // equivalent to
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: DataType): Column = withExpr { Cast(expr, to) }

  /**
   * Casts the column to a different data type, using the canonical string representation
   * of the type. The supported types are: `string`, `boolean`, `byte`, `short`, `int`, `long`,
   * `float`, `double`, `decimal`, `date`, `timestamp`.
   * {{{
   *   // Casts colA to integer.
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: String): Column = cast(CatalystSqlParser.parseDataType(to))

  /**
   * Returns a sort expression based on the descending order of the column.
   * {{{
   *   // Scala
   *   df.sort(df("age").desc)
   *
   *   // Java
   *   df.sort(df.col("age").desc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def desc: Column = withExpr { SortOrder(expr, Descending) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing first.
   *   df.sort(df("age").desc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_first: Column = withExpr { SortOrder(expr, Descending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing last.
   *   df.sort(df("age").desc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_last: Column = withExpr { SortOrder(expr, Descending, NullsLast, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order.
   *   df.sort(df("age").asc)
   *
   *   // Java
   *   df.sort(df.col("age").asc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def asc: Column = withExpr { SortOrder(expr, Ascending) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values return before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing first.
   *   df.sort(df("age").asc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_first: Column = withExpr { SortOrder(expr, Ascending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing last.
   *   df.sort(df("age").asc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_last: Column = withExpr { SortOrder(expr, Ascending, NullsLast, Set.empty) }

  /**
   * Prints the expression to the console for debugging purposes.
   *
   * @group df_ops
   * @since 1.3.0
   */
  def explain(extended: Boolean): Unit = {
    // scalastyle:off println
    if (extended) {
      println(expr)
    } else {
      println(expr.sql)
    }
    // scalastyle:on println
  }

  /**
   * Compute bitwise OR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseOR(other: Any): Column = withExpr { BitwiseOr(expr, lit(other).expr) }

  /**
   * Compute bitwise AND of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseAND($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseAND(other: Any): Column = withExpr { BitwiseAnd(expr, lit(other).expr) }

  /**
   * Compute bitwise XOR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseXOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseXOR(other: Any): Column = withExpr { BitwiseXor(expr, lit(other).expr) }

  /**
   * Defines a windowing column.
   *
   * {{{
   *   val w = Window.partitionBy("name").orderBy("id")
   *   df.select(
   *     sum("price").over(w.rangeBetween(Window.unboundedPreceding, 2)),
   *     avg("price").over(w.rowsBetween(Window.currentRow, 4))
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def over(window: expressions.WindowSpec): Column = window.withAggregate(this)

  /**
   * Defines an empty analytic clause. In this case the analytic function is applied
   * and presented for all rows in the result set.
   *
   * {{{
   *   df.select(
   *     sum("price").over(),
   *     avg("price").over()
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def over(): Column = over(Window.spec)

}


/**
 * A convenient class used for constructing schema.
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class ColumnName(name: String) extends Column(name) {

  /**
   * Creates a new `StructField` of type boolean.
   * @since 1.3.0
   */
  def boolean: StructField = StructField(name, BooleanType)

  /**
   * Creates a new `StructField` of type byte.
   * @since 1.3.0
   */
  def byte: StructField = StructField(name, ByteType)

  /**
   * Creates a new `StructField` of type short.
   * @since 1.3.0
   */
  def short: StructField = StructField(name, ShortType)

  /**
   * Creates a new `StructField` of type int.
   * @since 1.3.0
   */
  def int: StructField = StructField(name, IntegerType)

  /**
   * Creates a new `StructField` of type long.
   * @since 1.3.0
   */
  def long: StructField = StructField(name, LongType)

  /**
   * Creates a new `StructField` of type float.
   * @since 1.3.0
   */
  def float: StructField = StructField(name, FloatType)

  /**
   * Creates a new `StructField` of type double.
   * @since 1.3.0
   */
  def double: StructField = StructField(name, DoubleType)

  /**
   * Creates a new `StructField` of type string.
   * @since 1.3.0
   */
  def string: StructField = StructField(name, StringType)

  /**
   * Creates a new `StructField` of type date.
   * @since 1.3.0
   */
  def date: StructField = StructField(name, DateType)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal: StructField = StructField(name, DecimalType.USER_DEFAULT)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal(precision: Int, scale: Int): StructField =
    StructField(name, DecimalType(precision, scale))

  /**
   * Creates a new `StructField` of type timestamp.
   * @since 1.3.0
   */
  def timestamp: StructField = StructField(name, TimestampType)

  /**
   * Creates a new `StructField` of type binary.
   * @since 1.3.0
   */
  def binary: StructField = StructField(name, BinaryType)

  /**
   * Creates a new `StructField` of type array.
   * @since 1.3.0
   */
  def array(dataType: DataType): StructField = StructField(name, ArrayType(dataType))

  /**
   * Creates a new `StructField` of type map.
   * @since 1.3.0
   */
  def map(keyType: DataType, valueType: DataType): StructField =
    map(MapType(keyType, valueType))

  def map(mapType: MapType): StructField = StructField(name, mapType)

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(fields: StructField*): StructField = struct(StructType(fields))

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(structType: StructType): StructField = StructField(name, structType)
}

[0m2021.03.04 14:03:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:03:35 INFO  time: compiled root in 1.36s[0m
[0m2021.03.04 14:03:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:03:42 INFO  time: compiled root in 1.39s[0m
[0m2021.03.04 14:03:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:03:49 INFO  time: compiled root in 1.61s[0m
[0m2021.03.04 14:03:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:04:01 INFO  time: compiled root in 2.43s[0m
[0m2021.03.04 14:05:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:05:40 INFO  time: compiled root in 1.25s[0m
[0m2021.03.04 14:13:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:13:54 INFO  time: compiled root in 0.28s[0m
[0m2021.03.04 14:15:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:15:44 INFO  time: compiled root in 0.28s[0m
[0m2021.03.04 14:16:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:16:00 INFO  time: compiled root in 0.28s[0m
[0m2021.03.04 14:17:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:17:13 INFO  time: compiled root in 0.34s[0m
[0m2021.03.04 14:17:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:17:28 INFO  time: compiled root in 0.16s[0m
[0m2021.03.04 14:18:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:18:13 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 14:18:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:18:47 INFO  time: compiled root in 1.24s[0m
[0m2021.03.04 14:19:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:19:01 INFO  time: compiled root in 0.35s[0m
[0m2021.03.04 14:24:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:24:32 INFO  time: compiled root in 0.31s[0m
[0m2021.03.04 14:27:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:27:36 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 14:28:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:28:13 INFO  time: compiled root in 1.24s[0m
[0m2021.03.04 14:28:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:28:34 INFO  time: compiled root in 1.48s[0m
[0m2021.03.04 14:28:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:28:47 INFO  time: compiled root in 0.36s[0m
[0m2021.03.04 14:28:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:28:56 INFO  time: compiled root in 0.3s[0m
Mar 04, 2021 2:30:41 PM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTraceNotification
[0m2021.03.04 14:30:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:30:42 INFO  time: compiled root in 0.36s[0m
Mar 04, 2021 2:30:45 PM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTraceNotification
[0m2021.03.04 14:30:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:30:54 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 14:31:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:31:13 INFO  time: compiled root in 0.33s[0m
[0m2021.03.04 14:31:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:31:15 INFO  time: compiled root in 0.35s[0m
[0m2021.03.04 14:31:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:31:35 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 14:32:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:32:06 INFO  time: compiled root in 0.29s[0m
[0m2021.03.04 14:32:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:32:19 INFO  time: compiled root in 1.28s[0m
[0m2021.03.04 14:33:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:33:18 INFO  time: compiled root in 1.29s[0m
[0m2021.03.04 14:33:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:33:23 INFO  time: compiled root in 1.19s[0m
[0m2021.03.04 14:33:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:33:24 INFO  time: compiled root in 1.16s[0m
[0m2021.03.04 14:34:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:34:38 INFO  time: compiled root in 1.22s[0m
[0m2021.03.04 14:35:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:35:21 INFO  time: compiled root in 1.12s[0m
[0m2021.03.04 14:41:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:41:15 INFO  time: compiled root in 0.25s[0m
[0m2021.03.04 14:41:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:41:46 INFO  time: compiled root in 0.24s[0m
[0m2021.03.04 14:41:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:41:54 INFO  time: compiled root in 1.32s[0m
[0m2021.03.04 14:42:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:42:10 INFO  time: compiled root in 1.09s[0m
[0m2021.03.04 14:43:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:43:24 INFO  time: compiled root in 1.38s[0m
[0m2021.03.04 14:43:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:43:31 INFO  time: compiled root in 1.29s[0m
[0m2021.03.04 14:44:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:44:59 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 14:45:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:45:54 INFO  time: compiled root in 1.34s[0m
[0m2021.03.04 14:46:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:46:04 INFO  time: compiled root in 1.3s[0m
[0m2021.03.04 14:46:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:46:25 INFO  time: compiled root in 1.44s[0m
[0m2021.03.04 14:50:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:50:10 INFO  time: compiled root in 0.67s[0m
[0m2021.03.04 14:50:11 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:128:24: stale bloop error: ':' expected but identifier found.
  case class Test(WARC Header: String, last: String) {}
                       ^[0m
[0m2021.03.04 14:50:11 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:130:1: stale bloop error: identifier expected but '}' found.
}
^[0m
[0m2021.03.04 14:50:11 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:128:24: stale bloop error: ':' expected but identifier found.
  case class Test(WARC Header: String, last: String) {}
                       ^[0m
[0m2021.03.04 14:50:11 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:130:1: stale bloop error: identifier expected but '}' found.
}
^[0m
[0m2021.03.04 14:50:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:50:14 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 14:50:17 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:128:19: stale bloop error: identifier expected but string literal found.
  case class Test("WARC Header": String, last: String) {}
                  ^[0m
[0m2021.03.04 14:50:17 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:130:1: stale bloop error: ':' expected but '}' found.
}
^[0m
[0m2021.03.04 14:50:17 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:128:19: stale bloop error: identifier expected but string literal found.
  case class Test("WARC Header": String, last: String) {}
                  ^[0m
[0m2021.03.04 14:50:17 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:130:1: stale bloop error: ':' expected but '}' found.
}
^[0m
[0m2021.03.04 14:50:17 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:128:19: stale bloop error: identifier expected but string literal found.
  case class Test("WARC Header": String, last: String) {}
                  ^[0m
[0m2021.03.04 14:50:17 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:130:1: stale bloop error: ':' expected but '}' found.
}
^[0m
[0m2021.03.04 14:50:17 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:128:19: stale bloop error: identifier expected but string literal found.
  case class Test("WARC Header": String, last: String) {}
                  ^[0m
[0m2021.03.04 14:50:17 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:130:1: stale bloop error: ':' expected but '}' found.
}
^[0m
[0m2021.03.04 14:50:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:50:19 INFO  time: compiled root in 0.15s[0m
[0m2021.03.04 14:50:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:50:40 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 14:50:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:50:54 INFO  time: compiled root in 0.13s[0m
[0m2021.03.04 14:51:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:51:11 INFO  time: compiled root in 1.92s[0m
[0m2021.03.04 14:51:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:51:16 INFO  time: compiled root in 1.21s[0m
[0m2021.03.04 14:51:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:51:45 INFO  time: compiled root in 1.21s[0m
[0m2021.03.04 14:51:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:51:52 INFO  time: compiled root in 1.25s[0m
[0m2021.03.04 14:52:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:52:46 INFO  time: compiled root in 1.25s[0m
[0m2021.03.04 14:52:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:52:58 INFO  time: compiled root in 1.25s[0m
[0m2021.03.04 14:54:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:54:12 INFO  time: compiled root in 1.24s[0m
[0m2021.03.04 14:55:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:55:12 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 14:55:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:55:22 INFO  time: compiled root in 1.21s[0m
[0m2021.03.04 14:55:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:55:39 INFO  time: compiled root in 1.27s[0m
[0m2021.03.04 14:57:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:57:34 INFO  time: compiled root in 1.18s[0m
[0m2021.03.04 14:57:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:57:49 INFO  time: compiled root in 1.19s[0m
[0m2021.03.04 14:57:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:57:50 INFO  time: compiled root in 1.26s[0m
[0m2021.03.04 14:57:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:57:57 INFO  time: compiled root in 1.25s[0m
[0m2021.03.04 14:58:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:58:13 INFO  time: compiled root in 1.23s[0m
[0m2021.03.04 14:59:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:59:23 INFO  time: compiled root in 1.29s[0m
Mar 04, 2021 3:06:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4157
Mar 04, 2021 3:06:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4159
[0m2021.03.04 15:06:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:06:16 INFO  time: compiled root in 0.27s[0m
[0m2021.03.04 15:06:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:06:22 INFO  time: compiled root in 0.27s[0m
[0m2021.03.04 15:06:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:06:28 INFO  time: compiled root in 1.23s[0m
[0m2021.03.04 15:07:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:07:11 INFO  time: compiled root in 1.26s[0m
[0m2021.03.04 15:07:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:07:29 INFO  time: compiled root in 1.3s[0m
[0m2021.03.04 15:09:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:09:23 INFO  time: compiled root in 1.47s[0m
[0m2021.03.04 15:11:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:11:02 INFO  time: compiled root in 1.19s[0m
[0m2021.03.04 15:11:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:11:06 INFO  time: compiled root in 1.23s[0m
[0m2021.03.04 15:15:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:15:19 INFO  time: compiled root in 0.25s[0m
[0m2021.03.04 15:26:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:26:22 INFO  time: compiled root in 0.29s[0m
[0m2021.03.04 15:26:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:26:34 INFO  time: compiled root in 1.29s[0m
[0m2021.03.04 15:27:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:27:38 INFO  time: compiled root in 1.92s[0m
[0m2021.03.04 15:29:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:29:07 INFO  time: compiled root in 1.85s[0m
[0m2021.03.04 15:31:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:31:51 INFO  time: compiled root in 1.19s[0m
[0m2021.03.04 15:33:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:33:25 INFO  time: compiled root in 0.51s[0m
[0m2021.03.04 15:34:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:34:40 INFO  time: compiled root in 1.16s[0m
[0m2021.03.04 15:34:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:34:44 INFO  time: compiled root in 1.24s[0m
[0m2021.03.04 15:34:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:34:54 INFO  time: compiled root in 1.78s[0m
[0m2021.03.04 15:36:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:36:28 INFO  time: compiled root in 1.22s[0m
[0m2021.03.04 15:37:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:37:40 INFO  time: compiled root in 0.25s[0m
[0m2021.03.04 15:39:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:39:49 INFO  time: compiled root in 0.25s[0m
[0m2021.03.04 15:39:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:39:54 INFO  time: compiled root in 1.29s[0m
[0m2021.03.04 15:40:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:40:03 INFO  time: compiled root in 1.2s[0m
[0m2021.03.04 15:40:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:40:08 INFO  time: compiled root in 1.17s[0m
[0m2021.03.04 15:40:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:40:28 INFO  time: compiled root in 1.39s[0m
[0m2021.03.04 15:41:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:41:33 INFO  time: compiled root in 1.29s[0m
[0m2021.03.04 15:43:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:43:41 INFO  time: compiled root in 0.24s[0m
[0m2021.03.04 15:44:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:44:47 INFO  time: compiled root in 1.38s[0m
[0m2021.03.04 15:47:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:47:57 INFO  time: compiled root in 0.24s[0m
[0m2021.03.04 15:48:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:48:01 INFO  time: compiled root in 1.21s[0m
[0m2021.03.04 15:48:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:48:51 INFO  time: compiled root in 1.24s[0m
[0m2021.03.04 15:49:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:49:19 INFO  time: compiled root in 1.3s[0m
[0m2021.03.04 15:49:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:49:21 INFO  time: compiled root in 1.4s[0m
[0m2021.03.04 15:59:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:59:02 INFO  time: compiled root in 0.23s[0m
[0m2021.03.04 15:59:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:59:07 INFO  time: compiled root in 1.18s[0m
[0m2021.03.04 15:59:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 15:59:18 INFO  time: compiled root in 1.23s[0m
[0m2021.03.04 16:03:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 16:03:15 INFO  time: compiled root in 1.2s[0m
[0m2021.03.04 16:03:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 16:03:21 INFO  time: compiled root in 1.22s[0m
[0m2021.03.04 16:03:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 16:03:37 INFO  time: compiled root in 0.24s[0m
[0m2021.03.04 16:03:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 16:03:49 INFO  time: compiled root in 1.19s[0m
[0m2021.03.04 16:04:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 16:04:05 INFO  time: compiled root in 1.3s[0m
[0m2021.03.04 16:05:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 16:05:21 INFO  time: compiled root in 1.19s[0m
[0m2021.03.04 16:05:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 16:05:51 INFO  time: compiled root in 1.28s[0m
[0m2021.03.04 16:05:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 16:05:52 INFO  time: compiled root in 0.24s[0m
[0m2021.03.04 16:05:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 16:05:54 INFO  time: compiled root in 0.24s[0m
[0m2021.03.04 16:06:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 16:06:17 INFO  time: compiled root in 1.46s[0m
[0m2021.03.04 16:08:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 16:08:22 INFO  time: compiled root in 1.27s[0m
[0m2021.03.04 16:09:41 INFO  shutting down Metals[0m
[0m2021.03.04 16:09:41 INFO  Shut down connection with build server.[0m
[0m2021.03.04 16:09:41 INFO  Shut down connection with build server.[0m
No more data in the server stdin, exiting...
[0m2021.03.04 16:09:41 INFO  Shut down connection with build server.[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.05 09:39:42 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.54.0.[0m
[0m2021.03.05 09:39:43 INFO  time: initialize in 0.58s[0m
[0m2021.03.05 09:39:43 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.05 09:39:43 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2228875026806691625/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.05 09:39:44 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.03.05 09:39:46 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    // val stateList = rawStateList.toDF("State Code", "State Name")
    // stateList.show()

    // Combined the two dataFrames to get state codes assocaited with area name.

    // val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show(51, false)

    // val df = spark.read.load(
    //     "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
  
    val filterCrawl = cuttingCrawl
      .filter($"WARC Header" rlike ".*WARC-Target-URI:.*career.*" 
        or ($"WARC Header" rlike ".*WARC-Target-URI:.*/job.*") 
        or ($"WARC Header" rlike ".*WARC-Target-URI:.*employment.*"))
      .filter($"Plain Text" rlike ".*Frontend.*" 
        or ($"Plain Text" rlike ".*Backendend.*") 
        or ($"Plain Text" rlike ".*Fullstack.*")
        or ($"Plain Text" rlike ".*Cybersecurity.*") 
        or ($"Plain Text" rlike ".*Software.*") 
        or ($"Plain Text" rlike ".*Computer.*"))
      .select($"WARC Header", $"Plain Text")
       

    val sqlCrawl = filterCrawl
    .select((split($"Plain Text", " ").as("Words")))

    sqlCrawl.select("Words").rdd.foreach(println)
  
    

    // .select((split($"Plain Text", " ").as("Words")))
  
    // val test = sqlCrawl.filter(r => (r == "TX" || r == "FL")).groupBy("Words").count()
    // test.collect.foreach(println)


    
    // val test = spark.read.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz").take(1000).foreach(println)
  
    //s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz

    // val sure = commonCrawl.toString()
    // val reader = new BufferedSource(new FileInputStream(sure))

    // reader
    //   .getLines()
    //   .grouped(100)
    //   .map(_.toVector)
    //   .filter(vec => vec(0).contains("WARC/1.0"))
    //   .foreach(println)
      

    
    
    // commonCrawl.take(100).foreach(println)
    // commonCrawl.toDF().printSchema()

    // df.printSchema()
    // df
    // .select("url_host_name", "url_host_tld", "warc_record_offset")
    // .filter($"crawl" === "CC-MAIN-2021-04")
    // .filter($"subset" === "warc")
    // .filter($"url_host_tld" === "us")
    // .filter($"url_path".contains("job"))
    // .show(100, false)

    
  }

  case class Test(first: String, last: String) {}

}
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root-test' from '/home/delaneylekien/project3/scalas3read/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/delaneylekien/project3/scalas3read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2228875026806691625/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2228875026806691625/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 09:39:49 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 09:39:49 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4210077265211327603/bsp.socket'...
[0m2021.03.05 09:39:49 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8860752075590482799/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8860752075590482799/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8860752075590482799/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 09:39:50 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4210077265211327603/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4210077265211327603/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 09:39:50 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 09:39:50 INFO  time: Connected to build server in 6.82s[0m
[0m2021.03.05 09:39:50 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.05 09:39:51 INFO  time: Imported build in 0.49s[0m
[0m2021.03.05 09:39:54 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.05 09:39:55 INFO  time: indexed workspace in 4.64s[0m
[0m2021.03.05 09:43:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:43:18 INFO  time: compiled root in 6.72s[0m
[0m2021.03.05 09:47:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:47:29 INFO  time: compiled root in 0.93s[0m
[0m2021.03.05 09:47:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:47:47 INFO  time: compiled root in 2.49s[0m
[0m2021.03.05 09:48:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:48:26 INFO  time: compiled root in 1.61s[0m
[0m2021.03.05 09:52:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:52:33 INFO  time: compiled root in 0.63s[0m
[0m2021.03.05 09:52:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:52:49 INFO  time: compiled root in 0.36s[0m
[0m2021.03.05 09:53:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:53:50 INFO  time: compiled root in 1.37s[0m
[0m2021.03.05 09:54:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:54:45 INFO  time: compiled root in 1.64s[0m
Mar 05, 2021 9:55:08 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 648
[0m2021.03.05 09:55:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:55:22 INFO  time: compiled root in 0.31s[0m
[0m2021.03.05 09:55:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:55:31 INFO  time: compiled root in 0.3s[0m
[0m2021.03.05 09:55:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:55:39 INFO  time: compiled root in 0.31s[0m
[0m2021.03.05 09:55:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:55:57 INFO  time: compiled root in 0.16s[0m
Mar 05, 2021 9:56:06 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
java.util.concurrent.CompletionException: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:673)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:42)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
	at scala.reflect.internal.Symbols$Symbol$$anonfun$info$3.apply(Symbols.scala:1523)
	at scala.reflect.internal.Symbols$Symbol$$anonfun$info$3.apply(Symbols.scala:1521)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.reflect.internal.Symbols$Symbol.lock(Symbols.scala:567)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1521)
	at scala.tools.nsc.typechecker.Contexts$ImportInfo.qual(Contexts.scala:1416)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.pre$lzycompute$1(Completions.scala:719)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.scala$meta$internal$pc$completions$Completions$class$$anonfun$$pre$1(Completions.scala:719)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1$$anonfun$apply$1.apply(Completions.scala:722)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1$$anonfun$apply$1.apply(Completions.scala:720)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.apply(Completions.scala:720)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.apply(Completions.scala:718)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.meta.internal.pc.completions.Completions$class.renamedSymbols(Completions.scala:718)
	at scala.meta.internal.pc.MetalsGlobal.renamedSymbols(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.Signatures$ShortenedNames$.synthesize(Signatures.scala:56)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:151)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:75)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.toStream(Iterator.scala:1320)
	at scala.collection.AbstractIterator.toStream(Iterator.scala:1334)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)
	at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:134)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:40)
	at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:69)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.write(ReflectiveTypeAdapterFactory.java:125)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.write(ReflectiveTypeAdapterFactory.java:243)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:423)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:55)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at com.google.gson.Gson.toJson(Gson.java:648)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:145)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:140)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:59)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.lambda$handleRequest$1(RemoteEndpoint.java:281)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
	... 9 more

[0m2021.03.05 09:56:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:56:07 INFO  time: compiled root in 0.25s[0m
[0m2021.03.05 09:56:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:56:09 INFO  time: compiled root in 0.36s[0m
[0m2021.03.05 09:56:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:56:31 INFO  time: compiled root in 1.41s[0m
[0m2021.03.05 09:56:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:56:36 INFO  time: compiled root in 1.31s[0m
[0m2021.03.05 09:56:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:56:44 INFO  time: compiled root in 0.43s[0m
[0m2021.03.05 09:57:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:57:11 INFO  time: compiled root in 1.25s[0m
[0m2021.03.05 09:59:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 09:59:18 INFO  time: compiled root in 1.73s[0m
Mar 05, 2021 10:02:08 AM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: String index out of range: -1
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
	at java.lang.String.<init>(String.java:196)
	at scala.tools.nsc.interactive.Global.typeCompletions$1(Global.scala:1229)
	at scala.tools.nsc.interactive.Global.completionsAt(Global.scala:1252)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:375)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:373)
	at scala.Option.map(Option.scala:146)
	at scala.meta.internal.pc.SignatureHelpProvider.treeSymbol(SignatureHelpProvider.scala:373)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCall$.unapply(SignatureHelpProvider.scala:198)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.visit(SignatureHelpProvider.scala:309)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.traverse(SignatureHelpProvider.scala:303)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.fromTree(SignatureHelpProvider.scala:272)
	at scala.meta.internal.pc.SignatureHelpProvider.signatureHelp(SignatureHelpProvider.scala:27)

[0m2021.03.05 10:02:10 INFO  compiling root (1 scala source)[0m
Mar 05, 2021 10:02:10 AM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: String index out of range: -1
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
	at java.lang.String.<init>(String.java:196)
	at scala.tools.nsc.interactive.Global.typeCompletions$1(Global.scala:1229)
	at scala.tools.nsc.interactive.Global.completionsAt(Global.scala:1252)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:375)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:373)
	at scala.Option.map(Option.scala:146)
	at scala.meta.internal.pc.SignatureHelpProvider.treeSymbol(SignatureHelpProvider.scala:373)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCall$.unapply(SignatureHelpProvider.scala:198)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.visit(SignatureHelpProvider.scala:309)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.traverse(SignatureHelpProvider.scala:303)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.fromTree(SignatureHelpProvider.scala:272)
	at scala.meta.internal.pc.SignatureHelpProvider.signatureHelp(SignatureHelpProvider.scala:27)

[0m2021.03.05 10:02:10 INFO  time: compiled root in 0.22s[0m
[0m2021.03.05 10:02:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:02:19 INFO  time: compiled root in 0.34s[0m
[0m2021.03.05 10:02:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:02:28 INFO  time: compiled root in 0.3s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.CharArrayWriter

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.commons.lang3.StringUtils

import org.apache.spark.TaskContext
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.api.java.function._
import org.apache.spark.api.python.{PythonRDD, SerDeUtil}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection
import org.apache.spark.sql.catalyst.json.{JacksonGenerator, JSONOptions}
import org.apache.spark.sql.catalyst.optimizer.CombineUnions
import org.apache.spark.sql.catalyst.parser.{ParseException, ParserUtils}
import org.apache.spark.sql.catalyst.plans._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, PartitioningCollection}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.arrow.{ArrowBatchStreamWriter, ArrowConverters}
import org.apache.spark.sql.execution.command._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.execution.python.EvaluatePython
import org.apache.spark.sql.execution.stat.StatFunctions
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.types._
import org.apache.spark.sql.util.SchemaUtils
import org.apache.spark.storage.StorageLevel
import org.apache.spark.unsafe.array.ByteArrayMethods
import org.apache.spark.unsafe.types.CalendarInterval
import org.apache.spark.util.Utils

private[sql] object Dataset {
  def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = {
    val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]])
    // Eagerly bind the encoder so we verify that the encoder matches the underlying
    // schema. The user will get an error if this is not the case.
    // optimization: it is guaranteed that [[InternalRow]] can be converted to [[Row]] so
    // do not do this check in that case. this check can be expensive since it requires running
    // the whole [[Analyzer]] to resolve the deserializer
    if (dataset.exprEnc.clsTag.runtimeClass != classOf[Row]) {
      dataset.deserializer
    }
    dataset
  }

  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {
    val qe = sparkSession.sessionState.executePlan(logicalPlan)
    qe.assertAnalyzed()
    new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))
  }
}

/**
 * A Dataset is a strongly typed collection of domain-specific objects that can be transformed
 * in parallel using functional or relational operations. Each Dataset also has an untyped view
 * called a `DataFrame`, which is a Dataset of [[Row]].
 *
 * Operations available on Datasets are divided into transformations and actions. Transformations
 * are the ones that produce new Datasets, and actions are the ones that trigger computation and
 * return results. Example transformations include map, filter, select, and aggregate (`groupBy`).
 * Example actions count, show, or writing data out to file systems.
 *
 * Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally,
 * a Dataset represents a logical plan that describes the computation required to produce the data.
 * When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
 * physical plan for efficient execution in a parallel and distributed manner. To explore the
 * logical plan as well as optimized physical plan, use the `explain` function.
 *
 * To efficiently support domain-specific objects, an [[Encoder]] is required. The encoder maps
 * the domain specific type `T` to Spark's internal type system. For example, given a class `Person`
 * with two fields, `name` (string) and `age` (int), an encoder is used to tell Spark to generate
 * code at runtime to serialize the `Person` object into a binary structure. This binary structure
 * often has much lower memory footprint as well as are optimized for efficiency in data processing
 * (e.g. in a columnar format). To understand the internal binary representation for data, use the
 * `schema` function.
 *
 * There are typically two ways to create a Dataset. The most common way is by pointing Spark
 * to some files on storage systems, using the `read` function available on a `SparkSession`.
 * {{{
 *   val people = spark.read.parquet("...").as[Person]  // Scala
 *   Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java
 * }}}
 *
 * Datasets can also be created through transformations available on existing Datasets. For example,
 * the following creates a new Dataset by applying a filter on the existing one:
 * {{{
 *   val names = people.map(_.name)  // in Scala; names is a Dataset[String]
 *   Dataset<String> names = people.map((Person p) -> p.name, Encoders.STRING));
 * }}}
 *
 * Dataset operations can also be untyped, through various domain-specific-language (DSL)
 * functions defined in: Dataset (this class), [[Column]], and [[functions]]. These operations
 * are very similar to the operations available in the data frame abstraction in R or Python.
 *
 * To select a column from the Dataset, use `apply` method in Scala and `col` in Java.
 * {{{
 *   val ageCol = people("age")  // in Scala
 *   Column ageCol = people.col("age"); // in Java
 * }}}
 *
 * Note that the [[Column]] type can also be manipulated through its various functions.
 * {{{
 *   // The following creates a new column that increases everybody's age by 10.
 *   people("age") + 10  // in Scala
 *   people.col("age").plus(10);  // in Java
 * }}}
 *
 * A more concrete example in Scala:
 * {{{
 *   // To create Dataset[Row] using SparkSession
 *   val people = spark.read.parquet("...")
 *   val department = spark.read.parquet("...")
 *
 *   people.filter("age > 30")
 *     .join(department, people("deptId") === department("id"))
 *     .groupBy(department("name"), people("gender"))
 *     .agg(avg(people("salary")), max(people("age")))
 * }}}
 *
 * and in Java:
 * {{{
 *   // To create Dataset<Row> using SparkSession
 *   Dataset<Row> people = spark.read().parquet("...");
 *   Dataset<Row> department = spark.read().parquet("...");
 *
 *   people.filter(people.col("age").gt(30))
 *     .join(department, people.col("deptId").equalTo(department.col("id")))
 *     .groupBy(department.col("name"), people.col("gender"))
 *     .agg(avg(people.col("salary")), max(people.col("age")));
 * }}}
 *
 * @groupname basic Basic Dataset functions
 * @groupname action Actions
 * @groupname untypedrel Untyped transformations
 * @groupname typedrel Typed transformations
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class Dataset[T] private[sql](
    @transient val sparkSession: SparkSession,
    @DeveloperApi @InterfaceStability.Unstable @transient val queryExecution: QueryExecution,
    encoder: Encoder[T])
  extends Serializable {

  queryExecution.assertAnalyzed()

  // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure
  // you wrap it with `withNewExecutionId` if this actions doesn't call other action.

  def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sparkSession, sparkSession.sessionState.executePlan(logicalPlan), encoder)
  }

  def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sqlContext.sparkSession, logicalPlan, encoder)
  }

  @transient private[sql] val logicalPlan: LogicalPlan = {
    // For various commands (like DDL) and queries with side effects, we force query execution
    // to happen right away to let these side effects take place eagerly.
    queryExecution.analyzed match {
      case c: Command =>
        LocalRelation(c.output, withAction("command", queryExecution)(_.executeCollect()))
      case u @ Union(children) if children.forall(_.isInstanceOf[Command]) =>
        LocalRelation(u.output, withAction("command", queryExecution)(_.executeCollect()))
      case _ =>
        queryExecution.analyzed
    }
  }

  /**
   * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the
   * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use
   * it when constructing new Dataset objects that have the same object type (that will be
   * possibly resolved to a different schema).
   */
  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)

  // The deserializer expression which can be used to build a projection and turn rows to objects
  // of type T, after collecting rows to the driver side.
  private lazy val deserializer =
    exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer).deserializer

  private implicit def classTag = exprEnc.clsTag

  // sqlContext must be val because a stable identifier is expected when you import implicits
  @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext

  private[sql] def resolve(colName: String): NamedExpression = {
    queryExecution.analyzed.resolveQuoted(colName, sparkSession.sessionState.analyzer.resolver)
      .getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
  }

  private[sql] def numericColumns: Seq[Expression] = {
    schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n =>
      queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get
    }
  }

  /**
   * Get rows represented in Sequence by specific truncate and vertical requirement.
   *
   * @param numRows Number of rows to return
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   */
  private[sql] def getRows(
      numRows: Int,
      truncate: Int): Seq[Seq[String]] = {
    val newDf = toDF()
    val castCols = newDf.logicalPlan.output.map { col =>
      // Since binary types in top-level schema fields have a specific format to print,
      // so we do not cast them to strings here.
      if (col.dataType == BinaryType) {
        Column(col)
      } else {
        Column(col).cast(StringType)
      }
    }
    val data = newDf.select(castCols: _*).take(numRows + 1)

    // For array values, replace Seq and Array with square brackets
    // For cells that are beyond `truncate` characters, replace it with the
    // first `truncate-3` and "..."
    schema.fieldNames.toSeq +: data.map { row =>
      row.toSeq.map { cell =>
        val str = cell match {
          case null => "null"
          case binary: Array[Byte] => binary.map("%02X".format(_)).mkString("[", " ", "]")
          case _ => cell.toString
        }
        if (truncate > 0 && str.length > truncate) {
          // do not show ellipses for strings shorter than 4 characters.
          if (truncate < 4) str.substring(0, truncate)
          else str.substring(0, truncate - 3) + "..."
        } else {
          str
        }
      }: Seq[String]
    }
  }

  /**
   * Compose the string representing rows for output
   *
   * @param _numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   */
  private[sql] def showString(
      _numRows: Int,
      truncate: Int = 20,
      vertical: Boolean = false): String = {
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data.
    val tmpRows = getRows(numRows, truncate)

    val hasMoreData = tmpRows.length - 1 > numRows
    val rows = tmpRows.take(numRows + 1)

    val sb = new StringBuilder
    val numCols = schema.fieldNames.length
    // We set a minimum column width at '3'
    val minimumColWidth = 3

    if (!vertical) {
      // Initialise the width of each column to a minimum value
      val colWidths = Array.fill(numCols)(minimumColWidth)

      // Compute the width of each column
      for (row <- rows) {
        for ((cell, i) <- row.zipWithIndex) {
          colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell))
        }
      }

      val paddedRows = rows.map { row =>
        row.zipWithIndex.map { case (cell, i) =>
          if (truncate > 0) {
            StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          } else {
            StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          }
        }
      }

      // Create SeparateLine
      val sep: String = colWidths.map("-" * _).addString(sb, "+", "+", "+\n").toString()

      // column names
      paddedRows.head.addString(sb, "|", "|", "|\n")
      sb.append(sep)

      // data
      paddedRows.tail.foreach(_.addString(sb, "|", "|", "|\n"))
      sb.append(sep)
    } else {
      // Extended display mode enabled
      val fieldNames = rows.head
      val dataRows = rows.tail

      // Compute the width of field name and data columns
      val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) =>
        math.max(curMax, Utils.stringHalfWidth(fieldName))
      }
      val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) =>
        math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max)
      }

      dataRows.zipWithIndex.foreach { case (row, i) =>
        // "+ 5" in size means a character length except for padded names and data
        val rowHeader = StringUtils.rightPad(
          s"-RECORD $i", fieldNameColWidth + dataColWidth + 5, "-")
        sb.append(rowHeader).append("\n")
        row.zipWithIndex.map { case (cell, j) =>
          val fieldName = StringUtils.rightPad(fieldNames(j),
            fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length)
          val data = StringUtils.rightPad(cell,
            dataColWidth - Utils.stringHalfWidth(cell) + cell.length)
          s" $fieldName | $data "
        }.addString(sb, "", "\n", "\n")
      }
    }

    // Print a footer
    if (vertical && rows.tail.isEmpty) {
      // In a vertical mode, print an empty row set explicitly
      sb.append("(0 rows)\n")
    } else if (hasMoreData) {
      // For Data that has more than "numRows" records
      val rowsString = if (numRows == 1) "row" else "rows"
      sb.append(s"only showing top $numRows $rowsString\n")
    }

    sb.toString()
  }

  override def toString: String = {
    try {
      val builder = new StringBuilder
      val fields = schema.take(2).map {
        case f => s"${f.name}: ${f.dataType.simpleString(2)}"
      }
      builder.append("[")
      builder.append(fields.mkString(", "))
      if (schema.length > 2) {
        if (schema.length - fields.size == 1) {
          builder.append(" ... 1 more field")
        } else {
          builder.append(" ... " + (schema.length - 2) + " more fields")
        }
      }
      builder.append("]").toString()
    } catch {
      case NonFatal(e) =>
        s"Invalid tree; ${e.getMessage}:\n$queryExecution"
    }
  }

  /**
   * Converts this strongly typed collection of data to generic Dataframe. In contrast to the
   * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]
   * objects that allow fields to be accessed by ordinal or name.
   *
   * @group basic
   * @since 1.6.0
   */
  // This is declared with parentheses to prevent the Scala compiler from treating
  // `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))

  /**
   * :: Experimental ::
   * Returns a new Dataset where each record has been mapped on to the specified type. The
   * method used to map columns depend on the type of `U`:
   *  - When `U` is a class, fields for the class will be mapped to columns of the same name
   *    (case sensitivity is determined by `spark.sql.caseSensitive`).
   *  - When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will
   *    be assigned to `_1`).
   *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the
   *    `DataFrame` will be used.
   *
   * If the schema of the Dataset does not match the desired `U` type, you can use `select`
   * along with `alias` or `as` to rearrange or rename as required.
   *
   * Note that `as[]` only changes the view of the data that is passed into typed operations,
   * such as `map()`, and does not eagerly project away any columns that are not present in
   * the specified class.
   *
   * @group basic
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan)

  /**
   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.
   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with
   * meaningful names. For example:
   * {{{
   *   val rdd: RDD[(Int, String)] = ...
   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`
   *   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
   * }}}
   *
   * @group basic
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def toDF(colNames: String*): DataFrame = {
    require(schema.size == colNames.size,
      "The number of columns doesn't match.\n" +
        s"Old column names (${schema.size}): " + schema.fields.map(_.name).mkString(", ") + "\n" +
        s"New column names (${colNames.size}): " + colNames.mkString(", "))

    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>
      Column(oldAttribute).as(newName)
    }
    select(newCols : _*)
  }

  /**
   * Returns the schema of this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def schema: StructType = queryExecution.analyzed.schema

  /**
   * Prints the schema to the console in a nice tree format.
   *
   * @group basic
   * @since 1.6.0
   */
  // scalastyle:off println
  def printSchema(): Unit = println(schema.treeString)
  // scalastyle:on println

  /**
   * Prints the plans (logical and physical) to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(extended: Boolean): Unit = {
    val explain = ExplainCommand(queryExecution.logical, extended = extended)
    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {
      // scalastyle:off println
      r => println(r.getString(0))
      // scalastyle:on println
    }
  }

  /**
   * Prints the physical plan to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(): Unit = explain(extended = false)

  /**
   * Returns all column names and their data types as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def dtypes: Array[(String, String)] = schema.fields.map { field =>
    (field.name, field.dataType.toString)
  }

  /**
   * Returns all column names as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def columns: Array[String] = schema.fields.map(_.name)

  /**
   * Returns true if the `collect` and `take` methods can be run locally
   * (without any Spark executors).
   *
   * @group basic
   * @since 1.6.0
   */
  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]

  /**
   * Returns true if the `Dataset` is empty.
   *
   * @group basic
   * @since 2.4.0
   */
  def isEmpty: Boolean = withAction("isEmpty", limit(1).groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0) == 0
  }

  /**
   * Returns true if this Dataset contains one or more sources that continuously
   * return data as it arrives. A Dataset that reads data from a streaming source
   * must be executed as a `StreamingQuery` using the `start()` method in
   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or
   * `collect()`, will throw an [[AnalysisException]] when there is a streaming
   * source present.
   *
   * @group streaming
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def isStreaming: Boolean = logicalPlan.isStreaming

  /**
   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true)

  /**
   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
   * logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true)

  /**
   * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be
   * used to truncate the logical plan of this Dataset, which is especially useful in iterative
   * algorithms where the plan may grow exponentially. Local checkpoints are written to executor
   * storage and despite potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false)

  /**
   * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. Local checkpoints are written to executor storage and despite
   * potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint(
    eager = eager,
    reliableCheckpoint = false
  )

  /**
   * Returns a checkpointed version of this Dataset.
   *
   * @param eager Whether to checkpoint this dataframe immediately
   * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the
   *                           checkpoint directory. If false creates a local checkpoint using
   *                           the caching subsystem
   */
  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = {
    val internalRdd = queryExecution.toRdd.map(_.copy())
    if (reliableCheckpoint) {
      internalRdd.checkpoint()
    } else {
      internalRdd.localCheckpoint()
    }

    if (eager) {
      internalRdd.count()
    }

    val physicalPlan = queryExecution.executedPlan

    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the
    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner
    // joins.
    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {
      partitioning match {
        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)
        case p => p
      }
    }

    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)

    Dataset.ofRows(
      sparkSession,
      LogicalRDD(
        logicalPlan.output,
        internalRdd,
        outputPartitioning,
        physicalPlan.outputOrdering,
        isStreaming
      )(sparkSession)).as[T]
  }

  /**
   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time
   * before which we assume no more late data is going to arrive.
   *
   * Spark will use this watermark for several purposes:
   *  - To know when a given time window aggregation can be finalized and thus can be emitted when
   *    using output modes that do not allow updates.
   *  - To minimize the amount of state that we need to keep for on-going aggregations,
   *    `mapGroupsWithState` and `dropDuplicates` operators.
   *
   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across
   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost
   *  of coordinating this value across partitions, the actual watermark used is only guaranteed
   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still
   *  process records that arrive more than `delayThreshold` late.
   *
   * @param eventTime the name of the column that contains the event time of the row.
   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest
   *                       record that has been processed in the form of an interval
   *                       (e.g. "1 minute" or "5 hours"). NOTE: This should not be negative.
   *
   * @group streaming
   * @since 2.1.0
   */
  @InterfaceStability.Evolving
  // We only accept an existing column name, not a derived column here as a watermark that is
  // defined on a derived column cannot referenced elsewhere in the plan.
  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {
    val parsedDelay =
      try {
        CalendarInterval.fromCaseInsensitiveString(delayThreshold)
      } catch {
        case e: IllegalArgumentException =>
          throw new AnalysisException(
            s"Unable to parse time delay '$delayThreshold'",
            cause = Some(e))
      }
    require(parsedDelay.milliseconds >= 0 && parsedDelay.months >= 0,
      s"delay threshold ($delayThreshold) should not be negative.")
    EliminateEventTimeWatermark(
      EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan))
  }

  /**
   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
   * and all cells will be aligned right. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   *
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int): Unit = show(numRows, truncate = true)

  /**
   * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters
   * will be truncated, and all cells will be aligned right.
   *
   * @group action
   * @since 1.6.0
   */
  def show(): Unit = show(20)

  /**
   * Displays the top 20 rows of Dataset in a tabular form.
   *
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *                 be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  def show(truncate: Boolean): Unit = show(20, truncate)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   * @param numRows Number of rows to show
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *              be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {
    println(showString(numRows, truncate = 20))
  } else {
    println(showString(numRows, truncate = 0))
  }

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * If `vertical` enabled, this command prints output rows vertically (one line per column value)?
   *
   * {{{
   * -RECORD 0-------------------
   *  year            | 1980
   *  month           | 12
   *  AVG('Adj Close) | 0.503218
   *  AVG('Adj Close) | 0.595103
   * -RECORD 1-------------------
   *  year            | 1981
   *  month           | 01
   *  AVG('Adj Close) | 0.523289
   *  AVG('Adj Close) | 0.570307
   * -RECORD 2-------------------
   *  year            | 1982
   *  month           | 02
   *  AVG('Adj Close) | 0.436504
   *  AVG('Adj Close) | 0.475256
   * -RECORD 3-------------------
   *  year            | 1983
   *  month           | 03
   *  AVG('Adj Close) | 0.410516
   *  AVG('Adj Close) | 0.442194
   * -RECORD 4-------------------
   *  year            | 1984
   *  month           | 04
   *  AVG('Adj Close) | 0.450090
   *  AVG('Adj Close) | 0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   * @group action
   * @since 2.3.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Int, vertical: Boolean): Unit =
    println(showString(numRows, truncate, vertical))
  // scalastyle:on println

  /**
   * Returns a [[DataFrameNaFunctions]] for working with missing data.
   * {{{
   *   // Dropping rows containing any null values.
   *   ds.na.drop()
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())

  /**
   * Returns a [[DataFrameStatFunctions]] for working statistic functions support.
   * {{{
   *   // Finding frequent items in column with name 'a'.
   *   ds.stat.freqItems(Seq("a"))
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())

  /**
   * Join with another `DataFrame`.
   *
   * Behaves as an INNER JOIN and requires a subsequent join predicate.
   *
   * @param right Right side of the join operation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)
  }

  /**
   * Inner equi-join with another `DataFrame` using the given column.
   *
   * Different from other join functions, the join column will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the column "user_id"
   *   df1.join(df2, "user_id")
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumn Name of the column to join on. This column must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumn: String): DataFrame = {
    join(right, Seq(usingColumn))
  }

  /**
   * Inner equi-join with another `DataFrame` using the given columns.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the columns "user_id" and "user_name"
   *   df1.join(df2, Seq("user_id", "user_name"))
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {
    join(right, usingColumns, "inner")
  }

  /**
   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate
   * is specified as an inner join. If you would explicitly like to perform a cross join use the
   * `crossJoin` method.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {
    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right
    // by creating a new instance for one of the branch.
    val joined = sparkSession.sessionState.executePlan(
      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))
      .analyzed.asInstanceOf[Join]

    withPlan {
      Join(
        joined.left,
        joined.right,
        UsingJoin(JoinType(joinType), usingColumns),
        None)
    }
  }

  /**
   * Inner join with another `DataFrame`, using the given join expression.
   *
   * {{{
   *   // The following two are equivalent:
   *   df1.join(df2, $"df1Key" === $"df2Key")
   *   df1.join(df2).where($"df1Key" === $"df2Key")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, "inner")

  /**
   * Join with another `DataFrame`, using the given join expression. The following performs
   * a full outer join between `df1` and `df2`.
   *
   * {{{
   *   // Scala:
   *   import org.apache.spark.sql.functions._
   *   df1.join(df2, $"df1Key" === $"df2Key", "outer")
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
   * }}}
   *
   * @param right Right side of the join.
   * @param joinExprs Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {
    // Note that in this function, we introduce a hack in the case of self-join to automatically
    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].
    // Consider this case: df.join(df, df("key") === df("key"))
    // Since df("key") === df("key") is a trivially true condition, this actually becomes a
    // cartesian join. However, most likely users expect to perform a self join using "key".
    // With that assumption, this hack turns the trivially true condition into equality on join
    // keys that are resolved to both sides.

    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.
    // After the cloning, left and right side will have distinct expression ids.
    val plan = withPlan(
      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))
      .queryExecution.analyzed.asInstanceOf[Join]

    // If auto self join alias is disabled, return the plan.
    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {
      return withPlan(plan)
    }

    // If left/right have no output set intersection, return the plan.
    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed
    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed
    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {
      return withPlan(plan)
    }

    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.
    // By the time we get here, since we have already run analysis, all attributes should've been
    // resolved and become AttributeReference.
    val cond = plan.condition.map { _.transform {
      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)
          if a.sameRef(b) =>
        catalyst.expressions.EqualTo(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
      case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference)
        if a.sameRef(b) =>
        catalyst.expressions.EqualNullSafe(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
    }}

    withPlan {
      plan.copy(condition = cond)
    }
  }

  /**
   * Explicit cartesian join with another `DataFrame`.
   *
   * @param right Right side of the join operation.
   *
   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.
   *
   * @group untypedrel
   * @since 2.1.0
   */
  def crossJoin(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)
  }

  /**
   * :: Experimental ::
   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to
   * true.
   *
   * This is similar to the relation `join` function with one important difference in the
   * result schema. Since `joinWith` preserves objects present on either side of the join, the
   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.
   *
   * This type of join can be useful both for preserving type-safety with the original object
   * types as well as working with relational data where either side of the join has column
   * names in common.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {
    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,
    // etc.
    val joined = sparkSession.sessionState.executePlan(
      Join(
        this.logicalPlan,
        other.logicalPlan,
        JoinType(joinType),
        Some(condition.expr))).analyzed.asInstanceOf[Join]

    if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) {
      throw new AnalysisException("Invalid join type in joinWith: " + joined.joinType.sql)
    }

    // For both join side, combine all outputs into a single column and alias it with "_1" or "_2",
    // to match the schema for the encoder of the join result.
    // Note that we do this before joining them, to enable the join operator to return null for one
    // side, in cases like outer-join.
    val left = {
      val combined = if (this.exprEnc.flat) {
        assert(joined.left.output.length == 1)
        Alias(joined.left.output.head, "_1")()
      } else {
        Alias(CreateStruct(joined.left.output), "_1")()
      }
      Project(combined :: Nil, joined.left)
    }

    val right = {
      val combined = if (other.exprEnc.flat) {
        assert(joined.right.output.length == 1)
        Alias(joined.right.output.head, "_2")()
      } else {
        Alias(CreateStruct(joined.right.output), "_2")()
      }
      Project(combined :: Nil, joined.right)
    }

    // Rewrites the join condition to make the attribute point to correct column/field, after we
    // combine the outputs of each join side.
    val conditionExpr = joined.condition.get transformUp {
      case a: Attribute if joined.left.outputSet.contains(a) =>
        if (this.exprEnc.flat) {
          left.output.head
        } else {
          val index = joined.left.output.indexWhere(_.exprId == a.exprId)
          GetStructField(left.output.head, index)
        }
      case a: Attribute if joined.right.outputSet.contains(a) =>
        if (other.exprEnc.flat) {
          right.output.head
        } else {
          val index = joined.right.output.indexWhere(_.exprId == a.exprId)
          GetStructField(right.output.head, index)
        }
    }

    implicit val tuple2Encoder: Encoder[(T, U)] =
      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)

    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))
  }

  /**
   * :: Experimental ::
   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair
   * where `condition` evaluates to true.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {
    joinWith(other, condition, "inner")
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {
    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = false, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the specified column, all in ascending order.
   * {{{
   *   // The following 3 are equivalent
   *   ds.sort("sortcol")
   *   ds.sort($"sortcol")
   *   ds.sort($"sortcol".asc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortCol: String, sortCols: String*): Dataset[T] = {
    sort((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset sorted by the given expressions. For example:
   * {{{
   *   ds.sort($"col1", $"col2".desc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = true, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def apply(colName: String): Column = col(colName)

  /**
   * Specifies some hint on the current Dataset. As an example, the following code specifies
   * that one of the plan can be broadcasted:
   *
   * {{{
   *   df1.join(df2.hint("broadcast"))
   * }}}
   *
   * @group basic
   * @since 2.2.0
   */
  @scala.annotation.varargs
  def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan {
    UnresolvedHint(name, parameters, logicalPlan)
  }

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def col(colName: String): Column = colName match {
    case "*" =>
      Column(ResolvedStar(queryExecution.analyzed.output))
    case _ =>
      if (sqlContext.conf.supportQuotedRegexColumnName) {
        colRegex(colName)
      } else {
        val expr = resolve(colName)
        Column(expr)
      }
  }

  /**
   * Selects column based on the column name specified as a regex and returns it as [[Column]].
   * @group untypedrel
   * @since 2.3.0
   */
  def colRegex(colName: String): Column = {
    val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis
    colName match {
      case ParserUtils.escapedIdentifier(columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, None, caseSensitive))
      case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive))
      case _ =>
        Column(resolve(colName))
    }
  }

  /**
   * Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def as(alias: String): Dataset[T] = withTypedPlan {
    SubqueryAlias(alias, logicalPlan)
  }

  /**
   * (Scala-specific) Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def as(alias: Symbol): Dataset[T] = as(alias.name)

  /**
   * Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: String): Dataset[T] = as(alias)

  /**
   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: Symbol): Dataset[T] = as(alias)

  /**
   * Selects a set of column based expressions.
   * {{{
   *   ds.select($"colA", $"colB" + 1)
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(cols: Column*): DataFrame = withPlan {
    Project(cols.map(_.named), logicalPlan)
  }

  /**
   * Selects a set of columns. This is a variant of `select` that can only select
   * existing columns using column names (i.e. cannot construct expressions).
   *
   * {{{
   *   // The following two are equivalent:
   *   ds.select("colA", "colB")
   *   ds.select($"colA", $"colB")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)

  /**
   * Selects a set of SQL expressions. This is a variant of `select` that accepts
   * SQL expressions.
   *
   * {{{
   *   // The following are equivalent:
   *   ds.selectExpr("colA", "colB as newName", "abs(colC)")
   *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def selectExpr(exprs: String*): DataFrame = {
    select(exprs.map { expr =>
      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))
    }: _*)
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expression for each element.
   *
   * {{{
   *   val ds = Seq(1, 2, 3).toDS()
   *   val newDS = ds.select(expr("value + 1").as[Int])
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {
    implicit val encoder = c1.encoder
    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan)

    if (encoder.flat) {
      new Dataset[U1](sparkSession, project, encoder)
    } else {
      // Flattens inner fields of U1
      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)
    }
  }

  /**
   * Internal helper function for building typed selects that return tuples. For simplicity and
   * code reuse, we do this without the help of the type system and then use helper functions
   * that cast appropriately for the user facing interface.
   */
  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {
    val encoders = columns.map(_.encoder)
    val namedColumns =
      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)
    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))
    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =
    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =
    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =
    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4, U5](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4],
      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =
    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]

  /**
   * Filters rows using the given condition.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(condition: Column): Dataset[T] = withTypedPlan {
    Filter(condition.expr, logicalPlan)
  }

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.filter("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Filters rows using the given condition. This is an alias for `filter`.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(condition: Column): Dataset[T] = filter(condition)

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.where("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Groups the Dataset using the specified columns, so we can run aggregation on them. See
   * [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy($"department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)
  }

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)
  }

  /**
   * Groups the Dataset using the specified columns, so that we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of groupBy that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy("department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: (T, T) => T): T = withNewRDDExecutionId {
    rdd.reduce(func)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {
    val withGroupingKey = AppendColumns(func, logicalPlan)
    val executed = sparkSession.sessionState.executePlan(withGroupingKey)

    new KeyValueGroupedDataset(
      encoderFor[K],
      encoderFor[T],
      executed,
      logicalPlan.output,
      withGroupingKey.newColumns)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =
    groupByKey(func.call(_))(encoder)

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of rollup that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup("department", "group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of cube that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube("department", "group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg("age" -> "max", "salary" -> "avg")
   *   ds.groupBy().agg("age" -> "max", "salary" -> "avg")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
    groupBy().agg(aggExpr, aggExprs : _*)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * (Java-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(max($"age"), avg($"salary"))
   *   ds.groupBy().agg(max($"age"), avg($"salary"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)

  /**
   * Returns a new Dataset by taking the first `n` rows. The difference between this function
   * and `head` is that `head` is an action and returns an array (by triggering query execution)
   * while `limit` returns a new Dataset.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def limit(n: Int): Dataset[T] = withTypedPlan {
    Limit(Literal(n), logicalPlan)
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @deprecated("use union()", "2.0.0")
  def unionAll(other: Dataset[T]): Dataset[T] = union(other)

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.union(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   4|   5|   6|
   *   // +----+----+----+
   * }}}
   *
   * Notice that the column positions in the schema aren't necessarily matched with the
   * fields in the strongly typed objects in a Dataset. This function resolves columns
   * by their positions in the schema, not the fields in the strongly typed objects. Use
   * [[unionByName]] to resolve columns by field name in the typed objects.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def union(other: Dataset[T]): Dataset[T] = withSetOperator {
    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, other.logicalPlan))
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set
   * union (that does deduplication of elements), use this function followed by a [[distinct]].
   *
   * The difference between this function and [[union]] is that this function
   * resolves columns by name (not by position):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.unionByName(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   6|   4|   5|
   *   // +----+----+----+
   * }}}
   *
   * @group typedrel
   * @since 2.3.0
   */
  def unionByName(other: Dataset[T]): Dataset[T] = withSetOperator {
    // Check column name duplication
    val resolver = sparkSession.sessionState.analyzer.resolver
    val leftOutputAttrs = logicalPlan.output
    val rightOutputAttrs = other.logicalPlan.output

    SchemaUtils.checkColumnNameDuplication(
      leftOutputAttrs.map(_.name),
      "in the left attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)
    SchemaUtils.checkColumnNameDuplication(
      rightOutputAttrs.map(_.name),
      "in the right attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    // Builds a project list for `other` based on `logicalPlan` output names
    val rightProjectList = leftOutputAttrs.map { lattr =>
      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "${lattr.name}" among """ +
            s"""(${rightOutputAttrs.map(_.name).mkString(", ")})""")
      }
    }

    // Delegates failure checks to `CheckAnalysis`
    val notFoundAttrs = rightOutputAttrs.diff(rightProjectList)
    val rightChild = Project(rightProjectList ++ notFoundAttrs, other.logicalPlan)

    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, rightChild))
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.
   * This is equivalent to `INTERSECT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset while
   * preserving the duplicates.
   * This is equivalent to `INTERSECT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard
   * in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = true)
  }


  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.
   * This is equivalent to `EXCEPT DISTINCT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def except(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset while
   * preserving the duplicates.
   * This is equivalent to `EXCEPT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in
   * SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = true)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a user-supplied seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double, seed: Long): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction, seed = seed)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a random seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {
    withTypedPlan {
      Sample(0.0, fraction, withReplacement, seed, logicalPlan)
    }
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the total count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {
    sample(withReplacement, fraction, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * For Java API, use [[randomSplitAsList]].
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its
    // constituent partitions each time a split is materialized which could result in
    // overlapping splits. To prevent this, we explicitly sort each input partition to make the
    // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out
    // from the sort order.
    val sortOrder = logicalPlan.output
      .filter(attr => RowOrdering.isOrderable(attr.dataType))
      .map(SortOrder(_, Ascending))
    val plan = if (sortOrder.nonEmpty) {
      Sort(sortOrder, global = false, logicalPlan)
    } else {
      // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism
      cache()
      logicalPlan
    }
    val sum = weights.sum
    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
    normalizedCumWeights.sliding(2).map { x =>
      new Dataset[T](
        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder)
    }.toArray
  }

  /**
   * Returns a Java list that contains randomly split Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {
    val values = randomSplit(weights, seed)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {
    randomSplit(weights, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   */
  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {
    randomSplit(weights.toArray, seed)
  }

  /**
   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of
   * the input row are implicitly joined with each row that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count
   * the number of books that contain a given word:
   *
   * {{{
   *   case class Book(title: String, words: String)
   *   val ds: Dataset[Book]
   *
   *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))
   *
   *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
   * }}}
   *
   * Using `flatMap()` this can similarly be exploded as:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {
    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]

    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)

    val rowFunction =
      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))
    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero
   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All
   * columns of the input row are implicitly joined with each value that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()`:
   *
   * {{{
   *   ds.select(explode(split('words, " ")).as("word"))
   * }}}
   *
   * or `flatMap()`:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])
    : DataFrame = {
    val dataType = ScalaReflection.schemaFor[B].dataType
    val attributes = AttributeReference(outputColumn, dataType)() :: Nil
    // TODO handle the metadata?
    val elementSchema = attributes.toStructType

    def rowFunction(row: Row): TraversableOnce[InternalRow] = {
      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)
      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))
    }
    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * Returns a new Dataset by adding a column or replacing the existing column that has
   * the same name.
   *
   * `column`'s expression must only refer to attributes supplied by this Dataset. It is an
   * error to add a column that refers to some other Dataset.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col))

  /**
   * Returns a new Dataset by adding columns or replacing the existing columns that has
   * the same names.
   */
  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
    require(colNames.size == cols.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of columns: ${cols.size}")
    SchemaUtils.checkColumnNameDuplication(
      colNames,
      "in given column names",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output

    val columnMap = colNames.zip(cols).toMap

    val replacedAndExistingColumns = output.map { field =>
      columnMap.find { case (colName, _) =>
        resolver(field.name, colName)
      } match {
        case Some((colName: String, col: Column)) => col.as(colName)
        case _ => Column(field)
      }
    }

    val newColumns = columnMap.filter { case (colName, col) =>
      !output.exists(f => resolver(f.name, colName))
    }.map { case (colName, col) => col.as(colName) }

    select(replacedAndExistingColumns ++ newColumns : _*)
  }

  /**
   * Returns a new Dataset by adding columns with metadata.
   */
  private[spark] def withColumns(
      colNames: Seq[String],
      cols: Seq[Column],
      metadata: Seq[Metadata]): DataFrame = {
    require(colNames.size == metadata.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of metadata elements: ${metadata.size}")
    val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) =>
      col.as(colName, metadata)
    }
    withColumns(colNames, newCols)
  }

  /**
   * Returns a new Dataset by adding a column with metadata.
   */
  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame =
    withColumns(Seq(colName), Seq(col), Seq(metadata))

  /**
   * Returns a new Dataset with a column renamed.
   * This is a no-op if schema doesn't contain existingName.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumnRenamed(existingName: String, newName: String): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output
    val shouldRename = output.exists(f => resolver(f.name, existingName))
    if (shouldRename) {
      val columns = output.map { col =>
        if (resolver(col.name, existingName)) {
          Column(col).as(newName)
        } else {
          Column(col)
        }
      }
      select(columns : _*)
    } else {
      toDF()
    }
  }

  /**
   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain
   * column name.
   *
   * This method can only be used to drop top level columns. the colName string is treated
   * literally without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(colName: String): DataFrame = {
    drop(Seq(colName) : _*)
  }

  /**
   * Returns a new Dataset with columns dropped.
   * This is a no-op if schema doesn't contain column name(s).
   *
   * This method can only be used to drop top level columns. the colName string is treated literally
   * without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def drop(colNames: String*): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val remainingCols = allColumns.filter { attribute =>
      colNames.forall(n => !resolver(attribute.name, n))
    }.map(attribute => Column(attribute))
    if (remainingCols.size == allColumns.size) {
      toDF()
    } else {
      this.select(remainingCols: _*)
    }
  }

  /**
   * Returns a new Dataset with a column dropped.
   * This version of drop accepts a [[Column]] rather than a name.
   * This is a no-op if the Dataset doesn't have a column
   * with an equivalent expression.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(col: Column): DataFrame = {
    val expression = col match {
      case Column(u: UnresolvedAttribute) =>
        queryExecution.analyzed.resolveQuoted(
          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)
      case Column(expr: Expression) => expr
    }
    val attrs = this.logicalPlan.output
    val colsAfterDrop = attrs.filter { attr =>
      attr != expression
    }.map(attr => Column(attr))
    select(colsAfterDrop : _*)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `distinct`.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)

  /**
   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>
      // It is possibly there are more than one columns with the same name,
      // so we call filter instead of find.
      val cols = allColumns.filter(col => resolver(col.name, colName))
      if (cols.isEmpty) {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
      cols
    }
    Deduplicate(groupCols, logicalPlan)
  }

  /**
   * Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)

  /**
   * Returns a new [[Dataset]] with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {
    val colNames: Seq[String] = col1 +: cols
    dropDuplicates(colNames)
  }

  /**
   * Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
   * and max. If no columns are given, this function computes statistics for all numerical or
   * string columns.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.describe("age", "height").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // max     92.0  192.0
   * }}}
   *
   * Use [[summary]] for expanded statistics and control over which statistics to compute.
   *
   * @param cols Columns to compute statistics on.
   *
   * @group action
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def describe(cols: String*): DataFrame = {
    val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*)
    selected.summary("count", "mean", "stddev", "min", "max")
  }

  /**
   * Computes specified statistics for numeric and string columns. Available statistics are:
   *
   * - count
   * - mean
   * - stddev
   * - min
   * - max
   * - arbitrary approximate percentiles specified as a percentage (eg, 75%)
   *
   * If no statistics are given, this function computes count, mean, stddev, min,
   * approximate quartiles (percentiles at 25%, 50%, and 75%), and max.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.summary().show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 50%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * {{{
   *   ds.summary("count", "min", "25%", "75%", "max").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * To do a summary for specific columns first select them:
   *
   * {{{
   *   ds.select("age", "height").summary().show()
   * }}}
   *
   * See also [[describe]] for basic statistics.
   *
   * @param statistics Statistics from above list to be computed.
   *
   * @group action
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq)

  /**
   * Returns the first `n` rows.
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @group action
   * @since 1.6.0
   */
  def head(n: Int): Array[T] = withAction("head", limit(n).queryExecution)(collectFromPlan)

  /**
   * Returns the first row.
   * @group action
   * @since 1.6.0
   */
  def head(): T = head(1).head

  /**
   * Returns the first row. Alias for head().
   * @group action
   * @since 1.6.0
   */
  def first(): T = head()

  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: Dataset[T]): Dataset[U] = ...
   *
   *   ds
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: T => Boolean): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: FilterFunction[T]): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {
    MapElements[T, U](func, logicalPlan)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    implicit val uEnc = encoder
    withTypedPlan(MapElements[T, U](func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {
    new Dataset[U](
      sparkSession,
      MapPartitions[T, U](func, logicalPlan),
      implicitly[Encoder[U]])
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `f` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala
    mapPartitions(func)(encoder)
  }

  /**
   * Returns a new `DataFrame` that contains the result of applying a serialized R function
   * `func` to each partition.
   */
  private[sql] def mapPartitionsInR(
      func: Array[Byte],
      packageNames: Array[Byte],
      broadcastVars: Array[Broadcast[Object]],
      schema: StructType): DataFrame = {
    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]
    Dataset.ofRows(
      sparkSession,
      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =
    mapPartitions(_.flatMap(func))

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (T) => Iterator[U] = x => f.call(x).asScala
    flatMap(func)(encoder)
  }

  /**
   * Applies a function `f` to all rows.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(f: T => Unit): Unit = withNewRDDExecutionId {
    rdd.foreach(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each element of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))

  /**
   * Applies a function `f` to each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId {
    rdd.foreachPartition(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = {
    foreachPartition((it: Iterator[T]) => func.call(it.asJava))
  }

  /**
   * Returns the first `n` rows in the Dataset.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def take(n: Int): Array[T] = head(n)

  /**
   * Returns the first `n` rows in the Dataset as a list.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)

  /**
   * Returns an array that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * For Java API, use [[collectAsList]].
   *
   * @group action
   * @since 1.6.0
   */
  def collect(): Array[T] = withAction("collect", queryExecution)(collectFromPlan)

  /**
   * Returns a Java list that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def collectAsList(): java.util.List[T] = withAction("collectAsList", queryExecution) { plan =>
    val values = collectFromPlan(plan)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Returns an iterator that contains all rows in this Dataset.
   *
   * The iterator will consume as much memory as the largest partition in this Dataset.
   *
   * @note this results in multiple Spark jobs, and if the input Dataset is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input Dataset should be cached first.
   *
   * @group action
   * @since 2.0.0
   */
  def toLocalIterator(): java.util.Iterator[T] = {
    withAction("toLocalIterator", queryExecution) { plan =>
      // This projection writes output to a `InternalRow`, which means applying this projection is
      // not thread-safe. Here we create the projection inside this method to make `Dataset`
      // thread-safe.
      val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
      plan.executeToIterator().map { row =>
        // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
        // parameter of its `get` method, so it's safe to use null here.
        objProj(row).get(0, null).asInstanceOf[T]
      }.asJava
    }
  }

  /**
   * Returns the number of rows in the Dataset.
   * @group action
   * @since 1.6.0
   */
  def count(): Long = withAction("count", groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = true, logicalPlan)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments.
    // However, we don't want to complicate the semantics of this API method.
    // Instead, let's give users a friendly error message, pointing them to the new method.
    val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder])
    if (sortOrders.nonEmpty) throw new IllegalArgumentException(
      s"""Invalid partitionExprs specified: $sortOrders
         |For range partitioning use repartitionByRange(...) instead.
       """.stripMargin)
    withTypedPlan {
      RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(partitionExprs: Column*): Dataset[T] = {
    repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    require(partitionExprs.nonEmpty, "At least one partition-by expression must be specified.")
    val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match {
      case expr: SortOrder => expr
      case expr: Expression => SortOrder(expr, Ascending)
    })
    withTypedPlan {
      RepartitionByExpression(sortOrder, logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(partitionExprs: Column*): Dataset[T] = {
    repartitionByRange(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions
   * are requested. If a larger number of partitions is requested, it will stay at the current
   * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in
   * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not
   * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can call repartition. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @group typedrel
   * @since 1.6.0
   */
  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = false, logicalPlan)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `dropDuplicates`.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def distinct(): Dataset[T] = dropDuplicates()

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this)
    this
  }

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def cache(): this.type = persist()

  /**
   * Persist this Dataset with the given storage level.
   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,
   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,
   *                 `MEMORY_AND_DISK_2`, etc.
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(newLevel: StorageLevel): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)
    this
  }

  /**
   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
   *
   * @group basic
   * @since 2.1.0
   */
  def storageLevel: StorageLevel = {
    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>
      cachedData.cachedRepresentation.cacheBuilder.storageLevel
    }.getOrElse(StorageLevel.NONE)
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @param blocking Whether to block until all blocks are deleted.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(blocking: Boolean): this.type = {
    sparkSession.sharedState.cacheManager.uncacheQuery(this, cascade = false, blocking)
    this
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(): this.type = unpersist(blocking = false)

  // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`.
  @transient private lazy val rddQueryExecution: QueryExecution = {
    val deserialized = CatalystSerde.deserialize[T](logicalPlan)
    sparkSession.sessionState.executePlan(deserialized)
  }

  /**
   * Represents the content of the Dataset as an `RDD` of `T`.
   *
   * @group basic
   * @since 1.6.0
   */
  lazy val rdd: RDD[T] = {
    val objectType = exprEnc.deserializer.dataType
    rddQueryExecution.toRdd.mapPartitions { rows =>
      rows.map(_.get(0, objectType).asInstanceOf[T])
    }
  }

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def javaRDD: JavaRDD[T] = toJavaRDD

  /**
   * Registers this Dataset as a temporary table using the given name. The lifetime of this
   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  @deprecated("Use createOrReplaceTempView(viewName) instead.", "2.0.0")
  def registerTempTable(tableName: String): Unit = {
    createOrReplaceTempView(tableName)
  }

  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
   * created it, i.e. it will be automatically dropped when the session terminates. It's not
   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.0.0
   */
  @throws[AnalysisException]
  def createTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = false)
  }



  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 2.0.0
   */
  def createOrReplaceTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = false)
  }

  /**
   * Creates a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.1.0
   */
  @throws[AnalysisException]
  def createGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = true)
  }

  /**
   * Creates or replaces a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @group basic
   * @since 2.2.0
   */
  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = true)
  }

  private def createTempViewCommand(
      viewName: String,
      replace: Boolean,
      global: Boolean): CreateViewCommand = {
    val viewType = if (global) GlobalTempView else LocalTempView

    val tableIdentifier = try {
      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)
    } catch {
      case _: ParseException => throw new AnalysisException(s"Invalid view name: $viewName")
    }
    CreateViewCommand(
      name = tableIdentifier,
      userSpecifiedColumns = Nil,
      comment = None,
      properties = Map.empty,
      originalText = None,
      child = logicalPlan,
      allowExisting = false,
      replace = replace,
      viewType = viewType)
  }

  /**
   * Interface for saving the content of the non-streaming Dataset out into external storage.
   *
   * @group basic
   * @since 1.6.0
   */
  def write: DataFrameWriter[T] = {
    if (isStreaming) {
      logicalPlan.failAnalysis(
        "'write' can not be called on streaming Dataset/DataFrame")
    }
    new DataFrameWriter[T](this)
  }

  /**
   * Interface for saving the content of the streaming Dataset out into external storage.
   *
   * @group basic
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def writeStream: DataStreamWriter[T] = {
    if (!isStreaming) {
      logicalPlan.failAnalysis(
        "'writeStream' can be called only on streaming Dataset/DataFrame")
    }
    new DataStreamWriter[T](this)
  }


  /**
   * Returns the content of the Dataset as a Dataset of JSON strings.
   * @since 2.0.0
   */
  def toJSON: Dataset[String] = {
    val rowSchema = this.schema
    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone
    mapPartitions { iter =>
      val writer = new CharArrayWriter()
      // create the Generator without separator inserted between 2 records
      val gen = new JacksonGenerator(rowSchema, writer,
        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))

      new Iterator[String] {
        override def hasNext: Boolean = iter.hasNext
        override def next(): String = {
          gen.write(exprEnc.toRow(iter.next()))
          gen.flush()

          val json = writer.toString
          if (hasNext) {
            writer.reset()
          } else {
            gen.close()
          }

          json
        }
      }
    } (Encoders.STRING)
  }

  /**
   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply
   * asks each constituent BaseRelation for its respective files and takes the union of all results.
   * Depending on the source relations, this may not find all input files. Duplicates are removed.
   *
   * @group basic
   * @since 2.0.0
   */
  def inputFiles: Array[String] = {
    val files: Seq[String] = queryExecution.optimizedPlan.collect {
      case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) =>
        fsBasedRelation.inputFiles
      case fr: FileRelation =>
        fr.inputFiles
      case r: HiveTableRelation =>
        r.tableMeta.storage.locationUri.map(_.toString).toArray
    }.flatten
    files.toSet.toArray
  }

  ////////////////////////////////////////////////////////////////////////////
  // For Python API
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Converts a JavaRDD to a PythonRDD.
   */
  private[sql] def javaToPython: JavaRDD[Array[Byte]] = {
    val structType = schema  // capture it for closure
    val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType))
    EvaluatePython.javaToPython(rdd)
  }

  private[sql] def collectToPython(): Array[Any] = {
    EvaluatePython.registerPicklers()
    withAction("collectToPython", queryExecution) { plan =>
      val toJava: (Any) => Any = EvaluatePython.toJava(_, schema)
      val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
        plan.executeCollect().iterator.map(toJava))
      PythonRDD.serveIterator(iter, "serve-DataFrame")
    }
  }

  private[sql] def getRowsToPython(
      _numRows: Int,
      truncate: Int): Array[Any] = {
    EvaluatePython.registerPicklers()
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    val rows = getRows(numRows, truncate).map(_.toArray).toArray
    val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType)))
    val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
      rows.iterator.map(toJava))
    PythonRDD.serveIterator(iter, "serve-GetRows")
  }

  /**
   * Collect a Dataset as Arrow batches and serve stream to PySpark.
   */
  private[sql] def collectAsArrowToPython(): Array[Any] = {
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone

    PythonRDD.serveToStreamWithSync("serve-Arrow") { out =>
      withAction("collectAsArrowToPython", queryExecution) { plan =>
        val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId)
        val arrowBatchRdd = toArrowBatchRdd(plan)
        val numPartitions = arrowBatchRdd.partitions.length

        // Store collection results for worst case of 1 to N-1 partitions
        val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1))
        var lastIndex = -1  // index of last partition written

        // Handler to eagerly write partitions to Python in order
        def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = {
          // If result is from next partition in order
          if (index - 1 == lastIndex) {
            batchWriter.writeBatches(arrowBatches.iterator)
            lastIndex += 1
            // Write stored partitions that come next in order
            while (lastIndex < results.length && results(lastIndex) != null) {
              batchWriter.writeBatches(results(lastIndex).iterator)
              results(lastIndex) = null
              lastIndex += 1
            }
            // After last batch, end the stream
            if (lastIndex == results.length) {
              batchWriter.end()
            }
          } else {
            // Store partitions received out of order
            results(index - 1) = arrowBatches
          }
        }

        sparkSession.sparkContext.runJob(
          arrowBatchRdd,
          (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray,
          0 until numPartitions,
          handlePartitionBatches)
      }
    }
  }

  private[sql] def toPythonIterator(): Array[Any] = {
    withNewExecutionId {
      PythonRDD.toLocalIteratorAndServe(javaToPython.rdd)
    }
  }

  ////////////////////////////////////////////////////////////////////////////
  // Private Helpers
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with
   * an execution.
   */
  private def withNewExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, queryExecution)(body)
  }

  /**
   * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect
   * them with an execution. Before performing the action, the metrics of the executed plan will be
   * reset.
   */
  private def withNewRDDExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, rddQueryExecution) {
      rddQueryExecution.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      body
    }
  }

  /**
   * Wrap a Dataset action to track the QueryExecution and time cost, then report to the
   * user-registered callback functions.
   */
  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {
    try {
      qe.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      val start = System.nanoTime()
      val result = SQLExecution.withNewExecutionId(sparkSession, qe) {
        action(qe.executedPlan)
      }
      val end = System.nanoTime()
      sparkSession.listenerManager.onSuccess(name, qe, end - start)
      result
    } catch {
      case e: Throwable =>
        sparkSession.listenerManager.onFailure(name, qe, e)
        throw e
    }
  }

  /**
   * Collect all elements from a spark plan.
   */
  private def collectFromPlan(plan: SparkPlan): Array[T] = {
    // This projection writes output to a `InternalRow`, which means applying this projection is not
    // thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe.
    val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
    plan.executeCollect().map { row =>
      // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
      // parameter of its `get` method, so it's safe to use null here.
      objProj(row).get(0, null).asInstanceOf[T]
    }
  }

  private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = {
    val sortOrder: Seq[SortOrder] = sortExprs.map { col =>
      col.expr match {
        case expr: SortOrder =>
          expr
        case expr: Expression =>
          SortOrder(expr, Ascending)
      }
    }
    withTypedPlan {
      Sort(sortOrder, global = global, logicalPlan)
    }
  }

  /** A convenient function to wrap a logical plan and produce a DataFrame. */
  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = {
    Dataset.ofRows(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a logical plan and produce a Dataset. */
  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    Dataset(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a set based logical plan and produce a Dataset. */
  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) {
      // Set operators widen types (change the schema), so we cannot reuse the row encoder.
      Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]]
    } else {
      Dataset(sparkSession, logicalPlan)
    }
  }

  /** Convert to an RDD of serialized ArrowRecordBatches. */
  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = {
    val schemaCaptured = this.schema
    val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone
    plan.execute().mapPartitionsInternal { iter =>
      val context = TaskContext.get()
      ArrowConverters.toBatchIterator(
        iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context)
    }
  }

  // This is only used in tests, for now.
  private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = {
    toArrowBatchRdd(queryExecution.executedPlan)
  }
}

[0m2021.03.05 10:03:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:03:36 INFO  time: compiled root in 0.37s[0m
[0m2021.03.05 10:04:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:04:14 INFO  time: compiled root in 1.27s[0m
[0m2021.03.05 10:04:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:04:48 INFO  time: compiled root in 1.25s[0m
[0m2021.03.05 10:04:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:04:55 INFO  time: compiled root in 1.22s[0m
[0m2021.03.05 10:04:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:04:57 INFO  time: compiled root in 1.29s[0m
[0m2021.03.05 10:07:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:07:44 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 10:07:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:07:53 INFO  time: compiled root in 1.25s[0m
[0m2021.03.05 10:08:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:08:25 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 10:09:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:09:13 INFO  time: compiled root in 1.24s[0m
[0m2021.03.05 10:09:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:09:20 INFO  time: compiled root in 1.24s[0m
[0m2021.03.05 10:09:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:09:25 INFO  time: compiled root in 1.18s[0m
[0m2021.03.05 10:10:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:10:41 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 10:11:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:11:07 INFO  time: compiled root in 0.33s[0m
[0m2021.03.05 10:11:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:11:21 INFO  time: compiled root in 0.13s[0m
[0m2021.03.05 10:11:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:11:24 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 10:11:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:11:27 INFO  time: compiled root in 0.14s[0m
Mar 05, 2021 10:12:10 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
java.util.concurrent.CompletionException: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:673)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:42)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
	at scala.reflect.internal.Symbols$Symbol$$anonfun$info$3.apply(Symbols.scala:1523)
	at scala.reflect.internal.Symbols$Symbol$$anonfun$info$3.apply(Symbols.scala:1521)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.reflect.internal.Symbols$Symbol.lock(Symbols.scala:567)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1521)
	at scala.tools.nsc.typechecker.Contexts$ImportInfo.qual(Contexts.scala:1416)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.pre$lzycompute$1(Completions.scala:719)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.scala$meta$internal$pc$completions$Completions$class$$anonfun$$pre$1(Completions.scala:719)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1$$anonfun$apply$1.apply(Completions.scala:722)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1$$anonfun$apply$1.apply(Completions.scala:720)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.apply(Completions.scala:720)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.apply(Completions.scala:718)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.meta.internal.pc.completions.Completions$class.renamedSymbols(Completions.scala:718)
	at scala.meta.internal.pc.MetalsGlobal.renamedSymbols(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.Signatures$ShortenedNames$.synthesize(Signatures.scala:56)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:151)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:75)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.toStream(Iterator.scala:1320)
	at scala.collection.AbstractIterator.toStream(Iterator.scala:1334)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)
	at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:134)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:40)
	at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:69)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.write(ReflectiveTypeAdapterFactory.java:125)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.write(ReflectiveTypeAdapterFactory.java:243)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:423)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:55)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at com.google.gson.Gson.toJson(Gson.java:648)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:145)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:140)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:59)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.lambda$handleRequest$1(RemoteEndpoint.java:281)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
	... 9 more

[0m2021.03.05 10:14:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:14:04 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 10:24:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:24:11 INFO  time: compiled root in 1.27s[0m
[0m2021.03.05 10:24:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:24:25 INFO  time: compiled root in 0.3s[0m
[0m2021.03.05 10:30:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:30:02 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 10:30:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:30:07 INFO  time: compiled root in 1.35s[0m
[0m2021.03.05 10:33:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:33:13 INFO  time: compiled root in 1.42s[0m
[0m2021.03.05 10:33:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:33:27 INFO  time: compiled root in 0.16s[0m
[0m2021.03.05 10:34:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:34:03 INFO  time: compiled root in 0.32s[0m
[0m2021.03.05 10:35:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:35:05 INFO  time: compiled root in 0.32s[0m
[0m2021.03.05 10:35:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:35:34 INFO  time: compiled root in 0.27s[0m
[0m2021.03.05 10:35:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:35:56 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 10:36:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:36:08 INFO  time: compiled root in 0.31s[0m
[0m2021.03.05 10:36:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:36:19 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 10:37:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:37:41 INFO  time: compiled root in 0.32s[0m
[0m2021.03.05 10:38:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:38:51 INFO  time: compiled root in 1.37s[0m
[0m2021.03.05 10:39:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:39:19 INFO  time: compiled root in 0.33s[0m
[0m2021.03.05 10:39:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:39:57 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 10:40:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:40:27 INFO  time: compiled root in 1.27s[0m
[0m2021.03.05 10:40:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:40:44 INFO  time: compiled root in 2.32s[0m
[0m2021.03.05 10:40:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:40:49 INFO  time: compiled root in 1.37s[0m
[0m2021.03.05 10:43:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:43:22 INFO  time: compiled root in 0.29s[0m
[0m2021.03.05 10:43:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:43:47 INFO  time: compiled root in 1.31s[0m
[0m2021.03.05 10:43:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:43:52 INFO  time: compiled root in 1.42s[0m
[0m2021.03.05 10:45:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:45:18 INFO  time: compiled root in 1.41s[0m
[0m2021.03.05 10:46:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:46:19 INFO  time: compiled root in 1.41s[0m
[0m2021.03.05 10:46:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:46:22 INFO  time: compiled root in 1.37s[0m
[0m2021.03.05 10:48:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:48:44 INFO  time: compiled root in 0.35s[0m
[0m2021.03.05 10:48:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:48:48 INFO  time: compiled root in 1.27s[0m
[0m2021.03.05 10:50:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:50:45 INFO  time: compiled root in 1.4s[0m
[0m2021.03.05 10:51:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:51:07 INFO  time: compiled root in 0.29s[0m
[0m2021.03.05 10:52:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:52:50 INFO  time: compiled root in 1.3s[0m
[0m2021.03.05 10:54:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:54:59 INFO  time: compiled root in 1.55s[0m
[0m2021.03.05 10:55:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:55:14 INFO  time: compiled root in 0.34s[0m
[0m2021.03.05 10:55:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:55:44 INFO  time: compiled root in 0.35s[0m
[0m2021.03.05 10:56:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:56:27 INFO  time: compiled root in 0.4s[0m
[0m2021.03.05 10:57:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:57:02 INFO  time: compiled root in 0.29s[0m
[0m2021.03.05 10:58:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:58:15 INFO  time: compiled root in 1.41s[0m
[0m2021.03.05 10:58:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:58:18 INFO  time: compiled root in 0.38s[0m
[0m2021.03.05 10:58:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:58:28 INFO  time: compiled root in 1.47s[0m
[0m2021.03.05 10:59:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:59:36 INFO  time: compiled root in 1.5s[0m
[0m2021.03.05 10:59:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 10:59:54 INFO  time: compiled root in 1.6s[0m
[0m2021.03.05 11:00:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:00:02 INFO  time: compiled root in 1.56s[0m
[0m2021.03.05 11:02:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:02:29 INFO  time: compiled root in 0.15s[0m
[0m2021.03.05 11:02:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:02:35 INFO  time: compiled root in 1.51s[0m
[0m2021.03.05 11:02:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:02:48 INFO  time: compiled root in 1.41s[0m
[0m2021.03.05 11:04:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:04:44 INFO  time: compiled root in 1.89s[0m
Mar 05, 2021 11:09:10 AM scala.meta.internal.pc.completions.Completions$class completionPosition
SEVERE: null
java.lang.NullPointerException
	at scala.meta.internal.pc.MetalsGlobal.metalsSeenFromType(MetalsGlobal.scala:727)
	at scala.meta.internal.pc.completions.MatchCaseCompletions$CaseKeywordCompletion.<init>(MatchCaseCompletions.scala:49)
	at scala.meta.internal.pc.completions.Completions$class.fromIdentApply$1(Completions.scala:423)
	at scala.meta.internal.pc.completions.Completions$class.completionPositionUnsafe(Completions.scala:443)
	at scala.meta.internal.pc.MetalsGlobal.completionPositionUnsafe(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.completions.Completions$class.completionPosition(Completions.scala:396)
	at scala.meta.internal.pc.MetalsGlobal.completionPosition(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:441)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)
	at scala.meta.internal.pc.ScalaPresentationCompiler$$anonfun$complete$1.apply(ScalaPresentationCompiler.scala:124)
	at scala.meta.internal.pc.ScalaPresentationCompiler$$anonfun$complete$1.apply(ScalaPresentationCompiler.scala:124)
	at scala.meta.internal.pc.CompilerAccess.withSharedCompiler(CompilerAccess.scala:137)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$1.apply(CompilerAccess.scala:87)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$onCompilerJobQueue$1.apply$mcV$sp(CompilerAccess.scala:197)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Mar 05, 2021 11:09:10 AM scala.meta.internal.pc.completions.Completions$class completionPosition
SEVERE: null
java.lang.NullPointerException
	at scala.meta.internal.pc.MetalsGlobal.metalsSeenFromType(MetalsGlobal.scala:727)
	at scala.meta.internal.pc.completions.MatchCaseCompletions$CaseKeywordCompletion.<init>(MatchCaseCompletions.scala:49)
	at scala.meta.internal.pc.completions.Completions$class.fromIdentApply$1(Completions.scala:423)
	at scala.meta.internal.pc.completions.Completions$class.completionPositionUnsafe(Completions.scala:443)
	at scala.meta.internal.pc.MetalsGlobal.completionPositionUnsafe(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.completions.Completions$class.completionPosition(Completions.scala:396)
	at scala.meta.internal.pc.MetalsGlobal.completionPosition(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:441)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)
	at scala.meta.internal.pc.ScalaPresentationCompiler$$anonfun$complete$1.apply(ScalaPresentationCompiler.scala:124)
	at scala.meta.internal.pc.ScalaPresentationCompiler$$anonfun$complete$1.apply(ScalaPresentationCompiler.scala:124)
	at scala.meta.internal.pc.CompilerAccess.withSharedCompiler(CompilerAccess.scala:137)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$1.apply(CompilerAccess.scala:87)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$onCompilerJobQueue$1.apply$mcV$sp(CompilerAccess.scala:197)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Mar 05, 2021 11:09:35 AM scala.meta.internal.pc.completions.Completions$class completionPosition
SEVERE: null
java.lang.NullPointerException
	at scala.meta.internal.pc.MetalsGlobal.metalsSeenFromType(MetalsGlobal.scala:727)
	at scala.meta.internal.pc.completions.MatchCaseCompletions$CaseKeywordCompletion.<init>(MatchCaseCompletions.scala:49)
	at scala.meta.internal.pc.completions.Completions$class.fromIdentApply$1(Completions.scala:423)
	at scala.meta.internal.pc.completions.Completions$class.completionPositionUnsafe(Completions.scala:443)
	at scala.meta.internal.pc.MetalsGlobal.completionPositionUnsafe(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.completions.Completions$class.completionPosition(Completions.scala:396)
	at scala.meta.internal.pc.MetalsGlobal.completionPosition(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:441)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)
	at scala.meta.internal.pc.ScalaPresentationCompiler$$anonfun$complete$1.apply(ScalaPresentationCompiler.scala:124)
	at scala.meta.internal.pc.ScalaPresentationCompiler$$anonfun$complete$1.apply(ScalaPresentationCompiler.scala:124)
	at scala.meta.internal.pc.CompilerAccess.withSharedCompiler(CompilerAccess.scala:137)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$1.apply(CompilerAccess.scala:87)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$onCompilerJobQueue$1.apply$mcV$sp(CompilerAccess.scala:197)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Mar 05, 2021 11:09:36 AM scala.meta.internal.pc.completions.Completions$class completionPosition
SEVERE: null
java.lang.NullPointerException
	at scala.meta.internal.pc.MetalsGlobal.metalsSeenFromType(MetalsGlobal.scala:727)
	at scala.meta.internal.pc.completions.MatchCaseCompletions$CaseKeywordCompletion.<init>(MatchCaseCompletions.scala:49)
	at scala.meta.internal.pc.completions.Completions$class.fromIdentApply$1(Completions.scala:423)
	at scala.meta.internal.pc.completions.Completions$class.completionPositionUnsafe(Completions.scala:443)
	at scala.meta.internal.pc.MetalsGlobal.completionPositionUnsafe(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.completions.Completions$class.completionPosition(Completions.scala:396)
	at scala.meta.internal.pc.MetalsGlobal.completionPosition(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:441)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)
	at scala.meta.internal.pc.ScalaPresentationCompiler$$anonfun$complete$1.apply(ScalaPresentationCompiler.scala:124)
	at scala.meta.internal.pc.ScalaPresentationCompiler$$anonfun$complete$1.apply(ScalaPresentationCompiler.scala:124)
	at scala.meta.internal.pc.CompilerAccess.withSharedCompiler(CompilerAccess.scala:137)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$1.apply(CompilerAccess.scala:87)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$onCompilerJobQueue$1.apply$mcV$sp(CompilerAccess.scala:197)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

[0m2021.03.05 11:10:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:10:27 INFO  time: compiled root in 2.27s[0m
[0m2021.03.05 11:10:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:10:29 INFO  time: compiled root in 0.34s[0m
[0m2021.03.05 11:10:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:10:40 INFO  time: compiled root in 0.29s[0m
[0m2021.03.05 11:10:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:10:51 INFO  time: compiled root in 1.37s[0m
[0m2021.03.05 11:10:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:10:54 INFO  time: compiled root in 1.31s[0m
[0m2021.03.05 11:11:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:11:04 INFO  time: compiled root in 1.34s[0m
[0m2021.03.05 11:11:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:11:06 INFO  time: compiled root in 1.39s[0m
[0m2021.03.05 11:13:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:13:32 INFO  time: compiled root in 1.4s[0m
[0m2021.03.05 11:13:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:13:37 INFO  time: compiled root in 1.55s[0m
[0m2021.03.05 11:15:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:15:47 INFO  time: compiled root in 0.37s[0m
[0m2021.03.05 11:15:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:15:51 INFO  time: compiled root in 0.31s[0m
[0m2021.03.05 11:15:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:15:55 INFO  time: compiled root in 1.67s[0m
[0m2021.03.05 11:16:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:16:04 INFO  time: compiled root in 1.58s[0m
[0m2021.03.05 11:16:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:16:11 INFO  time: compiled root in 1.66s[0m
[0m2021.03.05 11:16:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:16:16 INFO  time: compiled root in 1.62s[0m
[0m2021.03.05 11:20:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:20:32 INFO  time: compiled root in 1.64s[0m
[0m2021.03.05 11:20:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:20:46 INFO  time: compiled root in 1.85s[0m
[0m2021.03.05 11:21:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:21:53 INFO  time: compiled root in 1.58s[0m
[0m2021.03.05 11:24:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:24:34 INFO  time: compiled root in 1.59s[0m
[0m2021.03.05 11:25:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:25:36 INFO  time: compiled root in 1.62s[0m
[0m2021.03.05 11:27:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:27:11 INFO  time: compiled root in 1.65s[0m
[0m2021.03.05 11:28:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:28:53 INFO  time: compiled root in 0.41s[0m
[0m2021.03.05 11:29:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:29:36 INFO  time: compiled root in 0.41s[0m
[0m2021.03.05 11:29:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:98:56: stale bloop error: not found: value col
    .withColumn("Tech Ads Proportional to Population", col($"Tech Job Total") / col($"Population Estimate Total"))
                                                       ^^^[0m
[0m2021.03.05 11:29:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:98:81: stale bloop error: not found: value col
    .withColumn("Tech Ads Proportional to Population", col($"Tech Job Total") / col($"Population Estimate Total"))
                                                                                ^^^[0m
[0m2021.03.05 11:29:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:98:56: stale bloop error: not found: value col
    .withColumn("Tech Ads Proportional to Population", col($"Tech Job Total") / col($"Population Estimate Total"))
                                                       ^^^[0m
[0m2021.03.05 11:29:48 INFO  /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala:98:81: stale bloop error: not found: value col
    .withColumn("Tech Ads Proportional to Population", col($"Tech Job Total") / col($"Population Estimate Total"))
                                                                                ^^^[0m
[0m2021.03.05 11:29:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:29:57 INFO  time: compiled root in 1.6s[0m
[0m2021.03.05 11:30:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:30:12 INFO  time: compiled root in 1.63s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.Map
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder

/**
 * A collection of implicit methods for converting common Scala objects into [[Dataset]]s.
 *
 * @since 1.6.0
 */
@InterfaceStability.Evolving
abstract class SQLImplicits extends LowPrioritySQLImplicits {

  protected def _sqlContext: SQLContext

  /**
   * Converts $"col name" into a [[Column]].
   *
   * @since 2.0.0
   */
  implicit class StringToColumn(val sc: StringContext) {
    def $(args: Any*): ColumnName = {
      new ColumnName(sc.s(args: _*))
    }
  }

  // Primitives

  /** @since 1.6.0 */
  implicit def newIntEncoder: Encoder[Int] = Encoders.scalaInt

  /** @since 1.6.0 */
  implicit def newLongEncoder: Encoder[Long] = Encoders.scalaLong

  /** @since 1.6.0 */
  implicit def newDoubleEncoder: Encoder[Double] = Encoders.scalaDouble

  /** @since 1.6.0 */
  implicit def newFloatEncoder: Encoder[Float] = Encoders.scalaFloat

  /** @since 1.6.0 */
  implicit def newByteEncoder: Encoder[Byte] = Encoders.scalaByte

  /** @since 1.6.0 */
  implicit def newShortEncoder: Encoder[Short] = Encoders.scalaShort

  /** @since 1.6.0 */
  implicit def newBooleanEncoder: Encoder[Boolean] = Encoders.scalaBoolean

  /** @since 1.6.0 */
  implicit def newStringEncoder: Encoder[String] = Encoders.STRING

  /** @since 2.2.0 */
  implicit def newJavaDecimalEncoder: Encoder[java.math.BigDecimal] = Encoders.DECIMAL

  /** @since 2.2.0 */
  implicit def newScalaDecimalEncoder: Encoder[scala.math.BigDecimal] = ExpressionEncoder()

  /** @since 2.2.0 */
  implicit def newDateEncoder: Encoder[java.sql.Date] = Encoders.DATE

  /** @since 2.2.0 */
  implicit def newTimeStampEncoder: Encoder[java.sql.Timestamp] = Encoders.TIMESTAMP


  // Boxed primitives

  /** @since 2.0.0 */
  implicit def newBoxedIntEncoder: Encoder[java.lang.Integer] = Encoders.INT

  /** @since 2.0.0 */
  implicit def newBoxedLongEncoder: Encoder[java.lang.Long] = Encoders.LONG

  /** @since 2.0.0 */
  implicit def newBoxedDoubleEncoder: Encoder[java.lang.Double] = Encoders.DOUBLE

  /** @since 2.0.0 */
  implicit def newBoxedFloatEncoder: Encoder[java.lang.Float] = Encoders.FLOAT

  /** @since 2.0.0 */
  implicit def newBoxedByteEncoder: Encoder[java.lang.Byte] = Encoders.BYTE

  /** @since 2.0.0 */
  implicit def newBoxedShortEncoder: Encoder[java.lang.Short] = Encoders.SHORT

  /** @since 2.0.0 */
  implicit def newBoxedBooleanEncoder: Encoder[java.lang.Boolean] = Encoders.BOOLEAN

  // Seqs

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newIntSeqEncoder: Encoder[Seq[Int]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newLongSeqEncoder: Encoder[Seq[Long]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newDoubleSeqEncoder: Encoder[Seq[Double]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newFloatSeqEncoder: Encoder[Seq[Float]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newByteSeqEncoder: Encoder[Seq[Byte]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newShortSeqEncoder: Encoder[Seq[Short]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newBooleanSeqEncoder: Encoder[Seq[Boolean]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newStringSeqEncoder: Encoder[Seq[String]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newProductSeqEncoder[A <: Product : TypeTag]: Encoder[Seq[A]] = ExpressionEncoder()

  /** @since 2.2.0 */
  implicit def newSequenceEncoder[T <: Seq[_] : TypeTag]: Encoder[T] = ExpressionEncoder()

  // Maps
  /** @since 2.3.0 */
  implicit def newMapEncoder[T <: Map[_, _] : TypeTag]: Encoder[T] = ExpressionEncoder()

  /**
   * Notice that we serialize `Set` to Catalyst array. The set property is only kept when
   * manipulating the domain objects. The serialization format doesn't keep the set property.
   * When we have a Catalyst array which contains duplicated elements and convert it to
   * `Dataset[Set[T]]` by using the encoder, the elements will be de-duplicated.
   *
   * @since 2.3.0
   */
  implicit def newSetEncoder[T <: Set[_] : TypeTag]: Encoder[T] = ExpressionEncoder()

  // Arrays

  /** @since 1.6.1 */
  implicit def newIntArrayEncoder: Encoder[Array[Int]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newLongArrayEncoder: Encoder[Array[Long]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newDoubleArrayEncoder: Encoder[Array[Double]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newFloatArrayEncoder: Encoder[Array[Float]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newByteArrayEncoder: Encoder[Array[Byte]] = Encoders.BINARY

  /** @since 1.6.1 */
  implicit def newShortArrayEncoder: Encoder[Array[Short]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newBooleanArrayEncoder: Encoder[Array[Boolean]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newStringArrayEncoder: Encoder[Array[String]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newProductArrayEncoder[A <: Product : TypeTag]: Encoder[Array[A]] =
    ExpressionEncoder()

  /**
   * Creates a [[Dataset]] from an RDD.
   *
   * @since 1.6.0
   */
  implicit def rddToDatasetHolder[T : Encoder](rdd: RDD[T]): DatasetHolder[T] = {
    DatasetHolder(_sqlContext.createDataset(rdd))
  }

  /**
   * Creates a [[Dataset]] from a local Seq.
   * @since 1.6.0
   */
  implicit def localSeqToDatasetHolder[T : Encoder](s: Seq[T]): DatasetHolder[T] = {
    DatasetHolder(_sqlContext.createDataset(s))
  }

  /**
   * An implicit conversion that turns a Scala `Symbol` into a [[Column]].
   * @since 1.3.0
   */
  implicit def symbolToColumn(s: Symbol): ColumnName = new ColumnName(s.name)

}

/**
 * Lower priority implicit methods for converting Scala objects into [[Dataset]]s.
 * Conflicting implicits are placed here to disambiguate resolution.
 *
 * Reasons for including specific implicits:
 * newProductEncoder - to disambiguate for `List`s which are both `Seq` and `Product`
 */
trait LowPrioritySQLImplicits {
  /** @since 1.6.0 */
  implicit def newProductEncoder[T <: Product : TypeTag]: Encoder[T] = Encoders.product[T]

}

[0m2021.03.05 11:33:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:33:02 INFO  time: compiled root in 1.63s[0m
[0m2021.03.05 11:33:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:33:23 INFO  time: compiled root in 0.33s[0m
[0m2021.03.05 11:34:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:34:26 INFO  time: compiled root in 1.7s[0m
[0m2021.03.05 11:37:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:37:49 INFO  time: compiled root in 1.65s[0m
[0m2021.03.05 11:40:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:40:13 INFO  time: compiled root in 1.77s[0m
Mar 05, 2021 11:40:16 AM scala.meta.internal.pc.CompletionProvider expected$1
WARNING: String index out of range: -1
[0m2021.03.05 11:40:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:40:29 INFO  time: compiled root in 2.52s[0m
[0m2021.03.05 11:43:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:43:27 INFO  time: compiled root in 3.1s[0m
[0m2021.03.05 11:44:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:44:10 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 11:44:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:44:24 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 11:44:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:44:51 INFO  time: compiled root in 0.35s[0m
[0m2021.03.05 11:44:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:44:58 INFO  time: compiled root in 0.34s[0m
[0m2021.03.05 11:46:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:46:18 INFO  time: compiled root in 1.52s[0m
[0m2021.03.05 11:46:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:46:25 INFO  time: compiled root in 1.56s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.{typeTag, TypeTag}
import scala.util.Try
import scala.util.control.NonFatal

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.sql.api.java._
import org.apache.spark.sql.catalyst.ScalaReflection
import org.apache.spark.sql.catalyst.analysis.{Star, UnresolvedFunction}
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.aggregate._
import org.apache.spark.sql.catalyst.plans.logical.{HintInfo, ResolvedHint}
import org.apache.spark.sql.execution.SparkSqlParser
import org.apache.spark.sql.expressions.{SparkUserDefinedFunction, UserDefinedFunction}
import org.apache.spark.sql.internal.SQLConf
import org.apache.spark.sql.types._
import org.apache.spark.util.Utils


/**
 * Commonly used functions available for DataFrame operations. Using functions defined here provides
 * a little bit more compile-time safety to make sure the function exists.
 *
 * Spark also includes more built-in functions that are less common and are not defined here.
 * You can still access them (and all the functions defined here) using the `functions.expr()` API
 * and calling them through a SQL expression string. You can find the entire list of functions
 * at SQL API documentation.
 *
 * As an example, `isnan` is a function that is defined here. You can use `isnan(col("myCol"))`
 * to invoke the `isnan` function. This way the programming language's compiler ensures `isnan`
 * exists and is of the proper form. You can also use `expr("isnan(myCol)")` function to invoke the
 * same function. In this case, Spark itself will ensure `isnan` exists when it analyzes the query.
 *
 * `regr_count` is an example of a function that is built-in but not defined here, because it is
 * less commonly used. To invoke it, use `expr("regr_count(yCol, xCol)")`.
 *
 * @groupname udf_funcs UDF functions
 * @groupname agg_funcs Aggregate functions
 * @groupname datetime_funcs Date time functions
 * @groupname sort_funcs Sorting functions
 * @groupname normal_funcs Non-aggregate functions
 * @groupname math_funcs Math functions
 * @groupname misc_funcs Misc functions
 * @groupname window_funcs Window functions
 * @groupname string_funcs String functions
 * @groupname collection_funcs Collection functions
 * @groupname Ungrouped Support functions for DataFrames
 * @since 1.3.0
 */
@InterfaceStability.Stable
// scalastyle:off
object functions {
// scalastyle:on

  private def withExpr(expr: Expression): Column = Column(expr)

  private def withAggregateFunction(
    func: AggregateFunction,
    isDistinct: Boolean = false): Column = {
    Column(func.toAggregateExpression(isDistinct))
  }

  /**
   * Returns a [[Column]] based on the given column name.
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def col(colName: String): Column = Column(colName)

  /**
   * Returns a [[Column]] based on the given column name. Alias of [[col]].
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def column(colName: String): Column = Column(colName)

  /**
   * Creates a [[Column]] of literal value.
   *
   * The passed in object is returned directly if it is already a [[Column]].
   * If the object is a Scala Symbol, it is converted into a [[Column]] also.
   * Otherwise, a new [[Column]] is created to represent the literal value.
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def lit(literal: Any): Column = typedLit(literal)

  /**
   * Creates a [[Column]] of literal value.
   *
   * The passed in object is returned directly if it is already a [[Column]].
   * If the object is a Scala Symbol, it is converted into a [[Column]] also.
   * Otherwise, a new [[Column]] is created to represent the literal value.
   * The difference between this function and [[lit]] is that this function
   * can handle parameterized scala types e.g.: List, Seq and Map.
   *
   * @group normal_funcs
   * @since 2.2.0
   */
  def typedLit[T : TypeTag](literal: T): Column = literal match {
    case c: Column => c
    case s: Symbol => new ColumnName(s.name)
    case _ => Column(Literal.create(literal))
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Sort functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Returns a sort expression based on ascending order of the column.
   * {{{
   *   df.sort(asc("dept"), desc("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 1.3.0
   */
  def asc(columnName: String): Column = Column(columnName).asc

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values return before non-null values.
   * {{{
   *   df.sort(asc_nulls_first("dept"), desc("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 2.1.0
   */
  def asc_nulls_first(columnName: String): Column = Column(columnName).asc_nulls_first

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   df.sort(asc_nulls_last("dept"), desc("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 2.1.0
   */
  def asc_nulls_last(columnName: String): Column = Column(columnName).asc_nulls_last

  /**
   * Returns a sort expression based on the descending order of the column.
   * {{{
   *   df.sort(asc("dept"), desc("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 1.3.0
   */
  def desc(columnName: String): Column = Column(columnName).desc

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear before non-null values.
   * {{{
   *   df.sort(asc("dept"), desc_nulls_first("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 2.1.0
   */
  def desc_nulls_first(columnName: String): Column = Column(columnName).desc_nulls_first

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   df.sort(asc("dept"), desc_nulls_last("age"))
   * }}}
   *
   * @group sort_funcs
   * @since 2.1.0
   */
  def desc_nulls_last(columnName: String): Column = Column(columnName).desc_nulls_last


  //////////////////////////////////////////////////////////////////////////////////////////////
  // Aggregate functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * @group agg_funcs
   * @since 1.3.0
   */
  @deprecated("Use approx_count_distinct", "2.1.0")
  def approxCountDistinct(e: Column): Column = approx_count_distinct(e)

  /**
   * @group agg_funcs
   * @since 1.3.0
   */
  @deprecated("Use approx_count_distinct", "2.1.0")
  def approxCountDistinct(columnName: String): Column = approx_count_distinct(columnName)

  /**
   * @group agg_funcs
   * @since 1.3.0
   */
  @deprecated("Use approx_count_distinct", "2.1.0")
  def approxCountDistinct(e: Column, rsd: Double): Column = approx_count_distinct(e, rsd)

  /**
   * @group agg_funcs
   * @since 1.3.0
   */
  @deprecated("Use approx_count_distinct", "2.1.0")
  def approxCountDistinct(columnName: String, rsd: Double): Column = {
    approx_count_distinct(Column(columnName), rsd)
  }

  /**
   * Aggregate function: returns the approximate number of distinct items in a group.
   *
   * @group agg_funcs
   * @since 2.1.0
   */
  def approx_count_distinct(e: Column): Column = withAggregateFunction {
    HyperLogLogPlusPlus(e.expr)
  }

  /**
   * Aggregate function: returns the approximate number of distinct items in a group.
   *
   * @group agg_funcs
   * @since 2.1.0
   */
  def approx_count_distinct(columnName: String): Column = approx_count_distinct(column(columnName))

  /**
   * Aggregate function: returns the approximate number of distinct items in a group.
   *
   * @param rsd maximum estimation error allowed (default = 0.05)
   *
   * @group agg_funcs
   * @since 2.1.0
   */
  def approx_count_distinct(e: Column, rsd: Double): Column = withAggregateFunction {
    HyperLogLogPlusPlus(e.expr, rsd, 0, 0)
  }

  /**
   * Aggregate function: returns the approximate number of distinct items in a group.
   *
   * @param rsd maximum estimation error allowed (default = 0.05)
   *
   * @group agg_funcs
   * @since 2.1.0
   */
  def approx_count_distinct(columnName: String, rsd: Double): Column = {
    approx_count_distinct(Column(columnName), rsd)
  }

  /**
   * Aggregate function: returns the average of the values in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def avg(e: Column): Column = withAggregateFunction { Average(e.expr) }

  /**
   * Aggregate function: returns the average of the values in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def avg(columnName: String): Column = avg(Column(columnName))

  /**
   * Aggregate function: returns a list of objects with duplicates.
   *
   * @note The function is non-deterministic because the order of collected results depends
   * on order of rows which may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def collect_list(e: Column): Column = withAggregateFunction { CollectList(e.expr) }

  /**
   * Aggregate function: returns a list of objects with duplicates.
   *
   * @note The function is non-deterministic because the order of collected results depends
   * on order of rows which may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def collect_list(columnName: String): Column = collect_list(Column(columnName))

  /**
   * Aggregate function: returns a set of objects with duplicate elements eliminated.
   *
   * @note The function is non-deterministic because the order of collected results depends
   * on order of rows which may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def collect_set(e: Column): Column = withAggregateFunction { CollectSet(e.expr) }

  /**
   * Aggregate function: returns a set of objects with duplicate elements eliminated.
   *
   * @note The function is non-deterministic because the order of collected results depends
   * on order of rows which may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def collect_set(columnName: String): Column = collect_set(Column(columnName))

  /**
   * Aggregate function: returns the Pearson Correlation Coefficient for two columns.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def corr(column1: Column, column2: Column): Column = withAggregateFunction {
    Corr(column1.expr, column2.expr)
  }

  /**
   * Aggregate function: returns the Pearson Correlation Coefficient for two columns.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def corr(columnName1: String, columnName2: String): Column = {
    corr(Column(columnName1), Column(columnName2))
  }

  /**
   * Aggregate function: returns the number of items in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def count(e: Column): Column = withAggregateFunction {
    e.expr match {
      // Turn count(*) into count(1)
      case s: Star => Count(Literal(1))
      case _ => Count(e.expr)
    }
  }

  /**
   * Aggregate function: returns the number of items in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def count(columnName: String): TypedColumn[Any, Long] =
    count(Column(columnName)).as(ExpressionEncoder[Long]())

  /**
   * Aggregate function: returns the number of distinct items in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  @scala.annotation.varargs
  def countDistinct(expr: Column, exprs: Column*): Column = {
    withAggregateFunction(Count.apply((expr +: exprs).map(_.expr)), isDistinct = true)
  }

  /**
   * Aggregate function: returns the number of distinct items in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  @scala.annotation.varargs
  def countDistinct(columnName: String, columnNames: String*): Column =
    countDistinct(Column(columnName), columnNames.map(Column.apply) : _*)

  /**
   * Aggregate function: returns the population covariance for two columns.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def covar_pop(column1: Column, column2: Column): Column = withAggregateFunction {
    CovPopulation(column1.expr, column2.expr)
  }

  /**
   * Aggregate function: returns the population covariance for two columns.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def covar_pop(columnName1: String, columnName2: String): Column = {
    covar_pop(Column(columnName1), Column(columnName2))
  }

  /**
   * Aggregate function: returns the sample covariance for two columns.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def covar_samp(column1: Column, column2: Column): Column = withAggregateFunction {
    CovSample(column1.expr, column2.expr)
  }

  /**
   * Aggregate function: returns the sample covariance for two columns.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def covar_samp(columnName1: String, columnName2: String): Column = {
    covar_samp(Column(columnName1), Column(columnName2))
  }

  /**
   * Aggregate function: returns the first value in a group.
   *
   * The function by default returns the first values it sees. It will return the first non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def first(e: Column, ignoreNulls: Boolean): Column = withAggregateFunction {
    new First(e.expr, ignoreNulls)
  }

  /**
   * Aggregate function: returns the first value of a column in a group.
   *
   * The function by default returns the first values it sees. It will return the first non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def first(columnName: String, ignoreNulls: Boolean): Column = {
    first(Column(columnName), ignoreNulls)
  }

  /**
   * Aggregate function: returns the first value in a group.
   *
   * The function by default returns the first values it sees. It will return the first non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def first(e: Column): Column = first(e, ignoreNulls = false)

  /**
   * Aggregate function: returns the first value of a column in a group.
   *
   * The function by default returns the first values it sees. It will return the first non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def first(columnName: String): Column = first(Column(columnName))

  /**
   * Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated
   * or not, returns 1 for aggregated or 0 for not aggregated in the result set.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def grouping(e: Column): Column = Column(Grouping(e.expr))

  /**
   * Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated
   * or not, returns 1 for aggregated or 0 for not aggregated in the result set.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def grouping(columnName: String): Column = grouping(Column(columnName))

  /**
   * Aggregate function: returns the level of grouping, equals to
   *
   * {{{
   *   (grouping(c1) <<; (n-1)) + (grouping(c2) <<; (n-2)) + ... + grouping(cn)
   * }}}
   *
   * @note The list of columns should match with grouping columns exactly, or empty (means all the
   * grouping columns).
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def grouping_id(cols: Column*): Column = Column(GroupingID(cols.map(_.expr)))

  /**
   * Aggregate function: returns the level of grouping, equals to
   *
   * {{{
   *   (grouping(c1) <<; (n-1)) + (grouping(c2) <<; (n-2)) + ... + grouping(cn)
   * }}}
   *
   * @note The list of columns should match with grouping columns exactly.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def grouping_id(colName: String, colNames: String*): Column = {
    grouping_id((Seq(colName) ++ colNames).map(n => Column(n)) : _*)
  }

  /**
   * Aggregate function: returns the kurtosis of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def kurtosis(e: Column): Column = withAggregateFunction { Kurtosis(e.expr) }

  /**
   * Aggregate function: returns the kurtosis of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def kurtosis(columnName: String): Column = kurtosis(Column(columnName))

  /**
   * Aggregate function: returns the last value in a group.
   *
   * The function by default returns the last values it sees. It will return the last non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def last(e: Column, ignoreNulls: Boolean): Column = withAggregateFunction {
    new Last(e.expr, ignoreNulls)
  }

  /**
   * Aggregate function: returns the last value of the column in a group.
   *
   * The function by default returns the last values it sees. It will return the last non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 2.0.0
   */
  def last(columnName: String, ignoreNulls: Boolean): Column = {
    last(Column(columnName), ignoreNulls)
  }

  /**
   * Aggregate function: returns the last value in a group.
   *
   * The function by default returns the last values it sees. It will return the last non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def last(e: Column): Column = last(e, ignoreNulls = false)

  /**
   * Aggregate function: returns the last value of the column in a group.
   *
   * The function by default returns the last values it sees. It will return the last non-null
   * value it sees when ignoreNulls is set to true. If all values are null, then null is returned.
   *
   * @note The function is non-deterministic because its results depends on order of rows which
   * may be non-deterministic after a shuffle.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def last(columnName: String): Column = last(Column(columnName), ignoreNulls = false)

  /**
   * Aggregate function: returns the maximum value of the expression in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def max(e: Column): Column = withAggregateFunction { Max(e.expr) }

  /**
   * Aggregate function: returns the maximum value of the column in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def max(columnName: String): Column = max(Column(columnName))

  /**
   * Aggregate function: returns the average of the values in a group.
   * Alias for avg.
   *
   * @group agg_funcs
   * @since 1.4.0
   */
  def mean(e: Column): Column = avg(e)

  /**
   * Aggregate function: returns the average of the values in a group.
   * Alias for avg.
   *
   * @group agg_funcs
   * @since 1.4.0
   */
  def mean(columnName: String): Column = avg(columnName)

  /**
   * Aggregate function: returns the minimum value of the expression in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def min(e: Column): Column = withAggregateFunction { Min(e.expr) }

  /**
   * Aggregate function: returns the minimum value of the column in a group.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def min(columnName: String): Column = min(Column(columnName))

  /**
   * Aggregate function: returns the skewness of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def skewness(e: Column): Column = withAggregateFunction { Skewness(e.expr) }

  /**
   * Aggregate function: returns the skewness of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def skewness(columnName: String): Column = skewness(Column(columnName))

  /**
   * Aggregate function: alias for `stddev_samp`.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev(e: Column): Column = withAggregateFunction { StddevSamp(e.expr) }

  /**
   * Aggregate function: alias for `stddev_samp`.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev(columnName: String): Column = stddev(Column(columnName))

  /**
   * Aggregate function: returns the sample standard deviation of
   * the expression in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev_samp(e: Column): Column = withAggregateFunction { StddevSamp(e.expr) }

  /**
   * Aggregate function: returns the sample standard deviation of
   * the expression in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev_samp(columnName: String): Column = stddev_samp(Column(columnName))

  /**
   * Aggregate function: returns the population standard deviation of
   * the expression in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev_pop(e: Column): Column = withAggregateFunction { StddevPop(e.expr) }

  /**
   * Aggregate function: returns the population standard deviation of
   * the expression in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def stddev_pop(columnName: String): Column = stddev_pop(Column(columnName))

  /**
   * Aggregate function: returns the sum of all values in the expression.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def sum(e: Column): Column = withAggregateFunction { Sum(e.expr) }

  /**
   * Aggregate function: returns the sum of all values in the given column.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def sum(columnName: String): Column = sum(Column(columnName))

  /**
   * Aggregate function: returns the sum of distinct values in the expression.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def sumDistinct(e: Column): Column = withAggregateFunction(Sum(e.expr), isDistinct = true)

  /**
   * Aggregate function: returns the sum of distinct values in the expression.
   *
   * @group agg_funcs
   * @since 1.3.0
   */
  def sumDistinct(columnName: String): Column = sumDistinct(Column(columnName))

  /**
   * Aggregate function: alias for `var_samp`.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def variance(e: Column): Column = withAggregateFunction { VarianceSamp(e.expr) }

  /**
   * Aggregate function: alias for `var_samp`.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def variance(columnName: String): Column = variance(Column(columnName))

  /**
   * Aggregate function: returns the unbiased variance of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def var_samp(e: Column): Column = withAggregateFunction { VarianceSamp(e.expr) }

  /**
   * Aggregate function: returns the unbiased variance of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def var_samp(columnName: String): Column = var_samp(Column(columnName))

  /**
   * Aggregate function: returns the population variance of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def var_pop(e: Column): Column = withAggregateFunction { VariancePop(e.expr) }

  /**
   * Aggregate function: returns the population variance of the values in a group.
   *
   * @group agg_funcs
   * @since 1.6.0
   */
  def var_pop(columnName: String): Column = var_pop(Column(columnName))


  //////////////////////////////////////////////////////////////////////////////////////////////
  // Window functions
  //////////////////////////////////////////////////////////////////////////////////////////////
  /**
   * This function has been deprecated in Spark 2.4. See SPARK-25842 for more information.
   *
   * @group window_funcs
   * @since 2.3.0
   */
  @deprecated("Use Window.unboundedPreceding", "2.4.0")
  def unboundedPreceding(): Column = Column(UnboundedPreceding)

  /**
   * This function has been deprecated in Spark 2.4. See SPARK-25842 for more information.
   *
   * @group window_funcs
   * @since 2.3.0
   */
  @deprecated("Use Window.unboundedFollowing", "2.4.0")
  def unboundedFollowing(): Column = Column(UnboundedFollowing)

  /**
   * This function has been deprecated in Spark 2.4. See SPARK-25842 for more information.
   *
   * @group window_funcs
   * @since 2.3.0
   */
  @deprecated("Use Window.currentRow", "2.4.0")
  def currentRow(): Column = Column(CurrentRow)

  /**
   * Window function: returns the cumulative distribution of values within a window partition,
   * i.e. the fraction of rows that are below the current row.
   *
   * {{{
   *   N = total number of rows in the partition
   *   cumeDist(x) = number of values before (and including) x / N
   * }}}
   *
   * @group window_funcs
   * @since 1.6.0
   */
  def cume_dist(): Column = withExpr { new CumeDist }

  /**
   * Window function: returns the rank of rows within a window partition, without any gaps.
   *
   * The difference between rank and dense_rank is that denseRank leaves no gaps in ranking
   * sequence when there are ties. That is, if you were ranking a competition using dense_rank
   * and had three people tie for second place, you would say that all three were in second
   * place and that the next person came in third. Rank would give me sequential numbers, making
   * the person that came in third place (after the ties) would register as coming in fifth.
   *
   * This is equivalent to the DENSE_RANK function in SQL.
   *
   * @group window_funcs
   * @since 1.6.0
   */
  def dense_rank(): Column = withExpr { new DenseRank }

  /**
   * Window function: returns the value that is `offset` rows before the current row, and
   * `null` if there is less than `offset` rows before the current row. For example,
   * an `offset` of one will return the previous row at any given point in the window partition.
   *
   * This is equivalent to the LAG function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lag(e: Column, offset: Int): Column = lag(e, offset, null)

  /**
   * Window function: returns the value that is `offset` rows before the current row, and
   * `null` if there is less than `offset` rows before the current row. For example,
   * an `offset` of one will return the previous row at any given point in the window partition.
   *
   * This is equivalent to the LAG function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lag(columnName: String, offset: Int): Column = lag(columnName, offset, null)

  /**
   * Window function: returns the value that is `offset` rows before the current row, and
   * `defaultValue` if there is less than `offset` rows before the current row. For example,
   * an `offset` of one will return the previous row at any given point in the window partition.
   *
   * This is equivalent to the LAG function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lag(columnName: String, offset: Int, defaultValue: Any): Column = {
    lag(Column(columnName), offset, defaultValue)
  }

  /**
   * Window function: returns the value that is `offset` rows before the current row, and
   * `defaultValue` if there is less than `offset` rows before the current row. For example,
   * an `offset` of one will return the previous row at any given point in the window partition.
   *
   * This is equivalent to the LAG function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lag(e: Column, offset: Int, defaultValue: Any): Column = withExpr {
    Lag(e.expr, Literal(offset), Literal(defaultValue))
  }

  /**
   * Window function: returns the value that is `offset` rows after the current row, and
   * `null` if there is less than `offset` rows after the current row. For example,
   * an `offset` of one will return the next row at any given point in the window partition.
   *
   * This is equivalent to the LEAD function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lead(columnName: String, offset: Int): Column = { lead(columnName, offset, null) }

  /**
   * Window function: returns the value that is `offset` rows after the current row, and
   * `null` if there is less than `offset` rows after the current row. For example,
   * an `offset` of one will return the next row at any given point in the window partition.
   *
   * This is equivalent to the LEAD function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lead(e: Column, offset: Int): Column = { lead(e, offset, null) }

  /**
   * Window function: returns the value that is `offset` rows after the current row, and
   * `defaultValue` if there is less than `offset` rows after the current row. For example,
   * an `offset` of one will return the next row at any given point in the window partition.
   *
   * This is equivalent to the LEAD function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lead(columnName: String, offset: Int, defaultValue: Any): Column = {
    lead(Column(columnName), offset, defaultValue)
  }

  /**
   * Window function: returns the value that is `offset` rows after the current row, and
   * `defaultValue` if there is less than `offset` rows after the current row. For example,
   * an `offset` of one will return the next row at any given point in the window partition.
   *
   * This is equivalent to the LEAD function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def lead(e: Column, offset: Int, defaultValue: Any): Column = withExpr {
    Lead(e.expr, Literal(offset), Literal(defaultValue))
  }

  /**
   * Window function: returns the ntile group id (from 1 to `n` inclusive) in an ordered window
   * partition. For example, if `n` is 4, the first quarter of the rows will get value 1, the second
   * quarter will get 2, the third quarter will get 3, and the last quarter will get 4.
   *
   * This is equivalent to the NTILE function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def ntile(n: Int): Column = withExpr { new NTile(Literal(n)) }

  /**
   * Window function: returns the relative rank (i.e. percentile) of rows within a window partition.
   *
   * This is computed by:
   * {{{
   *   (rank of row in its partition - 1) / (number of rows in the partition - 1)
   * }}}
   *
   * This is equivalent to the PERCENT_RANK function in SQL.
   *
   * @group window_funcs
   * @since 1.6.0
   */
  def percent_rank(): Column = withExpr { new PercentRank }

  /**
   * Window function: returns the rank of rows within a window partition.
   *
   * The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking
   * sequence when there are ties. That is, if you were ranking a competition using dense_rank
   * and had three people tie for second place, you would say that all three were in second
   * place and that the next person came in third. Rank would give me sequential numbers, making
   * the person that came in third place (after the ties) would register as coming in fifth.
   *
   * This is equivalent to the RANK function in SQL.
   *
   * @group window_funcs
   * @since 1.4.0
   */
  def rank(): Column = withExpr { new Rank }

  /**
   * Window function: returns a sequential number starting at 1 within a window partition.
   *
   * @group window_funcs
   * @since 1.6.0
   */
  def row_number(): Column = withExpr { RowNumber() }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Non-aggregate functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Creates a new array column. The input columns must all have the same data type.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def array(cols: Column*): Column = withExpr { CreateArray(cols.map(_.expr)) }

  /**
   * Creates a new array column. The input columns must all have the same data type.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def array(colName: String, colNames: String*): Column = {
    array((colName +: colNames).map(col) : _*)
  }

  /**
   * Creates a new map column. The input columns must be grouped as key-value pairs, e.g.
   * (key1, value1, key2, value2, ...). The key columns must all have the same data type, and can't
   * be null. The value columns must all have the same data type.
   *
   * @group normal_funcs
   * @since 2.0
   */
  @scala.annotation.varargs
  def map(cols: Column*): Column = withExpr { CreateMap(cols.map(_.expr)) }

  /**
   * Creates a new map column. The array in the first column is used for keys. The array in the
   * second column is used for values. All elements in the array for key should not be null.
   *
   * @group normal_funcs
   * @since 2.4
   */
  def map_from_arrays(keys: Column, values: Column): Column = withExpr {
    MapFromArrays(keys.expr, values.expr)
  }

  /**
   * Marks a DataFrame as small enough for use in broadcast joins.
   *
   * The following example marks the right DataFrame for broadcast hash join using `joinKey`.
   * {{{
   *   // left and right are DataFrames
   *   left.join(broadcast(right), "joinKey")
   * }}}
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  def broadcast[T](df: Dataset[T]): Dataset[T] = {
    Dataset[T](df.sparkSession,
      ResolvedHint(df.logicalPlan, HintInfo(broadcast = true)))(df.exprEnc)
  }

  /**
   * Returns the first column that is not null, or null if all inputs are null.
   *
   * For example, `coalesce(a, b, c)` will return a if a is not null,
   * or b if a is null and b is not null, or c if both a and b are null but c is not null.
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  @scala.annotation.varargs
  def coalesce(e: Column*): Column = withExpr { Coalesce(e.map(_.expr)) }

  /**
   * Creates a string column for the file name of the current Spark task.
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def input_file_name(): Column = withExpr { InputFileName() }

  /**
   * Return true iff the column is NaN.
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def isnan(e: Column): Column = withExpr { IsNaN(e.expr) }

  /**
   * Return true iff the column is null.
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def isnull(e: Column): Column = withExpr { IsNull(e.expr) }

  /**
   * A column expression that generates monotonically increasing 64-bit integers.
   *
   * The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.
   * The current implementation puts the partition ID in the upper 31 bits, and the record number
   * within each partition in the lower 33 bits. The assumption is that the data frame has
   * less than 1 billion partitions, and each partition has less than 8 billion records.
   *
   * As an example, consider a `DataFrame` with two partitions, each with 3 records.
   * This expression would return the following IDs:
   *
   * {{{
   * 0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.
   * }}}
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @deprecated("Use monotonically_increasing_id()", "2.0.0")
  def monotonicallyIncreasingId(): Column = monotonically_increasing_id()

  /**
   * A column expression that generates monotonically increasing 64-bit integers.
   *
   * The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.
   * The current implementation puts the partition ID in the upper 31 bits, and the record number
   * within each partition in the lower 33 bits. The assumption is that the data frame has
   * less than 1 billion partitions, and each partition has less than 8 billion records.
   *
   * As an example, consider a `DataFrame` with two partitions, each with 3 records.
   * This expression would return the following IDs:
   *
   * {{{
   * 0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.
   * }}}
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def monotonically_increasing_id(): Column = withExpr { MonotonicallyIncreasingID() }

  /**
   * Returns col1 if it is not NaN, or col2 if col1 is NaN.
   *
   * Both inputs should be floating point columns (DoubleType or FloatType).
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  def nanvl(col1: Column, col2: Column): Column = withExpr { NaNvl(col1.expr, col2.expr) }

  /**
   * Unary minus, i.e. negate the expression.
   * {{{
   *   // Select the amount column and negates all values.
   *   // Scala:
   *   df.select( -df("amount") )
   *
   *   // Java:
   *   df.select( negate(df.col("amount")) );
   * }}}
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def negate(e: Column): Column = -e

  /**
   * Inversion of boolean expression, i.e. NOT.
   * {{{
   *   // Scala: select rows that are not active (isActive === false)
   *   df.filter( !df("isActive") )
   *
   *   // Java:
   *   df.filter( not(df.col("isActive")) );
   * }}}
   *
   * @group normal_funcs
   * @since 1.3.0
   */
  def not(e: Column): Column = !e

  /**
   * Generate a random column with independent and identically distributed (i.i.d.) samples
   * uniformly distributed in [0.0, 1.0).
   *
   * @note The function is non-deterministic in general case.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def rand(seed: Long): Column = withExpr { Rand(seed) }

  /**
   * Generate a random column with independent and identically distributed (i.i.d.) samples
   * uniformly distributed in [0.0, 1.0).
   *
   * @note The function is non-deterministic in general case.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def rand(): Column = rand(Utils.random.nextLong)

  /**
   * Generate a column with independent and identically distributed (i.i.d.) samples from
   * the standard normal distribution.
   *
   * @note The function is non-deterministic in general case.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def randn(seed: Long): Column = withExpr { Randn(seed) }

  /**
   * Generate a column with independent and identically distributed (i.i.d.) samples from
   * the standard normal distribution.
   *
   * @note The function is non-deterministic in general case.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def randn(): Column = randn(Utils.random.nextLong)

  /**
   * Partition ID.
   *
   * @note This is non-deterministic because it depends on data partitioning and task scheduling.
   *
   * @group normal_funcs
   * @since 1.6.0
   */
  def spark_partition_id(): Column = withExpr { SparkPartitionID() }

  /**
   * Computes the square root of the specified float value.
   *
   * @group math_funcs
   * @since 1.3.0
   */
  def sqrt(e: Column): Column = withExpr { Sqrt(e.expr) }

  /**
   * Computes the square root of the specified float value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def sqrt(colName: String): Column = sqrt(Column(colName))

  /**
   * Creates a new struct column.
   * If the input column is a column in a `DataFrame`, or a derived column expression
   * that is named (i.e. aliased), its name would be retained as the StructField's name,
   * otherwise, the newly generated StructField's name would be auto generated as
   * `col` with a suffix `index + 1`, i.e. col1, col2, col3, ...
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def struct(cols: Column*): Column = withExpr { CreateStruct(cols.map(_.expr)) }

  /**
   * Creates a new struct column that composes multiple input columns.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def struct(colName: String, colNames: String*): Column = {
    struct((colName +: colNames).map(col) : _*)
  }

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def when(condition: Column, value: Any): Column = withExpr {
    CaseWhen(Seq((condition.expr, lit(value).expr)))
  }

  /**
   * Computes bitwise NOT (~) of a number.
   *
   * @group normal_funcs
   * @since 1.4.0
   */
  def bitwiseNOT(e: Column): Column = withExpr { BitwiseNot(e.expr) }

  /**
   * Parses the expression string into the column that it represents, similar to
   * [[Dataset#selectExpr]].
   * {{{
   *   // get the number of words of each length
   *   df.groupBy(expr("length(word)")).count()
   * }}}
   *
   * @group normal_funcs
   */
  def expr(expr: String): Column = {
    val parser = SparkSession.getActiveSession.map(_.sessionState.sqlParser).getOrElse {
      new SparkSqlParser(new SQLConf)
    }
    Column(parser.parseExpression(expr))
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Math Functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Computes the absolute value of a numeric value.
   *
   * @group math_funcs
   * @since 1.3.0
   */
  def abs(e: Column): Column = withExpr { Abs(e.expr) }

  /**
   * @return inverse cosine of `e` in radians, as if computed by `java.lang.Math.acos`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def acos(e: Column): Column = withExpr { Acos(e.expr) }

  /**
   * @return inverse cosine of `columnName`, as if computed by `java.lang.Math.acos`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def acos(columnName: String): Column = acos(Column(columnName))

  /**
   * @return inverse sine of `e` in radians, as if computed by `java.lang.Math.asin`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def asin(e: Column): Column = withExpr { Asin(e.expr) }

  /**
   * @return inverse sine of `columnName`, as if computed by `java.lang.Math.asin`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def asin(columnName: String): Column = asin(Column(columnName))

  /**
   * @return inverse tangent of `e`, as if computed by `java.lang.Math.atan`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan(e: Column): Column = withExpr { Atan(e.expr) }

  /**
   * @return inverse tangent of `columnName`, as if computed by `java.lang.Math.atan`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan(columnName: String): Column = atan(Column(columnName))

  /**
   * @param y coordinate on y-axis
   * @param x coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(y: Column, x: Column): Column = withExpr { Atan2(y.expr, x.expr) }

  /**
   * @param y coordinate on y-axis
   * @param xName coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(y: Column, xName: String): Column = atan2(y, Column(xName))

  /**
   * @param yName coordinate on y-axis
   * @param x coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yName: String, x: Column): Column = atan2(Column(yName), x)

  /**
   * @param yName coordinate on y-axis
   * @param xName coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yName: String, xName: String): Column =
    atan2(Column(yName), Column(xName))

  /**
   * @param y coordinate on y-axis
   * @param xValue coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(y: Column, xValue: Double): Column = atan2(y, lit(xValue))

  /**
   * @param yName coordinate on y-axis
   * @param xValue coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yName: String, xValue: Double): Column = atan2(Column(yName), xValue)

  /**
   * @param yValue coordinate on y-axis
   * @param x coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yValue: Double, x: Column): Column = atan2(lit(yValue), x)

  /**
   * @param yValue coordinate on y-axis
   * @param xName coordinate on x-axis
   * @return the <i>theta</i> component of the point
   *         (<i>r</i>, <i>theta</i>)
   *         in polar coordinates that corresponds to the point
   *         (<i>x</i>, <i>y</i>) in Cartesian coordinates,
   *         as if computed by `java.lang.Math.atan2`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def atan2(yValue: Double, xName: String): Column = atan2(yValue, Column(xName))

  /**
   * An expression that returns the string representation of the binary value of the given long
   * column. For example, bin("12") returns "1100".
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def bin(e: Column): Column = withExpr { Bin(e.expr) }

  /**
   * An expression that returns the string representation of the binary value of the given long
   * column. For example, bin("12") returns "1100".
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def bin(columnName: String): Column = bin(Column(columnName))

  /**
   * Computes the cube-root of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cbrt(e: Column): Column = withExpr { Cbrt(e.expr) }

  /**
   * Computes the cube-root of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cbrt(columnName: String): Column = cbrt(Column(columnName))

  /**
   * Computes the ceiling of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def ceil(e: Column): Column = withExpr { Ceil(e.expr) }

  /**
   * Computes the ceiling of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def ceil(columnName: String): Column = ceil(Column(columnName))

  /**
   * Convert a number in a string column from one base to another.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def conv(num: Column, fromBase: Int, toBase: Int): Column = withExpr {
    Conv(num.expr, lit(fromBase).expr, lit(toBase).expr)
  }

  /**
   * @param e angle in radians
   * @return cosine of the angle, as if computed by `java.lang.Math.cos`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cos(e: Column): Column = withExpr { Cos(e.expr) }

  /**
   * @param columnName angle in radians
   * @return cosine of the angle, as if computed by `java.lang.Math.cos`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cos(columnName: String): Column = cos(Column(columnName))

  /**
   * @param e hyperbolic angle
   * @return hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cosh(e: Column): Column = withExpr { Cosh(e.expr) }

  /**
   * @param columnName hyperbolic angle
   * @return hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def cosh(columnName: String): Column = cosh(Column(columnName))

  /**
   * Computes the exponential of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def exp(e: Column): Column = withExpr { Exp(e.expr) }

  /**
   * Computes the exponential of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def exp(columnName: String): Column = exp(Column(columnName))

  /**
   * Computes the exponential of the given value minus one.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def expm1(e: Column): Column = withExpr { Expm1(e.expr) }

  /**
   * Computes the exponential of the given column minus one.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def expm1(columnName: String): Column = expm1(Column(columnName))

  /**
   * Computes the factorial of the given value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def factorial(e: Column): Column = withExpr { Factorial(e.expr) }

  /**
   * Computes the floor of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def floor(e: Column): Column = withExpr { Floor(e.expr) }

  /**
   * Computes the floor of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def floor(columnName: String): Column = floor(Column(columnName))

  /**
   * Returns the greatest value of the list of values, skipping null values.
   * This function takes at least 2 parameters. It will return null iff all parameters are null.
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def greatest(exprs: Column*): Column = withExpr { Greatest(exprs.map(_.expr)) }

  /**
   * Returns the greatest value of the list of column names, skipping null values.
   * This function takes at least 2 parameters. It will return null iff all parameters are null.
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def greatest(columnName: String, columnNames: String*): Column = {
    greatest((columnName +: columnNames).map(Column.apply): _*)
  }

  /**
   * Computes hex value of the given column.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def hex(column: Column): Column = withExpr { Hex(column.expr) }

  /**
   * Inverse of hex. Interprets each pair of characters as a hexadecimal number
   * and converts to the byte representation of number.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def unhex(column: Column): Column = withExpr { Unhex(column.expr) }

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Column, r: Column): Column = withExpr { Hypot(l.expr, r.expr) }

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Column, rightName: String): Column = hypot(l, Column(rightName))

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(leftName: String, r: Column): Column = hypot(Column(leftName), r)

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(leftName: String, rightName: String): Column =
    hypot(Column(leftName), Column(rightName))

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Column, r: Double): Column = hypot(l, lit(r))

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(leftName: String, r: Double): Column = hypot(Column(leftName), r)

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Double, r: Column): Column = hypot(lit(l), r)

  /**
   * Computes `sqrt(a^2^ + b^2^)` without intermediate overflow or underflow.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def hypot(l: Double, rightName: String): Column = hypot(l, Column(rightName))

  /**
   * Returns the least value of the list of values, skipping null values.
   * This function takes at least 2 parameters. It will return null iff all parameters are null.
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def least(exprs: Column*): Column = withExpr { Least(exprs.map(_.expr)) }

  /**
   * Returns the least value of the list of column names, skipping null values.
   * This function takes at least 2 parameters. It will return null iff all parameters are null.
   *
   * @group normal_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def least(columnName: String, columnNames: String*): Column = {
    least((columnName +: columnNames).map(Column.apply): _*)
  }

  /**
   * Computes the natural logarithm of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log(e: Column): Column = withExpr { Log(e.expr) }

  /**
   * Computes the natural logarithm of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log(columnName: String): Column = log(Column(columnName))

  /**
   * Returns the first argument-base logarithm of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log(base: Double, a: Column): Column = withExpr { Logarithm(lit(base).expr, a.expr) }

  /**
   * Returns the first argument-base logarithm of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log(base: Double, columnName: String): Column = log(base, Column(columnName))

  /**
   * Computes the logarithm of the given value in base 10.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log10(e: Column): Column = withExpr { Log10(e.expr) }

  /**
   * Computes the logarithm of the given value in base 10.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log10(columnName: String): Column = log10(Column(columnName))

  /**
   * Computes the natural logarithm of the given value plus one.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log1p(e: Column): Column = withExpr { Log1p(e.expr) }

  /**
   * Computes the natural logarithm of the given column plus one.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def log1p(columnName: String): Column = log1p(Column(columnName))

  /**
   * Computes the logarithm of the given column in base 2.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def log2(expr: Column): Column = withExpr { Log2(expr.expr) }

  /**
   * Computes the logarithm of the given value in base 2.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def log2(columnName: String): Column = log2(Column(columnName))

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Column, r: Column): Column = withExpr { Pow(l.expr, r.expr) }

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Column, rightName: String): Column = pow(l, Column(rightName))

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(leftName: String, r: Column): Column = pow(Column(leftName), r)

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(leftName: String, rightName: String): Column = pow(Column(leftName), Column(rightName))

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Column, r: Double): Column = pow(l, lit(r))

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(leftName: String, r: Double): Column = pow(Column(leftName), r)

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Double, r: Column): Column = pow(lit(l), r)

  /**
   * Returns the value of the first argument raised to the power of the second argument.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def pow(l: Double, rightName: String): Column = pow(l, Column(rightName))

  /**
   * Returns the positive value of dividend mod divisor.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def pmod(dividend: Column, divisor: Column): Column = withExpr {
    Pmod(dividend.expr, divisor.expr)
  }

  /**
   * Returns the double value that is closest in value to the argument and
   * is equal to a mathematical integer.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def rint(e: Column): Column = withExpr { Rint(e.expr) }

  /**
   * Returns the double value that is closest in value to the argument and
   * is equal to a mathematical integer.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def rint(columnName: String): Column = rint(Column(columnName))

  /**
   * Returns the value of the column `e` rounded to 0 decimal places with HALF_UP round mode.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def round(e: Column): Column = round(e, 0)

  /**
   * Round the value of `e` to `scale` decimal places with HALF_UP round mode
   * if `scale` is greater than or equal to 0 or at integral part when `scale` is less than 0.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def round(e: Column, scale: Int): Column = withExpr { Round(e.expr, Literal(scale)) }

  /**
   * Returns the value of the column `e` rounded to 0 decimal places with HALF_EVEN round mode.
   *
   * @group math_funcs
   * @since 2.0.0
   */
  def bround(e: Column): Column = bround(e, 0)

  /**
   * Round the value of `e` to `scale` decimal places with HALF_EVEN round mode
   * if `scale` is greater than or equal to 0 or at integral part when `scale` is less than 0.
   *
   * @group math_funcs
   * @since 2.0.0
   */
  def bround(e: Column, scale: Int): Column = withExpr { BRound(e.expr, Literal(scale)) }

  /**
   * Shift the given value numBits left. If the given value is a long value, this function
   * will return a long value else it will return an integer value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def shiftLeft(e: Column, numBits: Int): Column = withExpr { ShiftLeft(e.expr, lit(numBits).expr) }

  /**
   * (Signed) shift the given value numBits right. If the given value is a long value, it will
   * return a long value else it will return an integer value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def shiftRight(e: Column, numBits: Int): Column = withExpr {
    ShiftRight(e.expr, lit(numBits).expr)
  }

  /**
   * Unsigned shift the given value numBits right. If the given value is a long value,
   * it will return a long value else it will return an integer value.
   *
   * @group math_funcs
   * @since 1.5.0
   */
  def shiftRightUnsigned(e: Column, numBits: Int): Column = withExpr {
    ShiftRightUnsigned(e.expr, lit(numBits).expr)
  }

  /**
   * Computes the signum of the given value.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def signum(e: Column): Column = withExpr { Signum(e.expr) }

  /**
   * Computes the signum of the given column.
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def signum(columnName: String): Column = signum(Column(columnName))

  /**
   * @param e angle in radians
   * @return sine of the angle, as if computed by `java.lang.Math.sin`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def sin(e: Column): Column = withExpr { Sin(e.expr) }

  /**
   * @param columnName angle in radians
   * @return sine of the angle, as if computed by `java.lang.Math.sin`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def sin(columnName: String): Column = sin(Column(columnName))

  /**
   * @param e hyperbolic angle
   * @return hyperbolic sine of the given value, as if computed by `java.lang.Math.sinh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def sinh(e: Column): Column = withExpr { Sinh(e.expr) }

  /**
   * @param columnName hyperbolic angle
   * @return hyperbolic sine of the given value, as if computed by `java.lang.Math.sinh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def sinh(columnName: String): Column = sinh(Column(columnName))

  /**
   * @param e angle in radians
   * @return tangent of the given value, as if computed by `java.lang.Math.tan`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def tan(e: Column): Column = withExpr { Tan(e.expr) }

  /**
   * @param columnName angle in radians
   * @return tangent of the given value, as if computed by `java.lang.Math.tan`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def tan(columnName: String): Column = tan(Column(columnName))

  /**
   * @param e hyperbolic angle
   * @return hyperbolic tangent of the given value, as if computed by `java.lang.Math.tanh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def tanh(e: Column): Column = withExpr { Tanh(e.expr) }

  /**
   * @param columnName hyperbolic angle
   * @return hyperbolic tangent of the given value, as if computed by `java.lang.Math.tanh`
   *
   * @group math_funcs
   * @since 1.4.0
   */
  def tanh(columnName: String): Column = tanh(Column(columnName))

  /**
   * @group math_funcs
   * @since 1.4.0
   */
  @deprecated("Use degrees", "2.1.0")
  def toDegrees(e: Column): Column = degrees(e)

  /**
   * @group math_funcs
   * @since 1.4.0
   */
  @deprecated("Use degrees", "2.1.0")
  def toDegrees(columnName: String): Column = degrees(Column(columnName))

  /**
   * Converts an angle measured in radians to an approximately equivalent angle measured in degrees.
   *
   * @param e angle in radians
   * @return angle in degrees, as if computed by `java.lang.Math.toDegrees`
   *
   * @group math_funcs
   * @since 2.1.0
   */
  def degrees(e: Column): Column = withExpr { ToDegrees(e.expr) }

  /**
   * Converts an angle measured in radians to an approximately equivalent angle measured in degrees.
   *
   * @param columnName angle in radians
   * @return angle in degrees, as if computed by `java.lang.Math.toDegrees`
   *
   * @group math_funcs
   * @since 2.1.0
   */
  def degrees(columnName: String): Column = degrees(Column(columnName))

  /**
   * @group math_funcs
   * @since 1.4.0
   */
  @deprecated("Use radians", "2.1.0")
  def toRadians(e: Column): Column = radians(e)

  /**
   * @group math_funcs
   * @since 1.4.0
   */
  @deprecated("Use radians", "2.1.0")
  def toRadians(columnName: String): Column = radians(Column(columnName))

  /**
   * Converts an angle measured in degrees to an approximately equivalent angle measured in radians.
   *
   * @param e angle in degrees
   * @return angle in radians, as if computed by `java.lang.Math.toRadians`
   *
   * @group math_funcs
   * @since 2.1.0
   */
  def radians(e: Column): Column = withExpr { ToRadians(e.expr) }

  /**
   * Converts an angle measured in degrees to an approximately equivalent angle measured in radians.
   *
   * @param columnName angle in degrees
   * @return angle in radians, as if computed by `java.lang.Math.toRadians`
   *
   * @group math_funcs
   * @since 2.1.0
   */
  def radians(columnName: String): Column = radians(Column(columnName))

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Misc functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Calculates the MD5 digest of a binary column and returns the value
   * as a 32 character hex string.
   *
   * @group misc_funcs
   * @since 1.5.0
   */
  def md5(e: Column): Column = withExpr { Md5(e.expr) }

  /**
   * Calculates the SHA-1 digest of a binary column and returns the value
   * as a 40 character hex string.
   *
   * @group misc_funcs
   * @since 1.5.0
   */
  def sha1(e: Column): Column = withExpr { Sha1(e.expr) }

  /**
   * Calculates the SHA-2 family of hash functions of a binary column and
   * returns the value as a hex string.
   *
   * @param e column to compute SHA-2 on.
   * @param numBits one of 224, 256, 384, or 512.
   *
   * @group misc_funcs
   * @since 1.5.0
   */
  def sha2(e: Column, numBits: Int): Column = {
    require(Seq(0, 224, 256, 384, 512).contains(numBits),
      s"numBits $numBits is not in the permitted values (0, 224, 256, 384, 512)")
    withExpr { Sha2(e.expr, lit(numBits).expr) }
  }

  /**
   * Calculates the cyclic redundancy check value  (CRC32) of a binary column and
   * returns the value as a bigint.
   *
   * @group misc_funcs
   * @since 1.5.0
   */
  def crc32(e: Column): Column = withExpr { Crc32(e.expr) }

  /**
   * Calculates the hash code of given columns, and returns the result as an int column.
   *
   * @group misc_funcs
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def hash(cols: Column*): Column = withExpr {
    new Murmur3Hash(cols.map(_.expr))
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // String functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Computes the numeric value of the first character of the string column, and returns the
   * result as an int column.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def ascii(e: Column): Column = withExpr { Ascii(e.expr) }

  /**
   * Computes the BASE64 encoding of a binary column and returns it as a string column.
   * This is the reverse of unbase64.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def base64(e: Column): Column = withExpr { Base64(e.expr) }

  /**
   * Concatenates multiple input string columns together into a single string column,
   * using the given separator.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def concat_ws(sep: String, exprs: Column*): Column = withExpr {
    ConcatWs(Literal.create(sep, StringType) +: exprs.map(_.expr))
  }

  /**
   * Computes the first argument into a string from a binary using the provided character set
   * (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').
   * If either argument is null, the result will also be null.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def decode(value: Column, charset: String): Column = withExpr {
    Decode(value.expr, lit(charset).expr)
  }

  /**
   * Computes the first argument into a binary from a string using the provided character set
   * (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').
   * If either argument is null, the result will also be null.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def encode(value: Column, charset: String): Column = withExpr {
    Encode(value.expr, lit(charset).expr)
  }

  /**
   * Formats numeric column x to a format like '#,###,###.##', rounded to d decimal places
   * with HALF_EVEN round mode, and returns the result as a string column.
   *
   * If d is 0, the result has no decimal point or fractional part.
   * If d is less than 0, the result will be null.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def format_number(x: Column, d: Int): Column = withExpr {
    FormatNumber(x.expr, lit(d).expr)
  }

  /**
   * Formats the arguments in printf-style and returns the result as a string column.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def format_string(format: String, arguments: Column*): Column = withExpr {
    FormatString((lit(format) +: arguments).map(_.expr): _*)
  }

  /**
   * Returns a new string column by converting the first letter of each word to uppercase.
   * Words are delimited by whitespace.
   *
   * For example, "hello world" will become "Hello World".
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def initcap(e: Column): Column = withExpr { InitCap(e.expr) }

  /**
   * Locate the position of the first occurrence of substr column in the given string.
   * Returns null if either of the arguments are null.
   *
   * @note The position is not zero based, but 1 based index. Returns 0 if substr
   * could not be found in str.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def instr(str: Column, substring: String): Column = withExpr {
    StringInstr(str.expr, lit(substring).expr)
  }

  /**
   * Computes the character length of a given string or number of bytes of a binary string.
   * The length of character strings include the trailing spaces. The length of binary strings
   * includes binary zeros.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def length(e: Column): Column = withExpr { Length(e.expr) }

  /**
   * Converts a string column to lower case.
   *
   * @group string_funcs
   * @since 1.3.0
   */
  def lower(e: Column): Column = withExpr { Lower(e.expr) }

  /**
   * Computes the Levenshtein distance of the two given string columns.
   * @group string_funcs
   * @since 1.5.0
   */
  def levenshtein(l: Column, r: Column): Column = withExpr { Levenshtein(l.expr, r.expr) }

  /**
   * Locate the position of the first occurrence of substr.
   *
   * @note The position is not zero based, but 1 based index. Returns 0 if substr
   * could not be found in str.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def locate(substr: String, str: Column): Column = withExpr {
    new StringLocate(lit(substr).expr, str.expr)
  }

  /**
   * Locate the position of the first occurrence of substr in a string column, after position pos.
   *
   * @note The position is not zero based, but 1 based index. returns 0 if substr
   * could not be found in str.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def locate(substr: String, str: Column, pos: Int): Column = withExpr {
    StringLocate(lit(substr).expr, str.expr, lit(pos).expr)
  }

  /**
   * Left-pad the string column with pad to a length of len. If the string column is longer
   * than len, the return value is shortened to len characters.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def lpad(str: Column, len: Int, pad: String): Column = withExpr {
    StringLPad(str.expr, lit(len).expr, lit(pad).expr)
  }

  /**
   * Trim the spaces from left end for the specified string value.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def ltrim(e: Column): Column = withExpr {StringTrimLeft(e.expr) }

  /**
   * Trim the specified character string from left end for the specified string column.
   * @group string_funcs
   * @since 2.3.0
   */
  def ltrim(e: Column, trimString: String): Column = withExpr {
    StringTrimLeft(e.expr, Literal(trimString))
  }

  /**
   * Extract a specific group matched by a Java regex, from the specified string column.
   * If the regex did not match, or the specified group did not match, an empty string is returned.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def regexp_extract(e: Column, exp: String, groupIdx: Int): Column = withExpr {
    RegExpExtract(e.expr, lit(exp).expr, lit(groupIdx).expr)
  }

  /**
   * Replace all substrings of the specified string value that match regexp with rep.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def regexp_replace(e: Column, pattern: String, replacement: String): Column = withExpr {
    RegExpReplace(e.expr, lit(pattern).expr, lit(replacement).expr)
  }

  /**
   * Replace all substrings of the specified string value that match regexp with rep.
   *
   * @group string_funcs
   * @since 2.1.0
   */
  def regexp_replace(e: Column, pattern: Column, replacement: Column): Column = withExpr {
    RegExpReplace(e.expr, pattern.expr, replacement.expr)
  }

  /**
   * Decodes a BASE64 encoded string column and returns it as a binary column.
   * This is the reverse of base64.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def unbase64(e: Column): Column = withExpr { UnBase64(e.expr) }

  /**
   * Right-pad the string column with pad to a length of len. If the string column is longer
   * than len, the return value is shortened to len characters.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def rpad(str: Column, len: Int, pad: String): Column = withExpr {
    StringRPad(str.expr, lit(len).expr, lit(pad).expr)
  }

  /**
   * Repeats a string column n times, and returns it as a new string column.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def repeat(str: Column, n: Int): Column = withExpr {
    StringRepeat(str.expr, lit(n).expr)
  }

  /**
   * Trim the spaces from right end for the specified string value.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def rtrim(e: Column): Column = withExpr { StringTrimRight(e.expr) }

  /**
   * Trim the specified character string from right end for the specified string column.
   * @group string_funcs
   * @since 2.3.0
   */
  def rtrim(e: Column, trimString: String): Column = withExpr {
    StringTrimRight(e.expr, Literal(trimString))
  }

  /**
   * Returns the soundex code for the specified expression.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def soundex(e: Column): Column = withExpr { SoundEx(e.expr) }

  /**
   * Splits str around pattern (pattern is a regular expression).
   *
   * @note Pattern is a string representation of the regular expression.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def split(str: Column, pattern: String): Column = withExpr {
    StringSplit(str.expr, lit(pattern).expr)
  }

  /**
   * Substring starts at `pos` and is of length `len` when str is String type or
   * returns the slice of byte array that starts at `pos` in byte and is of length `len`
   * when str is Binary type
   *
   * @note The position is not zero based, but 1 based index.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def substring(str: Column, pos: Int, len: Int): Column = withExpr {
    Substring(str.expr, lit(pos).expr, lit(len).expr)
  }

  /**
   * Returns the substring from string str before count occurrences of the delimiter delim.
   * If count is positive, everything the left of the final delimiter (counting from left) is
   * returned. If count is negative, every to the right of the final delimiter (counting from the
   * right) is returned. substring_index performs a case-sensitive match when searching for delim.
   *
   * @group string_funcs
   */
  def substring_index(str: Column, delim: String, count: Int): Column = withExpr {
    SubstringIndex(str.expr, lit(delim).expr, lit(count).expr)
  }

  /**
   * Translate any character in the src by a character in replaceString.
   * The characters in replaceString correspond to the characters in matchingString.
   * The translate will happen when any character in the string matches the character
   * in the `matchingString`.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def translate(src: Column, matchingString: String, replaceString: String): Column = withExpr {
    StringTranslate(src.expr, lit(matchingString).expr, lit(replaceString).expr)
  }

  /**
   * Trim the spaces from both ends for the specified string column.
   *
   * @group string_funcs
   * @since 1.5.0
   */
  def trim(e: Column): Column = withExpr { StringTrim(e.expr) }

  /**
   * Trim the specified character from both ends for the specified string column.
   * @group string_funcs
   * @since 2.3.0
   */
  def trim(e: Column, trimString: String): Column = withExpr {
    StringTrim(e.expr, Literal(trimString))
  }

  /**
   * Converts a string column to upper case.
   *
   * @group string_funcs
   * @since 1.3.0
   */
  def upper(e: Column): Column = withExpr { Upper(e.expr) }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // DateTime functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Returns the date that is `numMonths` after `startDate`.
   *
   * @param startDate A date, timestamp or string. If a string, the data must be in a format that
   *                  can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param numMonths The number of months to add to `startDate`, can be negative to subtract months
   * @return A date, or null if `startDate` was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def add_months(startDate: Column, numMonths: Int): Column = withExpr {
    AddMonths(startDate.expr, Literal(numMonths))
  }

  /**
   * Returns the current date as a date column.
   *
   * @group datetime_funcs
   * @since 1.5.0
   */
  def current_date(): Column = withExpr { CurrentDate() }

  /**
   * Returns the current timestamp as a timestamp column.
   *
   * @group datetime_funcs
   * @since 1.5.0
   */
  def current_timestamp(): Column = withExpr { CurrentTimestamp() }

  /**
   * Converts a date/timestamp/string to a value of string in the format specified by the date
   * format given by the second argument.
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param dateExpr A date, timestamp or string. If a string, the data must be in a format that
   *                 can be cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param format A pattern `dd.MM.yyyy` would return a string like `18.03.1993`
   * @return A string, or null if `dateExpr` was a string that could not be cast to a timestamp
   * @note Use specialized functions like [[year]] whenever possible as they benefit from a
   * specialized implementation.
   * @throws IllegalArgumentException if the `format` pattern is invalid
   * @group datetime_funcs
   * @since 1.5.0
   */
  def date_format(dateExpr: Column, format: String): Column = withExpr {
    DateFormatClass(dateExpr.expr, Literal(format))
  }

  /**
   * Returns the date that is `days` days after `start`
   *
   * @param start A date, timestamp or string. If a string, the data must be in a format that
   *              can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param days  The number of days to add to `start`, can be negative to subtract days
   * @return A date, or null if `start` was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def date_add(start: Column, days: Int): Column = withExpr { DateAdd(start.expr, Literal(days)) }

  /**
   * Returns the date that is `days` days before `start`
   *
   * @param start A date, timestamp or string. If a string, the data must be in a format that
   *              can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param days  The number of days to subtract from `start`, can be negative to add days
   * @return A date, or null if `start` was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def date_sub(start: Column, days: Int): Column = withExpr { DateSub(start.expr, Literal(days)) }

  /**
   * Returns the number of days from `start` to `end`.
   *
   * Only considers the date part of the input. For example:
   * {{{
   * dateddiff("2018-01-10 00:00:00", "2018-01-09 23:59:59")
   * // returns 1
   * }}}
   *
   * @param end A date, timestamp or string. If a string, the data must be in a format that
   *            can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param start A date, timestamp or string. If a string, the data must be in a format that
   *              can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return An integer, or null if either `end` or `start` were strings that could not be cast to
   *         a date. Negative if `end` is before `start`
   * @group datetime_funcs
   * @since 1.5.0
   */
  def datediff(end: Column, start: Column): Column = withExpr { DateDiff(end.expr, start.expr) }

  /**
   * Extracts the year as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def year(e: Column): Column = withExpr { Year(e.expr) }

  /**
   * Extracts the quarter as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def quarter(e: Column): Column = withExpr { Quarter(e.expr) }

  /**
   * Extracts the month as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def month(e: Column): Column = withExpr { Month(e.expr) }

  /**
   * Extracts the day of the week as an integer from a given date/timestamp/string.
   * Ranges from 1 for a Sunday through to 7 for a Saturday
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 2.3.0
   */
  def dayofweek(e: Column): Column = withExpr { DayOfWeek(e.expr) }

  /**
   * Extracts the day of the month as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def dayofmonth(e: Column): Column = withExpr { DayOfMonth(e.expr) }

  /**
   * Extracts the day of the year as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def dayofyear(e: Column): Column = withExpr { DayOfYear(e.expr) }

  /**
   * Extracts the hours as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def hour(e: Column): Column = withExpr { Hour(e.expr) }

  /**
   * Returns the last day of the month which the given date belongs to.
   * For example, input "2015-07-27" returns "2015-07-31" since July 31 is the last day of the
   * month in July 2015.
   *
   * @param e A date, timestamp or string. If a string, the data must be in a format that can be
   *          cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return A date, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def last_day(e: Column): Column = withExpr { LastDay(e.expr) }

  /**
   * Extracts the minutes as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def minute(e: Column): Column = withExpr { Minute(e.expr) }

  /**
   * Returns number of months between dates `start` and `end`.
   *
   * A whole number is returned if both inputs have the same day of month or both are the last day
   * of their respective months. Otherwise, the difference is calculated assuming 31 days per month.
   *
   * For example:
   * {{{
   * months_between("2017-11-14", "2017-07-14")  // returns 4.0
   * months_between("2017-01-01", "2017-01-10")  // returns 0.29032258
   * months_between("2017-06-01", "2017-06-16 12:00:00")  // returns -0.5
   * }}}
   *
   * @param end   A date, timestamp or string. If a string, the data must be in a format that can
   *              be cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param start A date, timestamp or string. If a string, the data must be in a format that can
   *              cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return A double, or null if either `end` or `start` were strings that could not be cast to a
   *         timestamp. Negative if `end` is before `start`
   * @group datetime_funcs
   * @since 1.5.0
   */
  def months_between(end: Column, start: Column): Column = withExpr {
    new MonthsBetween(end.expr, start.expr)
  }

  /**
   * Returns number of months between dates `end` and `start`. If `roundOff` is set to true, the
   * result is rounded off to 8 digits; it is not rounded otherwise.
   * @group datetime_funcs
   * @since 2.4.0
   */
  def months_between(end: Column, start: Column, roundOff: Boolean): Column = withExpr {
    MonthsBetween(end.expr, start.expr, lit(roundOff).expr)
  }

  /**
   * Returns the first date which is later than the value of the `date` column that is on the
   * specified day of the week.
   *
   * For example, `next_day('2015-07-27', "Sunday")` returns 2015-08-02 because that is the first
   * Sunday after 2015-07-27.
   *
   * @param date      A date, timestamp or string. If a string, the data must be in a format that
   *                  can be cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param dayOfWeek Case insensitive, and accepts: "Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"
   * @return A date, or null if `date` was a string that could not be cast to a date or if
   *         `dayOfWeek` was an invalid value
   * @group datetime_funcs
   * @since 1.5.0
   */
  def next_day(date: Column, dayOfWeek: String): Column = withExpr {
    NextDay(date.expr, lit(dayOfWeek).expr)
  }

  /**
   * Extracts the seconds as an integer from a given date/timestamp/string.
   * @return An integer, or null if the input was a string that could not be cast to a timestamp
   * @group datetime_funcs
   * @since 1.5.0
   */
  def second(e: Column): Column = withExpr { Second(e.expr) }

  /**
   * Extracts the week number as an integer from a given date/timestamp/string.
   *
   * A week is considered to start on a Monday and week 1 is the first week with more than 3 days,
   * as defined by ISO 8601
   *
   * @return An integer, or null if the input was a string that could not be cast to a date
   * @group datetime_funcs
   * @since 1.5.0
   */
  def weekofyear(e: Column): Column = withExpr { WeekOfYear(e.expr) }

  /**
   * Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string
   * representing the timestamp of that moment in the current system time zone in the
   * yyyy-MM-dd HH:mm:ss format.
   *
   * @param ut A number of a type that is castable to a long, such as string or integer. Can be
   *           negative for timestamps before the unix epoch
   * @return A string, or null if the input was a string that could not be cast to a long
   * @group datetime_funcs
   * @since 1.5.0
   */
  def from_unixtime(ut: Column): Column = withExpr {
    FromUnixTime(ut.expr, Literal("yyyy-MM-dd HH:mm:ss"))
  }

  /**
   * Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string
   * representing the timestamp of that moment in the current system time zone in the given
   * format.
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param ut A number of a type that is castable to a long, such as string or integer. Can be
   *           negative for timestamps before the unix epoch
   * @param f  A date time pattern that the input will be formatted to
   * @return A string, or null if `ut` was a string that could not be cast to a long or `f` was
   *         an invalid date time pattern
   * @group datetime_funcs
   * @since 1.5.0
   */
  def from_unixtime(ut: Column, f: String): Column = withExpr {
    FromUnixTime(ut.expr, Literal(f))
  }

  /**
   * Returns the current Unix timestamp (in seconds) as a long.
   *
   * @note All calls of `unix_timestamp` within the same query return the same value
   * (i.e. the current timestamp is calculated at the start of query evaluation).
   *
   * @group datetime_funcs
   * @since 1.5.0
   */
  def unix_timestamp(): Column = withExpr {
    UnixTimestamp(CurrentTimestamp(), Literal("yyyy-MM-dd HH:mm:ss"))
  }

  /**
   * Converts time string in format yyyy-MM-dd HH:mm:ss to Unix timestamp (in seconds),
   * using the default timezone and the default locale.
   *
   * @param s A date, timestamp or string. If a string, the data must be in the
   *          `yyyy-MM-dd HH:mm:ss` format
   * @return A long, or null if the input was a string not of the correct format
   * @group datetime_funcs
   * @since 1.5.0
   */
  def unix_timestamp(s: Column): Column = withExpr {
    UnixTimestamp(s.expr, Literal("yyyy-MM-dd HH:mm:ss"))
  }

  /**
   * Converts time string with given pattern to Unix timestamp (in seconds).
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param s A date, timestamp or string. If a string, the data must be in a format that can be
   *          cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param p A date time pattern detailing the format of `s` when `s` is a string
   * @return A long, or null if `s` was a string that could not be cast to a date or `p` was
   *         an invalid format
   * @group datetime_funcs
   * @since 1.5.0
   */
  def unix_timestamp(s: Column, p: String): Column = withExpr { UnixTimestamp(s.expr, Literal(p)) }

  /**
   * Converts to a timestamp by casting rules to `TimestampType`.
   *
   * @param s A date, timestamp or string. If a string, the data must be in a format that can be
   *          cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return A timestamp, or null if the input was a string that could not be cast to a timestamp
   * @group datetime_funcs
   * @since 2.2.0
   */
  def to_timestamp(s: Column): Column = withExpr {
    new ParseToTimestamp(s.expr)
  }

  /**
   * Converts time string with the given pattern to timestamp.
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param s   A date, timestamp or string. If a string, the data must be in a format that can be
   *            cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss`
   * @param fmt A date time pattern detailing the format of `s` when `s` is a string
   * @return A timestamp, or null if `s` was a string that could not be cast to a timestamp or
   *         `fmt` was an invalid format
   * @group datetime_funcs
   * @since 2.2.0
   */
  def to_timestamp(s: Column, fmt: String): Column = withExpr {
    new ParseToTimestamp(s.expr, Literal(fmt))
  }

  /**
   * Converts the column into `DateType` by casting rules to `DateType`.
   *
   * @group datetime_funcs
   * @since 1.5.0
   */
  def to_date(e: Column): Column = withExpr { new ParseToDate(e.expr) }

  /**
   * Converts the column into a `DateType` with a specified format
   *
   * See [[java.text.SimpleDateFormat]] for valid date and time format patterns
   *
   * @param e   A date, timestamp or string. If a string, the data must be in a format that can be
   *            cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param fmt A date time pattern detailing the format of `e` when `e`is a string
   * @return A date, or null if `e` was a string that could not be cast to a date or `fmt` was an
   *         invalid format
   * @group datetime_funcs
   * @since 2.2.0
   */
  def to_date(e: Column, fmt: String): Column = withExpr {
    new ParseToDate(e.expr, Literal(fmt))
  }

  /**
   * Returns date truncated to the unit specified by the format.
   *
   * For example, `trunc("2018-11-19 12:01:19", "year")` returns 2018-01-01
   *
   * @param date A date, timestamp or string. If a string, the data must be in a format that can be
   *             cast to a date, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param format: 'year', 'yyyy', 'yy' for truncate by year,
   *               or 'month', 'mon', 'mm' for truncate by month
   *
   * @return A date, or null if `date` was a string that could not be cast to a date or `format`
   *         was an invalid value
   * @group datetime_funcs
   * @since 1.5.0
   */
  def trunc(date: Column, format: String): Column = withExpr {
    TruncDate(date.expr, Literal(format))
  }

  /**
   * Returns timestamp truncated to the unit specified by the format.
   *
   * For example, `date_tunc("2018-11-19 12:01:19", "year")` returns 2018-01-01 00:00:00
   *
   * @param format: 'year', 'yyyy', 'yy' for truncate by year,
   *                'month', 'mon', 'mm' for truncate by month,
   *                'day', 'dd' for truncate by day,
   *                Other options are: 'second', 'minute', 'hour', 'week', 'month', 'quarter'
   * @param timestamp A date, timestamp or string. If a string, the data must be in a format that
   *                  can be cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @return A timestamp, or null if `timestamp` was a string that could not be cast to a timestamp
   *         or `format` was an invalid value
   * @group datetime_funcs
   * @since 2.3.0
   */
  def date_trunc(format: String, timestamp: Column): Column = withExpr {
    TruncTimestamp(Literal(format), timestamp.expr)
  }

  /**
   * Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC, and renders
   * that time as a timestamp in the given time zone. For example, 'GMT+1' would yield
   * '2017-07-14 03:40:00.0'.
   *
   * @param ts A date, timestamp or string. If a string, the data must be in a format that can be
   *           cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param tz A string detailing the time zone that the input should be adjusted to, such as
   *           `Europe/London`, `PST` or `GMT+5`
   * @return A timestamp, or null if `ts` was a string that could not be cast to a timestamp or
   *         `tz` was an invalid value
   * @group datetime_funcs
   * @since 1.5.0
   */
  def from_utc_timestamp(ts: Column, tz: String): Column = withExpr {
    FromUTCTimestamp(ts.expr, Literal(tz))
  }

  /**
   * Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in UTC, and renders
   * that time as a timestamp in the given time zone. For example, 'GMT+1' would yield
   * '2017-07-14 03:40:00.0'.
   * @group datetime_funcs
   * @since 2.4.0
   */
  def from_utc_timestamp(ts: Column, tz: Column): Column = withExpr {
    FromUTCTimestamp(ts.expr, tz.expr)
  }

  /**
   * Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the given time
   * zone, and renders that time as a timestamp in UTC. For example, 'GMT+1' would yield
   * '2017-07-14 01:40:00.0'.
   *
   * @param ts A date, timestamp or string. If a string, the data must be in a format that can be
   *           cast to a timestamp, such as `yyyy-MM-dd` or `yyyy-MM-dd HH:mm:ss.SSSS`
   * @param tz A string detailing the time zone that the input belongs to, such as `Europe/London`,
   *           `PST` or `GMT+5`
   * @return A timestamp, or null if `ts` was a string that could not be cast to a timestamp or
   *         `tz` was an invalid value
   * @group datetime_funcs
   * @since 1.5.0
   */
  def to_utc_timestamp(ts: Column, tz: String): Column = withExpr {
    ToUTCTimestamp(ts.expr, Literal(tz))
  }

  /**
   * Given a timestamp like '2017-07-14 02:40:00.0', interprets it as a time in the given time
   * zone, and renders that time as a timestamp in UTC. For example, 'GMT+1' would yield
   * '2017-07-14 01:40:00.0'.
   * @group datetime_funcs
   * @since 2.4.0
   */
  def to_utc_timestamp(ts: Column, tz: Column): Column = withExpr {
    ToUTCTimestamp(ts.expr, tz.expr)
  }

  /**
   * Bucketize rows into one or more time windows given a timestamp specifying column. Window
   * starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window
   * [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in
   * the order of months are not supported. The following example takes the average stock price for
   * a one minute window every 10 seconds starting 5 seconds after the hour:
   *
   * {{{
   *   val df = ... // schema => timestamp: TimestampType, stockId: StringType, price: DoubleType
   *   df.groupBy(window($"time", "1 minute", "10 seconds", "5 seconds"), $"stockId")
   *     .agg(mean("price"))
   * }}}
   *
   * The windows will look like:
   *
   * {{{
   *   09:00:05-09:01:05
   *   09:00:15-09:01:15
   *   09:00:25-09:01:25 ...
   * }}}
   *
   * For a streaming query, you may use the function `current_timestamp` to generate windows on
   * processing time.
   *
   * @param timeColumn The column or the expression to use as the timestamp for windowing by time.
   *                   The time column must be of TimestampType.
   * @param windowDuration A string specifying the width of the window, e.g. `10 minutes`,
   *                       `1 second`. Check `org.apache.spark.unsafe.types.CalendarInterval` for
   *                       valid duration identifiers. Note that the duration is a fixed length of
   *                       time, and does not vary over time according to a calendar. For example,
   *                       `1 day` always means 86,400,000 milliseconds, not a calendar day.
   * @param slideDuration A string specifying the sliding interval of the window, e.g. `1 minute`.
   *                      A new window will be generated every `slideDuration`. Must be less than
   *                      or equal to the `windowDuration`. Check
   *                      `org.apache.spark.unsafe.types.CalendarInterval` for valid duration
   *                      identifiers. This duration is likewise absolute, and does not vary
   *                      according to a calendar.
   * @param startTime The offset with respect to 1970-01-01 00:00:00 UTC with which to start
   *                  window intervals. For example, in order to have hourly tumbling windows that
   *                  start 15 minutes past the hour, e.g. 12:15-13:15, 13:15-14:15... provide
   *                  `startTime` as `15 minutes`.
   *
   * @group datetime_funcs
   * @since 2.0.0
   */
  def window(
      timeColumn: Column,
      windowDuration: String,
      slideDuration: String,
      startTime: String): Column = {
    withExpr {
      TimeWindow(timeColumn.expr, windowDuration, slideDuration, startTime)
    }.as("window")
  }


  /**
   * Bucketize rows into one or more time windows given a timestamp specifying column. Window
   * starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window
   * [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in
   * the order of months are not supported. The windows start beginning at 1970-01-01 00:00:00 UTC.
   * The following example takes the average stock price for a one minute window every 10 seconds:
   *
   * {{{
   *   val df = ... // schema => timestamp: TimestampType, stockId: StringType, price: DoubleType
   *   df.groupBy(window($"time", "1 minute", "10 seconds"), $"stockId")
   *     .agg(mean("price"))
   * }}}
   *
   * The windows will look like:
   *
   * {{{
   *   09:00:00-09:01:00
   *   09:00:10-09:01:10
   *   09:00:20-09:01:20 ...
   * }}}
   *
   * For a streaming query, you may use the function `current_timestamp` to generate windows on
   * processing time.
   *
   * @param timeColumn The column or the expression to use as the timestamp for windowing by time.
   *                   The time column must be of TimestampType.
   * @param windowDuration A string specifying the width of the window, e.g. `10 minutes`,
   *                       `1 second`. Check `org.apache.spark.unsafe.types.CalendarInterval` for
   *                       valid duration identifiers. Note that the duration is a fixed length of
   *                       time, and does not vary over time according to a calendar. For example,
   *                       `1 day` always means 86,400,000 milliseconds, not a calendar day.
   * @param slideDuration A string specifying the sliding interval of the window, e.g. `1 minute`.
   *                      A new window will be generated every `slideDuration`. Must be less than
   *                      or equal to the `windowDuration`. Check
   *                      `org.apache.spark.unsafe.types.CalendarInterval` for valid duration
   *                      identifiers. This duration is likewise absolute, and does not vary
   *                      according to a calendar.
   *
   * @group datetime_funcs
   * @since 2.0.0
   */
  def window(timeColumn: Column, windowDuration: String, slideDuration: String): Column = {
    window(timeColumn, windowDuration, slideDuration, "0 second")
  }

  /**
   * Generates tumbling time windows given a timestamp specifying column. Window
   * starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window
   * [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in
   * the order of months are not supported. The windows start beginning at 1970-01-01 00:00:00 UTC.
   * The following example takes the average stock price for a one minute tumbling window:
   *
   * {{{
   *   val df = ... // schema => timestamp: TimestampType, stockId: StringType, price: DoubleType
   *   df.groupBy(window($"time", "1 minute"), $"stockId")
   *     .agg(mean("price"))
   * }}}
   *
   * The windows will look like:
   *
   * {{{
   *   09:00:00-09:01:00
   *   09:01:00-09:02:00
   *   09:02:00-09:03:00 ...
   * }}}
   *
   * For a streaming query, you may use the function `current_timestamp` to generate windows on
   * processing time.
   *
   * @param timeColumn The column or the expression to use as the timestamp for windowing by time.
   *                   The time column must be of TimestampType.
   * @param windowDuration A string specifying the width of the window, e.g. `10 minutes`,
   *                       `1 second`. Check `org.apache.spark.unsafe.types.CalendarInterval` for
   *                       valid duration identifiers.
   *
   * @group datetime_funcs
   * @since 2.0.0
   */
  def window(timeColumn: Column, windowDuration: String): Column = {
    window(timeColumn, windowDuration, windowDuration, "0 second")
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Collection functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Returns null if the array is null, true if the array contains `value`, and false otherwise.
   * @group collection_funcs
   * @since 1.5.0
   */
  def array_contains(column: Column, value: Any): Column = withExpr {
    ArrayContains(column.expr, lit(value).expr)
  }

  /**
   * Returns `true` if `a1` and `a2` have at least one non-null element in common. If not and both
   * the arrays are non-empty and any of them contains a `null`, it returns `null`. It returns
   * `false` otherwise.
   * @group collection_funcs
   * @since 2.4.0
   */
  def arrays_overlap(a1: Column, a2: Column): Column = withExpr {
    ArraysOverlap(a1.expr, a2.expr)
  }

  /**
   * Returns an array containing all the elements in `x` from index `start` (or starting from the
   * end if `start` is negative) with the specified `length`.
   *
   * @param x the array column to be sliced
   * @param start the starting index
   * @param length the length of the slice
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def slice(x: Column, start: Int, length: Int): Column = withExpr {
    Slice(x.expr, Literal(start), Literal(length))
  }

  /**
   * Concatenates the elements of `column` using the `delimiter`. Null values are replaced with
   * `nullReplacement`.
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_join(column: Column, delimiter: String, nullReplacement: String): Column = withExpr {
    ArrayJoin(column.expr, Literal(delimiter), Some(Literal(nullReplacement)))
  }

  /**
   * Concatenates the elements of `column` using the `delimiter`.
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_join(column: Column, delimiter: String): Column = withExpr {
    ArrayJoin(column.expr, Literal(delimiter), None)
  }

  /**
   * Concatenates multiple input columns together into a single column.
   * The function works with strings, binary and compatible array columns.
   *
   * @group collection_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def concat(exprs: Column*): Column = withExpr { Concat(exprs.map(_.expr)) }

  /**
   * Locates the position of the first occurrence of the value in the given array as long.
   * Returns null if either of the arguments are null.
   *
   * @note The position is not zero based, but 1 based index. Returns 0 if value
   * could not be found in array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_position(column: Column, value: Any): Column = withExpr {
    ArrayPosition(column.expr, lit(value).expr)
  }

  /**
   * Returns element of array at given index in value if column is array. Returns value for
   * the given key in value if column is map.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def element_at(column: Column, value: Any): Column = withExpr {
    ElementAt(column.expr, lit(value).expr)
  }

  /**
   * Sorts the input array in ascending order. The elements of the input array must be orderable.
   * Null elements will be placed at the end of the returned array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_sort(e: Column): Column = withExpr { ArraySort(e.expr) }

  /**
   * Remove all elements that equal to element from the given array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_remove(column: Column, element: Any): Column = withExpr {
    ArrayRemove(column.expr, lit(element).expr)
  }

  /**
   * Removes duplicate values from the array.
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_distinct(e: Column): Column = withExpr { ArrayDistinct(e.expr) }

  /**
   * Returns an array of the elements in the intersection of the given two arrays,
   * without duplicates.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_intersect(col1: Column, col2: Column): Column = withExpr {
    ArrayIntersect(col1.expr, col2.expr)
  }

  /**
   * Returns an array of the elements in the union of the given two arrays, without duplicates.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_union(col1: Column, col2: Column): Column = withExpr {
    ArrayUnion(col1.expr, col2.expr)
  }

  /**
   * Returns an array of the elements in the first array but not in the second array,
   * without duplicates. The order of elements in the result is not determined
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_except(col1: Column, col2: Column): Column = withExpr {
    ArrayExcept(col1.expr, col2.expr)
  }

  /**
   * Creates a new row for each element in the given array or map column.
   *
   * @group collection_funcs
   * @since 1.3.0
   */
  def explode(e: Column): Column = withExpr { Explode(e.expr) }

  /**
   * Creates a new row for each element in the given array or map column.
   * Unlike explode, if the array/map is null or empty then null is produced.
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def explode_outer(e: Column): Column = withExpr { GeneratorOuter(Explode(e.expr)) }

  /**
   * Creates a new row for each element with position in the given array or map column.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def posexplode(e: Column): Column = withExpr { PosExplode(e.expr) }

  /**
   * Creates a new row for each element with position in the given array or map column.
   * Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def posexplode_outer(e: Column): Column = withExpr { GeneratorOuter(PosExplode(e.expr)) }

  /**
   * Extracts json object from a json string based on json path specified, and returns json string
   * of the extracted json object. It will return null if the input json string is invalid.
   *
   * @group collection_funcs
   * @since 1.6.0
   */
  def get_json_object(e: Column, path: String): Column = withExpr {
    GetJsonObject(e.expr, lit(path).expr)
  }

  /**
   * Creates a new row for a json column according to the given field names.
   *
   * @group collection_funcs
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def json_tuple(json: Column, fields: String*): Column = withExpr {
    require(fields.nonEmpty, "at least 1 field name should be given.")
    JsonTuple(json.expr +: fields.map(Literal.apply))
  }

  /**
   * (Scala-specific) Parses a column containing a JSON string into a `StructType` with the
   * specified schema. Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. Accepts the same options as the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def from_json(e: Column, schema: StructType, options: Map[String, String]): Column =
    from_json(e, schema.asInstanceOf[DataType], options)

  /**
   * (Scala-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. accepts the same options and the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def from_json(e: Column, schema: DataType, options: Map[String, String]): Column = withExpr {
    JsonToStructs(schema, options, e.expr)
  }

  /**
   * (Java-specific) Parses a column containing a JSON string into a `StructType` with the
   * specified schema. Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. accepts the same options and the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def from_json(e: Column, schema: StructType, options: java.util.Map[String, String]): Column =
    from_json(e, schema, options.asScala.toMap)

  /**
   * (Java-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. accepts the same options and the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def from_json(e: Column, schema: DataType, options: java.util.Map[String, String]): Column =
    from_json(e, schema, options.asScala.toMap)

  /**
   * Parses a column containing a JSON string into a `StructType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def from_json(e: Column, schema: StructType): Column =
    from_json(e, schema, Map.empty[String, String])

  /**
   * Parses a column containing a JSON string into a `MapType` with `StringType` as keys type,
   * `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   *
   * @group collection_funcs
   * @since 2.2.0
   */
  def from_json(e: Column, schema: DataType): Column =
    from_json(e, schema, Map.empty[String, String])

  /**
   * (Java-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string as a json string. In Spark 2.1,
   *               the user-provided schema has to be in JSON format. Since Spark 2.2, the DDL
   *               format is also supported for the schema.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def from_json(e: Column, schema: String, options: java.util.Map[String, String]): Column = {
    from_json(e, schema, options.asScala.toMap)
  }

  /**
   * (Scala-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string as a json string, it could be a
   *               JSON format string or a DDL-formatted string.
   *
   * @group collection_funcs
   * @since 2.3.0
   */
  def from_json(e: Column, schema: String, options: Map[String, String]): Column = {
    val dataType = try {
      DataType.fromJson(schema)
    } catch {
      case NonFatal(_) => DataType.fromDDL(schema)
    }
    from_json(e, dataType, options)
  }

  /**
   * (Scala-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` of `StructType`s with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def from_json(e: Column, schema: Column): Column = {
    from_json(e, schema, Map.empty[String, String].asJava)
  }

  /**
   * (Java-specific) Parses a column containing a JSON string into a `MapType` with `StringType`
   * as keys type, `StructType` or `ArrayType` of `StructType`s with the specified schema.
   * Returns `null`, in the case of an unparseable string.
   *
   * @param e a string column containing JSON data.
   * @param schema the schema to use when parsing the json string
   * @param options options to control how the json is parsed. accepts the same options and the
   *                json data source.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def from_json(e: Column, schema: Column, options: java.util.Map[String, String]): Column = {
    withExpr(new JsonToStructs(e.expr, schema.expr, options.asScala.toMap))
  }

  /**
   * Parses a JSON string and infers its schema in DDL format.
   *
   * @param json a JSON string.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def schema_of_json(json: String): Column = schema_of_json(lit(json))

  /**
   * Parses a JSON string and infers its schema in DDL format.
   *
   * @param json a string literal containing a JSON string.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def schema_of_json(json: Column): Column = withExpr(new SchemaOfJson(json.expr))

  /**
   * (Scala-specific) Converts a column containing a `StructType`, `ArrayType` or
   * a `MapType` into a JSON string with the specified schema.
   * Throws an exception, in the case of an unsupported type.
   *
   * @param e a column containing a struct, an array or a map.
   * @param options options to control how the struct column is converted into a json string.
   *                accepts the same options and the json data source.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def to_json(e: Column, options: Map[String, String]): Column = withExpr {
    StructsToJson(options, e.expr)
  }

  /**
   * (Java-specific) Converts a column containing a `StructType`, `ArrayType` or
   * a `MapType` into a JSON string with the specified schema.
   * Throws an exception, in the case of an unsupported type.
   *
   * @param e a column containing a struct, an array or a map.
   * @param options options to control how the struct column is converted into a json string.
   *                accepts the same options and the json data source.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def to_json(e: Column, options: java.util.Map[String, String]): Column =
    to_json(e, options.asScala.toMap)

  /**
   * Converts a column containing a `StructType`, `ArrayType` or
   * a `MapType` into a JSON string with the specified schema.
   * Throws an exception, in the case of an unsupported type.
   *
   * @param e a column containing a struct, an array or a map.
   *
   * @group collection_funcs
   * @since 2.1.0
   */
  def to_json(e: Column): Column =
    to_json(e, Map.empty[String, String])

  /**
   * Returns length of array or map.
   *
   * @group collection_funcs
   * @since 1.5.0
   */
  def size(e: Column): Column = withExpr { Size(e.expr) }

  /**
   * Sorts the input array for the given column in ascending order,
   * according to the natural ordering of the array elements.
   * Null elements will be placed at the beginning of the returned array.
   *
   * @group collection_funcs
   * @since 1.5.0
   */
  def sort_array(e: Column): Column = sort_array(e, asc = true)

  /**
   * Sorts the input array for the given column in ascending or descending order,
   * according to the natural ordering of the array elements.
   * Null elements will be placed at the beginning of the returned array in ascending order or
   * at the end of the returned array in descending order.
   *
   * @group collection_funcs
   * @since 1.5.0
   */
  def sort_array(e: Column, asc: Boolean): Column = withExpr { SortArray(e.expr, lit(asc).expr) }

  /**
   * Returns the minimum value in the array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_min(e: Column): Column = withExpr { ArrayMin(e.expr) }

  /**
   * Returns the maximum value in the array.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_max(e: Column): Column = withExpr { ArrayMax(e.expr) }

  /**
   * Returns a random permutation of the given array.
   *
   * @note The function is non-deterministic.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def shuffle(e: Column): Column = withExpr { Shuffle(e.expr) }

  /**
   * Returns a reversed string or an array with reverse order of elements.
   * @group collection_funcs
   * @since 1.5.0
   */
  def reverse(e: Column): Column = withExpr { Reverse(e.expr) }

  /**
   * Creates a single array from an array of arrays. If a structure of nested arrays is deeper than
   * two levels, only one level of nesting is removed.
   * @group collection_funcs
   * @since 2.4.0
   */
  def flatten(e: Column): Column = withExpr { Flatten(e.expr) }

  /**
   * Generate a sequence of integers from start to stop, incrementing by step.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def sequence(start: Column, stop: Column, step: Column): Column = withExpr {
    new Sequence(start.expr, stop.expr, step.expr)
  }

  /**
   * Generate a sequence of integers from start to stop,
   * incrementing by 1 if start is less than or equal to stop, otherwise -1.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def sequence(start: Column, stop: Column): Column = withExpr {
    new Sequence(start.expr, stop.expr)
  }

  /**
   * Creates an array containing the left argument repeated the number of times given by the
   * right argument.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_repeat(left: Column, right: Column): Column = withExpr {
    ArrayRepeat(left.expr, right.expr)
  }

  /**
   * Creates an array containing the left argument repeated the number of times given by the
   * right argument.
   *
   * @group collection_funcs
   * @since 2.4.0
   */
  def array_repeat(e: Column, count: Int): Column = array_repeat(e, lit(count))

  /**
   * Returns an unordered array containing the keys of the map.
   * @group collection_funcs
   * @since 2.3.0
   */
  def map_keys(e: Column): Column = withExpr { MapKeys(e.expr) }

  /**
   * Returns an unordered array containing the values of the map.
   * @group collection_funcs
   * @since 2.3.0
   */
  def map_values(e: Column): Column = withExpr { MapValues(e.expr) }

  /**
   * Returns a map created from the given array of entries.
   * @group collection_funcs
   * @since 2.4.0
   */
  def map_from_entries(e: Column): Column = withExpr { MapFromEntries(e.expr) }

  /**
   * Returns a merged array of structs in which the N-th struct contains all N-th values of input
   * arrays.
   * @group collection_funcs
   * @since 2.4.0
   */
  @scala.annotation.varargs
  def arrays_zip(e: Column*): Column = withExpr { ArraysZip(e.map(_.expr)) }

  /**
   * Returns the union of all the given maps.
   * @group collection_funcs
   * @since 2.4.0
   */
  @scala.annotation.varargs
  def map_concat(cols: Column*): Column = withExpr { MapConcat(cols.map(_.expr)) }

  // scalastyle:off line.size.limit
  // scalastyle:off parameter.number

  /* Use the following code to generate:

  (0 to 10).foreach { x =>
    val types = (1 to x).foldRight("RT")((i, s) => {s"A$i, $s"})
    val typeTags = (1 to x).map(i => s"A$i: TypeTag").foldLeft("RT: TypeTag")(_ + ", " + _)
    val inputSchemas = (1 to x).foldRight("Nil")((i, s) => {s"Try(ScalaReflection.schemaFor(typeTag[A$i])).toOption :: $s"})
    println(s"""
      |/**
      | * Defines a Scala closure of $x arguments as user-defined function (UDF).
      | * The data types are automatically inferred based on the Scala closure's
      | * signature. By default the returned UDF is deterministic. To change it to
      | * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
      | *
      | * @group udf_funcs
      | * @since 1.3.0
      | */
      |def udf[$typeTags](f: Function$x[$types]): UserDefinedFunction = {
      |  val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
      |  val inputSchemas = $inputSchemas
      |  val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
      |  if (nullable) udf else udf.asNonNullable()
      |}""".stripMargin)
  }

  (0 to 10).foreach { i =>
    val extTypeArgs = (0 to i).map(_ => "_").mkString(", ")
    val anyTypeArgs = (0 to i).map(_ => "Any").mkString(", ")
    val anyCast = s".asInstanceOf[UDF$i[$anyTypeArgs]]"
    val anyParams = (1 to i).map(_ => "_: Any").mkString(", ")
    val funcCall = if (i == 0) "() => func" else "func"
    println(s"""
      |/**
      | * Defines a Java UDF$i instance as user-defined function (UDF).
      | * The caller must specify the output data type, and there is no automatic input type coercion.
      | * By default the returned UDF is deterministic. To change it to nondeterministic, call the
      | * API `UserDefinedFunction.asNondeterministic()`.
      | *
      | * @group udf_funcs
      | * @since 2.3.0
      | */
      |def udf(f: UDF$i[$extTypeArgs], returnType: DataType): UserDefinedFunction = {
      |  val func = f$anyCast.call($anyParams)
      |  SparkUserDefinedFunction.create($funcCall, returnType, inputSchemas = Seq.fill($i)(None))
      |}""".stripMargin)
  }

  */

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Scala UDF functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Defines a Scala closure of 0 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag](f: Function0[RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 1 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag](f: Function1[A1, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 2 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag](f: Function2[A1, A2, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 3 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag](f: Function3[A1, A2, A3, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 4 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag](f: Function4[A1, A2, A3, A4, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 5 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag](f: Function5[A1, A2, A3, A4, A5, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 6 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag](f: Function6[A1, A2, A3, A4, A5, A6, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 7 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag, A7: TypeTag](f: Function7[A1, A2, A3, A4, A5, A6, A7, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A7])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 8 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag, A7: TypeTag, A8: TypeTag](f: Function8[A1, A2, A3, A4, A5, A6, A7, A8, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A7])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A8])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 9 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag, A7: TypeTag, A8: TypeTag, A9: TypeTag](f: Function9[A1, A2, A3, A4, A5, A6, A7, A8, A9, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A7])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A8])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A9])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  /**
   * Defines a Scala closure of 10 arguments as user-defined function (UDF).
   * The data types are automatically inferred based on the Scala closure's
   * signature. By default the returned UDF is deterministic. To change it to
   * nondeterministic, call the API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 1.3.0
   */
  def udf[RT: TypeTag, A1: TypeTag, A2: TypeTag, A3: TypeTag, A4: TypeTag, A5: TypeTag, A6: TypeTag, A7: TypeTag, A8: TypeTag, A9: TypeTag, A10: TypeTag](f: Function10[A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, RT]): UserDefinedFunction = {
    val ScalaReflection.Schema(dataType, nullable) = ScalaReflection.schemaFor[RT]
    val inputSchemas = Try(ScalaReflection.schemaFor(typeTag[A1])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A2])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A3])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A4])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A5])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A6])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A7])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A8])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A9])).toOption :: Try(ScalaReflection.schemaFor(typeTag[A10])).toOption :: Nil
    val udf = SparkUserDefinedFunction.create(f, dataType, inputSchemas)
    if (nullable) udf else udf.asNonNullable()
  }

  //////////////////////////////////////////////////////////////////////////////////////////////
  // Java UDF functions
  //////////////////////////////////////////////////////////////////////////////////////////////

  /**
   * Defines a Java UDF0 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF0[_], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF0[Any]].call()
    SparkUserDefinedFunction.create(() => func, returnType, inputSchemas = Seq.fill(0)(None))
  }

  /**
   * Defines a Java UDF1 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF1[_, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF1[Any, Any]].call(_: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(1)(None))
  }

  /**
   * Defines a Java UDF2 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF2[_, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF2[Any, Any, Any]].call(_: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(2)(None))
  }

  /**
   * Defines a Java UDF3 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF3[_, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF3[Any, Any, Any, Any]].call(_: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(3)(None))
  }

  /**
   * Defines a Java UDF4 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF4[_, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF4[Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(4)(None))
  }

  /**
   * Defines a Java UDF5 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF5[_, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF5[Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(5)(None))
  }

  /**
   * Defines a Java UDF6 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF6[_, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF6[Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(6)(None))
  }

  /**
   * Defines a Java UDF7 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF7[_, _, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF7[Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(7)(None))
  }

  /**
   * Defines a Java UDF8 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF8[_, _, _, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF8[Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(8)(None))
  }

  /**
   * Defines a Java UDF9 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF9[_, _, _, _, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF9[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(9)(None))
  }

  /**
   * Defines a Java UDF10 instance as user-defined function (UDF).
   * The caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @group udf_funcs
   * @since 2.3.0
   */
  def udf(f: UDF10[_, _, _, _, _, _, _, _, _, _, _], returnType: DataType): UserDefinedFunction = {
    val func = f.asInstanceOf[UDF10[Any, Any, Any, Any, Any, Any, Any, Any, Any, Any, Any]].call(_: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any, _: Any)
    SparkUserDefinedFunction.create(func, returnType, inputSchemas = Seq.fill(10)(None))
  }

  // scalastyle:on parameter.number
  // scalastyle:on line.size.limit

  /**
   * Defines a deterministic user-defined function (UDF) using a Scala closure. For this variant,
   * the caller must specify the output data type, and there is no automatic input type coercion.
   * By default the returned UDF is deterministic. To change it to nondeterministic, call the
   * API `UserDefinedFunction.asNondeterministic()`.
   *
   * @param f  A closure in Scala
   * @param dataType  The output data type of the UDF
   *
   * @group udf_funcs
   * @since 2.0.0
   */
  def udf(f: AnyRef, dataType: DataType): UserDefinedFunction = {
    // TODO: should call SparkUserDefinedFunction.create() instead but inputSchemas is currently
    // unavailable. We may need to create type-safe overloaded versions of udf() methods.
    new UserDefinedFunction(f, dataType, inputTypes = None)
  }

  /**
   * Call an user-defined function.
   * Example:
   * {{{
   *  import org.apache.spark.sql._
   *
   *  val df = Seq(("id1", 1), ("id2", 4), ("id3", 5)).toDF("id", "value")
   *  val spark = df.sparkSession
   *  spark.udf.register("simpleUDF", (v: Int) => v * v)
   *  df.select($"id", callUDF("simpleUDF", $"value"))
   * }}}
   *
   * @group udf_funcs
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def callUDF(udfName: String, cols: Column*): Column = withExpr {
    UnresolvedFunction(udfName, cols.map(_.expr), isDistinct = false)
  }
}

[0m2021.03.05 11:51:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:51:04 INFO  time: compiled root in 1.72s[0m
[0m2021.03.05 11:52:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:52:41 INFO  time: compiled root in 1.52s[0m
[0m2021.03.05 11:53:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:53:17 INFO  time: compiled root in 1.59s[0m
[0m2021.03.05 11:55:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:55:32 INFO  time: compiled root in 1.52s[0m
[0m2021.03.05 11:55:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:55:37 INFO  time: compiled root in 1.71s[0m
[0m2021.03.05 11:58:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:58:56 INFO  time: compiled root in 1.48s[0m
[0m2021.03.05 11:59:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 11:59:07 INFO  time: compiled root in 1.48s[0m
[0m2021.03.05 12:00:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:00:04 INFO  time: compiled root in 1.38s[0m
[0m2021.03.05 12:00:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:00:42 INFO  time: compiled root in 1.48s[0m
[0m2021.03.05 12:00:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:00:48 INFO  time: compiled root in 1.44s[0m
[0m2021.03.05 12:01:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:01:13 INFO  time: compiled root in 1.42s[0m
[0m2021.03.05 12:02:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:02:59 INFO  time: compiled root in 1.41s[0m
[0m2021.03.05 12:03:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:03:59 INFO  time: compiled root in 1.45s[0m
[0m2021.03.05 12:04:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:04:07 INFO  time: compiled root in 1.43s[0m
[0m2021.03.05 12:05:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:05:21 INFO  time: compiled root in 1.35s[0m
[0m2021.03.05 12:05:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:05:46 INFO  time: compiled root in 1.44s[0m
[0m2021.03.05 12:07:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:07:53 INFO  time: compiled root in 1.35s[0m
[0m2021.03.05 12:07:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:07:55 INFO  time: compiled root in 1.47s[0m
[0m2021.03.05 12:08:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 12:09:00 INFO  time: compiled root in 1.42s[0m
[0m2021.03.05 12:33:44 INFO  shutting down Metals[0m
[0m2021.03.05 12:33:44 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.05 12:33:44 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
[0m2021.03.05 12:33:44 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.05 14:01:31 INFO  Started: Metals version 0.10.0 in workspace '/home/delaneylekien/project3/scalas3read' for client vscode 1.54.1.[0m
[0m2021.03.05 14:01:32 INFO  time: initialize in 0.74s[0m
[0m2021.03.05 14:01:33 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.05 14:01:32 WARN  no build target for: /home/delaneylekien/project3/scalas3read/src/main/scala/com/revature/scalas3read/Runner.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1588010729899781536/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.05 14:01:33 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.03.05 14:01:36 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split
import org.apache.spark.sql.functions.round 
import java.util.Arrays
import java.sql

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    val stateList = rawStateList.toDF("State Code", "State Name")

    // Combined the two dataFrames to get state codes assocaited with area name.

    val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show()
    
    //fetch_time BETWEEN TIMESTAMP '2020-03-01 00:00:00' AND TIMESTAMP '2020-03-31 23:59:59'

    // Read WET data from the Common Crawl s3 bucket
    // Since WET is read in as one long text file, use lineSep to split on each header of WET record
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    // Splitting the header WARC data from the plain text content for WET files
    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
  
    // Below is filtering for only Tech Jobs
    // First filter for URI's with career/job/employment
    // Second filter makes sure all results returned have keys words for tech jobs
    // This filters out non tech related jobs hat return from job in URI
    val filterCrawl = cuttingCrawl
      .filter($"WARC Header" rlike ".*WARC-Target-URI:.*career.*" 
        or ($"WARC Header" rlike ".*WARC-Target-URI:.*/job.*") 
        or ($"WARC Header" rlike ".*WARC-Target-URI:.*employment.*"))
      .filter($"Plain Text" rlike ".*Frontend.*" 
        or ($"Plain Text" rlike ".*Backendend.*") 
        or ($"Plain Text" rlike ".*Fullstack.*")
        or ($"Plain Text" rlike ".*Cybersecurity.*") 
        or ($"Plain Text" rlike ".*Software.*") 
        or ($"Plain Text" rlike ".*Computer.*"))
      .select($"WARC Header", $"Plain Text")
       

    // Turning Dataframe into RDD in order to get Key-Value pairs of occurrences of State Codes
    val sqlCrawl = filterCrawl
    .select($"Plain Text").as[String].flatMap(line => line.split(" ")).rdd

    val rddCrawl = sqlCrawl.map(word => (word, 1))
    .filter({case (key, value) => key.length < 3})
    .reduceByKey(_ + _)
    .toDF("State Code", "Tech Job Total")

    // Join earlier combinedCensusData Dataframe to rddCrawl Dataframe in order to determine 
    // question: "Where do we see relatively fewer tech ads proportional to population?"
    val combinedCrawl = rddCrawl.join(combinedCensusData,("State Code"))
    .withColumn("Tech Ads Proportional to Population", round(($"Tech Job Total" / $"Population Estimate Total" * 100) , 8))
    .select($"State Code", $"Geographic Area Name", $"Tech Job Total", $"Population Estimate Total", $"Tech Ads Proportional to Population")
    .show(51, false)
  
  }

}
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/delaneylekien/project3/scalas3read/.bloop'...
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/delaneylekien/project3/scalas3read/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/delaneylekien/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root-test' from '/home/delaneylekien/project3/scalas3read/target/streams/test/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/delaneylekien/project3/scalas3read/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1588010729899781536/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1588010729899781536/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 14:01:38 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
package com.revature.scalas3read

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.functions
import org.apache.spark.sql.types.IntegerType
import com.google.flatbuffers.Struct
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.Row
import scala.io.BufferedSource
import java.io.FileInputStream
import org.apache.spark.sql.functions.split
import org.apache.spark.sql.functions.round 
import java.util.Arrays
import java.sql

object Runner {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession
      .builder()
      .appName("scalas3read")
      .master("local[4]")
      // .config("spark.debug.maxToStringFields", 100)
      .getOrCreate()

    // Reference: https://sparkbyexamples.com/spark/spark-read-text-file-from-s3/#s3-dependency
    val key = System.getenv(("AWS_ACCESS_KEY_ID"))
    val secret = System.getenv(("AWS_SECRET_ACCESS_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")
    
    import spark.implicits._
    spark.sparkContext.setLogLevel("WARN")

    // Read in a Census CSV gathered from https://data.census.gov/cedsci/table?q=dp05&g=0100000US.04000.001&y=2019&tid=ACSDT1Y2019.B01003&moe=false&hidePreview=true
    // This CSV was of 2019 1 year estimate for population in every state

    val censusData = spark.read
      .format("csv")
      .option("header", "true")
      .load("ACSDT1Y2019.B01003_data_with_overlays_2021-02-23T105100.csv")

    // Created a State Code list for easier joining with additional warc data. 
    val rawStateList = Seq(
      ("AL", "Alabama"), ("AK", "Alaska"), ("AZ", "Arizona"), ("AR", "Arkansas"), ("CA", "California"), ("CO", "Colorado"), ("CT", "Connecticut"), ("DE", "Delaware"), 
      ("DC", "District of Columbia"), ("FL", "Florida"), ("GA", "Georgia"), ("HI", "Hawaii"), ("ID", "Idaho"), ("IL", "Illinois"), ("IN", "Indiana"), ("IA", "Iowa"), 
      ("KS", "Kansas"), ("KY", "Kentucky"), ("LA", "Louisiana"), ("ME", "Maine"), ("MD", "Maryland"), ("MA", "Massachusetts"), ("MI", "Michigan"), ("MN", "Minnesota"), 
      ("MS", "Mississippi"), ("MO", "Missouri"), ("MT", "Montana"), ("NE", "Nebraska"), ("NV", "Nevada"), ("NH", "New Hampshire"), ("NJ", "New Jersey"), ("NM", "New Mexico"), 
      ("NY", "New York"), ("NC", "North Carolina"), ("ND", "North Dakota"), ("OH", "Ohio"), ("OK", "Oklahoma"), ("OR", "Oregon"), ("PA", "Pennsylvania"), ("RI", "Rhode Island"), 
      ("SC", "South Carolina"), ("SD", "South Dakota"), ("TN", "Tennessee"), ("TX", "Texas"), ("UT", "Utah"), ("VT", "Vermont"), ("VA", "Virginia"), ("WA", "Washington"), 
      ("WV", "West Virginia"), ("WI", "Wisconsin"), ("WY", "Wyoming"))

    val stateList = rawStateList.toDF("State Code", "State Name")

    // Combined the two dataFrames to get state codes assocaited with area name.

    val combinedCensusData = censusData.join(stateList, $"Geographic Area Name" === $"State Name")

    // combinedCensusData
    //   .select("State Name", "State Code", "Population Estimate Total")
    //   .show()
    
    //fetch_time BETWEEN TIMESTAMP '2020-03-01 00:00:00' AND TIMESTAMP '2020-03-31 23:59:59'

    // Read WET data from the Common Crawl s3 bucket
    // Since WET is read in as one long text file, use lineSep to split on each header of WET record
    val commonCrawl = spark.read.option("lineSep", "WARC/1.0").text(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
    )
    .as[String]
    .map((str)=>{str.substring(str.indexOf("\n")+1)})
    .toDF("cut WET")

    // Splitting the header WARC data from the plain text content for WET files
    val cuttingCrawl = commonCrawl
      .withColumn("_tmp", split($"cut WET", "\r\n\r\n"))
      .select($"_tmp".getItem(0).as("WARC Header"), $"_tmp".getItem(1).as("Plain Text"))
  
    // Below is filtering for only Tech Jobs
    // First filter for URI's with career/job/employment
    // Second filter makes sure all results returned have keys words for tech jobs
    // This filters out non tech related jobs hat return from job in URI
    val filterCrawl = cuttingCrawl
      .filter($"WARC Header" rlike ".*WARC-Target-URI:.*career.*" 
        or ($"WARC Header" rlike ".*WARC-Target-URI:.*/job.*") 
        or ($"WARC Header" rlike ".*WARC-Target-URI:.*employment.*"))
      .filter($"Plain Text" rlike ".*Frontend.*" 
        or ($"Plain Text" rlike ".*Backendend.*") 
        or ($"Plain Text" rlike ".*Fullstack.*")
        or ($"Plain Text" rlike ".*Cybersecurity.*") 
        or ($"Plain Text" rlike ".*Software.*") 
        or ($"Plain Text" rlike ".*Computer.*"))
      .select($"WARC Header", $"Plain Text")
       

    // Turning Dataframe into RDD in order to get Key-Value pairs of occurrences of State Codes
    val sqlCrawl = filterCrawl
    .select($"Plain Text").as[String].flatMap(line => line.split(" ")).rdd

    val rddCrawl = sqlCrawl.map(word => (word, 1))
    .filter({case (key, value) => key.length < 3})
    .reduceByKey(_ + _)
    .toDF("State Code", "Tech Job Total")

    // Join earlier combinedCensusData Dataframe to rddCrawl Dataframe in order to determine 
    // question: "Where do we see relatively fewer tech ads proportional to population?"
    val combinedCrawl = rddCrawl.join(combinedCensusData,("State Code"))
    .withColumn("Tech Ads Proportional to Population", round(($"Tech Job Total" / $"Population Estimate Total" * 100) , 8))
    .select($"State Code", $"Geographic Area Name", $"Tech Job Total", $"Population Estimate Total", $"Tech Ads Proportional to Population")
    .show(51, false)
  
  }

}
[0m2021.03.05 14:01:39 INFO  time: code lens generation in 6.04s[0m
[0m2021.03.05 14:01:38 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0mOpening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher906117605765401679/bsp.socket'...2021.03.05 14:01:38 INFO  Attempting to connect to the build server...[0m

Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4233683376837492424/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher906117605765401679/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher906117605765401679/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4233683376837492424/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4233683376837492424/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 14:01:39 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 14:01:39 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/delaneylekien/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 14:01:39 INFO  time: Connected to build server in 6.43s[0m
[0m2021.03.05 14:01:39 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.05 14:01:40 INFO  time: Imported build in 0.38s[0m
[0m2021.03.05 14:01:42 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.05 14:01:42 INFO  time: indexed workspace in 3.51s[0m
